{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started in LCRC If you are looking to get access to LCRC for the first time, see below for either Argonne Employees or Non-Employees/Outside Collaborators . If you need to extend or reactivate your Argonne Collaborator account or if your Argonne username has changed , please find information below as well. To access LCRC resources, you need to complete 3 tasks at a minimum: Get an Argonne Domain or Collaborator Account Join an LCRC Project Set Up an SSH Connection Until ALL of these steps are completed, you will not be able to login to LCRC resources. Once you have setup your account, you should be able to access the resource of your choice by following our system documentation. If you have any questions related to your Argonne account or Argonne Collaborator account status or if you need a password reset, please send email to help@anl.gov or call the Argonne Service Desk at (630) 252-9999. LCRC support staff does not have access to this information directly.","title":"Getting Started"},{"location":"#getting-started-in-lcrc","text":"If you are looking to get access to LCRC for the first time, see below for either Argonne Employees or Non-Employees/Outside Collaborators . If you need to extend or reactivate your Argonne Collaborator account or if your Argonne username has changed , please find information below as well. To access LCRC resources, you need to complete 3 tasks at a minimum: Get an Argonne Domain or Collaborator Account Join an LCRC Project Set Up an SSH Connection Until ALL of these steps are completed, you will not be able to login to LCRC resources. Once you have setup your account, you should be able to access the resource of your choice by following our system documentation. If you have any questions related to your Argonne account or Argonne Collaborator account status or if you need a password reset, please send email to help@anl.gov or call the Argonne Service Desk at (630) 252-9999. LCRC support staff does not have access to this information directly.","title":"Getting Started in LCRC"},{"location":"search-test/","text":"Test test test {% include \".icons/material/magnify.svg\" %}","title":"Search test"},{"location":"account-project-management/accounts-and-access/","text":"Accounts and Access For Current Argonne Employees Existing Argonne Domain Account If you have an active badge, you likely have an account. It's the same account used for Argonne email and other apps. If you don't have an account or forgot your credentials, contact help@anl.gov or call the Argonne Service Desk at (630) 252-9999. LCRC support cannot directly address issues with your Argonne account. Accessing LCRC Clusters : Go to https://accounts.lcrc.anl.gov and log in with your Argonne credentials. Select Join Project on the left side. Search for the lcrc project and request membership. Instant membership for Argonne employees. You'll receive an email confirming LCRC access. Final Steps : Add a public SSH key ( Instructions ). Once the key is added, you can log in to the LCRC clusters. For Non-Employees/Outside Collaborators Argonne Collaborator Account : Apply at https://apps.anl.gov/registration/collaborators . Requires official Argonne email address of your sponsor (must be an Argonne employee). Accounts are annual and renewable. Non-US citizens may experience a waiting period before approval. Upon approval, you'll receive an email notification. Sponsors can check account status at Cyber Gate Pass . Accessing LCRC Clusters : Join an existing LCRC project. Log in at https://accounts.lcrc.anl.gov using your Argonne Collaborator credentials. Click Join Project and search for the project indicated by your sponsor. Request membership and await approval from your sponsor or the project owner. Upon approval, you'll be added to the LCRC project and receive a home directory on the LCRC clusters. Final Steps : Add a public SSH key ( Instructions ). Once the key is added, you can log in to the LCRC clusters. Managing Argonne Collaborator Accounts Extend or Reactivate Collaborator Accounts If an account is set to expire soon : Users : Contact your Argonne sponsor to request an extension. Argonne Sponsors : Have your division HR representative update the end date in Workday and ensure the job profile is \"Outside Collaborator.\" If an account has already expired : Users : Re-register at Argonne Collaborator Registration page using your existing username to retain access to previous data. Argonne Sponsors : Invite the collaborator to re-register via Cyber Gate Pass , and ensure they use their original username for continuity. LCRC Access Post-Argonne Appointment Transitioning to an External Collaborator : Apply for an Argonne Collaborator account using your sponsor's Argonne email address after your appointment ends. Cannot apply while the current Argonne appointment is active. If approved within 90 days of appointment expiration, contact LCRC support with old and new usernames for assistance in data access. Changes to Your Argonne Username If your Argonne username changes : Contact LCRC support with both your old and new usernames to update your user information on LCRC's end.","title":"Accounts and Access"},{"location":"account-project-management/accounts-and-access/#accounts-and-access","text":"","title":"Accounts and Access"},{"location":"account-project-management/accounts-and-access/#for-current-argonne-employees","text":"Existing Argonne Domain Account If you have an active badge, you likely have an account. It's the same account used for Argonne email and other apps. If you don't have an account or forgot your credentials, contact help@anl.gov or call the Argonne Service Desk at (630) 252-9999. LCRC support cannot directly address issues with your Argonne account. Accessing LCRC Clusters : Go to https://accounts.lcrc.anl.gov and log in with your Argonne credentials. Select Join Project on the left side. Search for the lcrc project and request membership. Instant membership for Argonne employees. You'll receive an email confirming LCRC access. Final Steps : Add a public SSH key ( Instructions ). Once the key is added, you can log in to the LCRC clusters.","title":"For Current Argonne Employees"},{"location":"account-project-management/accounts-and-access/#for-non-employeesoutside-collaborators","text":"Argonne Collaborator Account : Apply at https://apps.anl.gov/registration/collaborators . Requires official Argonne email address of your sponsor (must be an Argonne employee). Accounts are annual and renewable. Non-US citizens may experience a waiting period before approval. Upon approval, you'll receive an email notification. Sponsors can check account status at Cyber Gate Pass . Accessing LCRC Clusters : Join an existing LCRC project. Log in at https://accounts.lcrc.anl.gov using your Argonne Collaborator credentials. Click Join Project and search for the project indicated by your sponsor. Request membership and await approval from your sponsor or the project owner. Upon approval, you'll be added to the LCRC project and receive a home directory on the LCRC clusters. Final Steps : Add a public SSH key ( Instructions ). Once the key is added, you can log in to the LCRC clusters.","title":"For Non-Employees/Outside Collaborators"},{"location":"account-project-management/accounts-and-access/#managing-argonne-collaborator-accounts","text":"","title":"Managing Argonne Collaborator Accounts"},{"location":"account-project-management/accounts-and-access/#extend-or-reactivate-collaborator-accounts","text":"If an account is set to expire soon : Users : Contact your Argonne sponsor to request an extension. Argonne Sponsors : Have your division HR representative update the end date in Workday and ensure the job profile is \"Outside Collaborator.\" If an account has already expired : Users : Re-register at Argonne Collaborator Registration page using your existing username to retain access to previous data. Argonne Sponsors : Invite the collaborator to re-register via Cyber Gate Pass , and ensure they use their original username for continuity.","title":"Extend or Reactivate Collaborator Accounts"},{"location":"account-project-management/accounts-and-access/#lcrc-access-post-argonne-appointment","text":"Transitioning to an External Collaborator : Apply for an Argonne Collaborator account using your sponsor's Argonne email address after your appointment ends. Cannot apply while the current Argonne appointment is active. If approved within 90 days of appointment expiration, contact LCRC support with old and new usernames for assistance in data access.","title":"LCRC Access Post-Argonne Appointment"},{"location":"account-project-management/accounts-and-access/#changes-to-your-argonne-username","text":"If your Argonne username changes : Contact LCRC support with both your old and new usernames to update your user information on LCRC's end.","title":"Changes to Your Argonne Username"},{"location":"account-project-management/project-management/","text":"Managing Projects This page describes projects in detail, including policies, concepts, background, etc. The Project Life Cycle Projects normally go through the following life cycle. Detailed descriptions of the life cycle will be elaborated further down on this page. The Principal Investigator (PI) requests a project using at https://accounts.lcrc.anl.gov . The PI must be a current Argonne employee. The project request is sent to the LCRC Allocations Administrator and to the LCRC Allocations Committee. The committee's decision is reported via email to the project requester. If the project has been approved, the mail will include the number of hours allocated to the project. This number may be different from what was requested. This number may also be some initial allocation that will subsequently be augmented based on decisions by the LCRC Allocations Committee. As a part of creating the project, someone on the LCRC Allocations Committee will be affiliated with the project as a Point of Contact (PoC). That person will be the project's contact on the committee. They are expected to become moderately familiar with the project, to act as the project's advocate if necessary, and to be able to explain the project and the project's status to the committee. Every fiscal quarter starting October 1, all projects will have their allocations zeroed out and be restarted with new allocations as requested and approved. This will be done in order to adjust and balance the use of the system. Unused time does not carry over to the next quarter. When making quarterly decisions, the Allocations Committee PoC will look at the usage for the previous quarter and may query any projects that have not been using most of their time to learn what their plan is for the next quarter. Sometimes they also query projects that have used a lot of time for their future plans. Once the project has been created, PIs can add other users to the project, appoint Proxies (or Co-PIs), and administer the project's allocation . In advance of the annual October (Argonne's new fiscal year) re-allocation, PIs will be reminded to send in a request for the next year's allocation for their project. Also in advance of the annual October re-allocation, PIs will also be required to provide a report on their project at that time. If reports had been sent in for previous quarters, those will be sufficient, if they are current. The basic goal here is to get at least one annual report for each project. This report will be used to help describe the overall use of the systems and will also be used when deciding on annual allocations for all projects. The format of the report will be roughly 2-3 pages long, with pictures highly encouraged. Projects that do not submit a report will not receive any new time for the new fiscal year until a report is received. LCRC allocations granted each quarter use Argonne's fiscal calendar. Allocations are granted on the first business day of the new quarter around 10AM CST. Quarters are divided into the following time frames: 1st Quarter (October 1 \u2013 December 31) 2nd Quarter (January 1 \u2013 March 31) 3rd Quarter (April 1 \u2013 June 30) 4th Quarter (July 1 \u2013 September 30) Example Project: \"Popcorn\" Let's explore a sample project to illustrate how the LCRC allocation system functions. The project, named popcorn , aims to simulate Popcorn Kernel Dynamics. It involves a Principal Investigator (PI) and three collaborating scientists. Project Life Cycle & Initial Allocation After its creation, popcorn would be granted an initial allocation, for instance, 20,000 core-hours for the year . Usage Scenario Imagine one of the collaborating scientists runs a job that takes 10 hours and utilizes 25, 36-core nodes . This would consume: 10 hours (job duration) x 25 nodes x 36 cores (per node) = 9,000 core-hours As a result, 11,000 core-hours would remain from the original allocation. Running Out of Core-Hours Once all 11,000 remaining core-hours are exhausted, the project will be unable to execute additional jobs. At this point, the team would need to consider requesting more computational time. Requesting Additional Allocation The designated PI can formally request additional core-hours for the project. This request will be subject to review and approval by the LCRC Allocations Committee . Communication : The decision on the request will be emailed to the PI upon completion of the required web form. Additional Resources For a practical guide to project allocation requests, refer to the Sample Project Request . Project PIs and User Accounts Each project within the LCRC system is managed by at least one Primary Investigator (PI) . The PI serves as the main point of contact for matters like resource allocations. In addition, projects can have Proxies (also known as Co-PIs) who share managerial responsibilities with the PI. Eligibility for PIs and Proxies Primary Investigator (PI) : Must be a current Argonne employee. Proxies (Co-PIs) : Also need to be current Argonne employees. User Account Association A project can have various levels of user involvement: Single User : Some projects might only have one user account, which is the PI. Multiple Users : Other projects could involve multiple user accounts, possibly even dozens. A single user account can be associated with multiple projects and can also serve as the PI for multiple projects. Roles and Responsibilities PI Responsibilities User Management : The PI is in charge of adding and removing user accounts linked to the project. Allocation Management : The PI administers the core-hour allocations for the project, outlining who among the project members is authorized to use what portion of the allocated resources. Summary In a nutshell, the PI and any Proxies are responsible for both user and resource management within their projects. They are the go-to contacts for allocation decisions and administrative tasks. Requesting a New LCRC Project Who Can Request? Only Argonne employees are eligible to initiate a new project within the LCRC. Step-by-Step Guide 1. Log In: Visit the LCRC Accounts page . Use your Argonne credentials to log in. 2. Access the Request Link: Locate and click on the Request New LCRC Project option in the left-hand sidebar. Note : If the link isn't visible, join the lcrc project. Follow this guide for assistance. 3. Complete the Form: Provide the necessary details in the required fields. Click the Request project button upon completion. 4. Approval Process: Your project request will undergo a review cycle. Await an email notification regarding the approval decision. 5. Post-Approval Actions: Once approved and the project is set up, you can: Add or remove users and Proxies (Co-PIs). Request additional allocations. Edit project information. Your project will have a corresponding Unix group with the same name for member access. 6. Storage Requests: For storage requests exceeding 1TB, provide a justification. Join an Existing LCRC Project Who Can Join? Anyone with a new or existing LCRC account can request to join an existing project. If you are an Argonne employee, you should first join the lcrc project. However, if you are a collaborator, you will need to join a sub-project of LCRC. Steps to Join a Project 1. Contact the Project PI: For collaborators, if you already know the project PI within LCRC that you wish to join, contact them directly and request them to add you to the project. 2. Self-Initiate Membership: If you'd like to send a request to the project PI: a. Access LCRC Accounts: Visit the LCRC Accounts page . Log in using your Argonne credentials. b. Locate Join Project: Click on the Join Project option on the left sidebar. c. Search for the Project: Enter the name of the project you wish to join in the search box. The list will update to display projects matching your search term. d. Request Membership: Click on the desired project name from the list. Then click on the Request Membership button. 3. Approval: After the project owner approves your membership request, you'll have access to use the project hours for your tasks. Managing Your LCRC Project If you hold the role of a Project Owner (PI) or a Proxy (Co-PI), you're empowered to manage and tailor your LCRC project. Follow this structured guide: 1. Access the LCRC Account Page : Visit the LCRC Accounts page . Sign in with your Argonne credentials. 2. Find Your Project : Go to Projects > Owned on the left sidebar. Select the project you'd like to oversee. 3. Project Management Options : Within the project management dashboard, you can: Add or remove project members. Handle pending membership requests. Designate or exclude Proxies (Co-PIs). Seek more project allocations or storage. Update project information. 4. Save Your Adjustments : Remember to hit the Save Project info button post-modifications. For support or inquiries, email support@lcrc.anl.gov . Mid-Quarter Allocations Overview Mid-Quarter Allocations can be granted for LCRC projects after not previously requesting time or exhausting a current allocation. Principal Investigators (PIs) need to understand the guidelines and requirements for requesting additional computational resources. Allocation Planning and Timing Mid-quarter allocations are made only once per project per quarter. PIs should ensure their allocated hours last the entire quarter. LCRC reviews mid-quarter allocation requests once a week. How to Request Additional Allocations Check Current Usage : Verify usage of allocated resources using lcrc-usage <project_name> on Bebop/Swing and sbank-list-allocations -p <project_name> on Improv. Prepare Documentation : Justify core-hour requirements, guided by the Sample Project Request PDF . Special Criteria for Large Requests : Provide scaling results for requests exceeding 100K core-hours (781 node hours). Request Time : Request the additional hours following our Managing Your LCRC Project documentation above. Note : Requests with incomplete or unclear information may result in a delay of up to two weeks in the decision-making process. Allocation Limits and Calculation Examples Maximum request: 250K core-hours (2,000 node-hours on Improv) or half of the initial allocation , whichever is less. This amount is pro-rated based on the remaining time in the quarter. Examples Initial Allocation of 600K Core-Hours : If you request an additional 500K core-hours 6 weeks into a 12-week quarter, you may be granted up to 125K additional core-hours . This is calculated as half of the maximum requestable amount, since half the quarter has passed ( 250K * 6/12 ). Initial Allocation of 200K Core-Hours : If you request an additional 300K core-hours after 7 weeks into the quarter, the maximum you may receive is approximately 58K additional core-hours . This is calculated by taking half of the initial allocation, prorated for the remaining 5 weeks of the quarter ( 200K * 0.5 * 5/12 ). Other Important Notes No allocations are made within two weeks before the quarter's end Project renewals are at the start of the fiscal year . Failing to renew before the allocation deadline results in postponement to the following quarter (Q2 onwards) . For large core-hour needs or high usage rates, consider applying through the Director's Discretionary Allocation Program in ALCF.","title":"Managing Projects"},{"location":"account-project-management/project-management/#managing-projects","text":"This page describes projects in detail, including policies, concepts, background, etc.","title":"Managing Projects"},{"location":"account-project-management/project-management/#the-project-life-cycle","text":"Projects normally go through the following life cycle. Detailed descriptions of the life cycle will be elaborated further down on this page. The Principal Investigator (PI) requests a project using at https://accounts.lcrc.anl.gov . The PI must be a current Argonne employee. The project request is sent to the LCRC Allocations Administrator and to the LCRC Allocations Committee. The committee's decision is reported via email to the project requester. If the project has been approved, the mail will include the number of hours allocated to the project. This number may be different from what was requested. This number may also be some initial allocation that will subsequently be augmented based on decisions by the LCRC Allocations Committee. As a part of creating the project, someone on the LCRC Allocations Committee will be affiliated with the project as a Point of Contact (PoC). That person will be the project's contact on the committee. They are expected to become moderately familiar with the project, to act as the project's advocate if necessary, and to be able to explain the project and the project's status to the committee. Every fiscal quarter starting October 1, all projects will have their allocations zeroed out and be restarted with new allocations as requested and approved. This will be done in order to adjust and balance the use of the system. Unused time does not carry over to the next quarter. When making quarterly decisions, the Allocations Committee PoC will look at the usage for the previous quarter and may query any projects that have not been using most of their time to learn what their plan is for the next quarter. Sometimes they also query projects that have used a lot of time for their future plans. Once the project has been created, PIs can add other users to the project, appoint Proxies (or Co-PIs), and administer the project's allocation . In advance of the annual October (Argonne's new fiscal year) re-allocation, PIs will be reminded to send in a request for the next year's allocation for their project. Also in advance of the annual October re-allocation, PIs will also be required to provide a report on their project at that time. If reports had been sent in for previous quarters, those will be sufficient, if they are current. The basic goal here is to get at least one annual report for each project. This report will be used to help describe the overall use of the systems and will also be used when deciding on annual allocations for all projects. The format of the report will be roughly 2-3 pages long, with pictures highly encouraged. Projects that do not submit a report will not receive any new time for the new fiscal year until a report is received. LCRC allocations granted each quarter use Argonne's fiscal calendar. Allocations are granted on the first business day of the new quarter around 10AM CST. Quarters are divided into the following time frames: 1st Quarter (October 1 \u2013 December 31) 2nd Quarter (January 1 \u2013 March 31) 3rd Quarter (April 1 \u2013 June 30) 4th Quarter (July 1 \u2013 September 30)","title":"The Project Life Cycle"},{"location":"account-project-management/project-management/#example-project-popcorn","text":"Let's explore a sample project to illustrate how the LCRC allocation system functions. The project, named popcorn , aims to simulate Popcorn Kernel Dynamics. It involves a Principal Investigator (PI) and three collaborating scientists.","title":"Example Project: \"Popcorn\""},{"location":"account-project-management/project-management/#project-life-cycle-initial-allocation","text":"After its creation, popcorn would be granted an initial allocation, for instance, 20,000 core-hours for the year .","title":"Project Life Cycle &amp; Initial Allocation"},{"location":"account-project-management/project-management/#usage-scenario","text":"Imagine one of the collaborating scientists runs a job that takes 10 hours and utilizes 25, 36-core nodes . This would consume: 10 hours (job duration) x 25 nodes x 36 cores (per node) = 9,000 core-hours As a result, 11,000 core-hours would remain from the original allocation.","title":"Usage Scenario"},{"location":"account-project-management/project-management/#running-out-of-core-hours","text":"Once all 11,000 remaining core-hours are exhausted, the project will be unable to execute additional jobs. At this point, the team would need to consider requesting more computational time.","title":"Running Out of Core-Hours"},{"location":"account-project-management/project-management/#requesting-additional-allocation","text":"The designated PI can formally request additional core-hours for the project. This request will be subject to review and approval by the LCRC Allocations Committee . Communication : The decision on the request will be emailed to the PI upon completion of the required web form.","title":"Requesting Additional Allocation"},{"location":"account-project-management/project-management/#additional-resources","text":"For a practical guide to project allocation requests, refer to the Sample Project Request .","title":"Additional Resources"},{"location":"account-project-management/project-management/#project-pis-and-user-accounts","text":"Each project within the LCRC system is managed by at least one Primary Investigator (PI) . The PI serves as the main point of contact for matters like resource allocations. In addition, projects can have Proxies (also known as Co-PIs) who share managerial responsibilities with the PI.","title":"Project PIs and User Accounts"},{"location":"account-project-management/project-management/#eligibility-for-pis-and-proxies","text":"Primary Investigator (PI) : Must be a current Argonne employee. Proxies (Co-PIs) : Also need to be current Argonne employees.","title":"Eligibility for PIs and Proxies"},{"location":"account-project-management/project-management/#user-account-association","text":"A project can have various levels of user involvement: Single User : Some projects might only have one user account, which is the PI. Multiple Users : Other projects could involve multiple user accounts, possibly even dozens. A single user account can be associated with multiple projects and can also serve as the PI for multiple projects.","title":"User Account Association"},{"location":"account-project-management/project-management/#roles-and-responsibilities","text":"","title":"Roles and Responsibilities"},{"location":"account-project-management/project-management/#pi-responsibilities","text":"User Management : The PI is in charge of adding and removing user accounts linked to the project. Allocation Management : The PI administers the core-hour allocations for the project, outlining who among the project members is authorized to use what portion of the allocated resources.","title":"PI Responsibilities"},{"location":"account-project-management/project-management/#summary","text":"In a nutshell, the PI and any Proxies are responsible for both user and resource management within their projects. They are the go-to contacts for allocation decisions and administrative tasks.","title":"Summary"},{"location":"account-project-management/project-management/#requesting-a-new-lcrc-project","text":"","title":"Requesting a New LCRC Project"},{"location":"account-project-management/project-management/#who-can-request","text":"Only Argonne employees are eligible to initiate a new project within the LCRC.","title":"Who Can Request?"},{"location":"account-project-management/project-management/#step-by-step-guide","text":"1. Log In: Visit the LCRC Accounts page . Use your Argonne credentials to log in. 2. Access the Request Link: Locate and click on the Request New LCRC Project option in the left-hand sidebar. Note : If the link isn't visible, join the lcrc project. Follow this guide for assistance. 3. Complete the Form: Provide the necessary details in the required fields. Click the Request project button upon completion. 4. Approval Process: Your project request will undergo a review cycle. Await an email notification regarding the approval decision. 5. Post-Approval Actions: Once approved and the project is set up, you can: Add or remove users and Proxies (Co-PIs). Request additional allocations. Edit project information. Your project will have a corresponding Unix group with the same name for member access. 6. Storage Requests: For storage requests exceeding 1TB, provide a justification.","title":"Step-by-Step Guide"},{"location":"account-project-management/project-management/#join-an-existing-lcrc-project","text":"","title":"Join an Existing LCRC Project"},{"location":"account-project-management/project-management/#who-can-join","text":"Anyone with a new or existing LCRC account can request to join an existing project. If you are an Argonne employee, you should first join the lcrc project. However, if you are a collaborator, you will need to join a sub-project of LCRC.","title":"Who Can Join?"},{"location":"account-project-management/project-management/#steps-to-join-a-project","text":"1. Contact the Project PI: For collaborators, if you already know the project PI within LCRC that you wish to join, contact them directly and request them to add you to the project. 2. Self-Initiate Membership: If you'd like to send a request to the project PI: a. Access LCRC Accounts: Visit the LCRC Accounts page . Log in using your Argonne credentials. b. Locate Join Project: Click on the Join Project option on the left sidebar. c. Search for the Project: Enter the name of the project you wish to join in the search box. The list will update to display projects matching your search term. d. Request Membership: Click on the desired project name from the list. Then click on the Request Membership button. 3. Approval: After the project owner approves your membership request, you'll have access to use the project hours for your tasks.","title":"Steps to Join a Project"},{"location":"account-project-management/project-management/#managing-your-lcrc-project","text":"If you hold the role of a Project Owner (PI) or a Proxy (Co-PI), you're empowered to manage and tailor your LCRC project. Follow this structured guide: 1. Access the LCRC Account Page : Visit the LCRC Accounts page . Sign in with your Argonne credentials. 2. Find Your Project : Go to Projects > Owned on the left sidebar. Select the project you'd like to oversee. 3. Project Management Options : Within the project management dashboard, you can: Add or remove project members. Handle pending membership requests. Designate or exclude Proxies (Co-PIs). Seek more project allocations or storage. Update project information. 4. Save Your Adjustments : Remember to hit the Save Project info button post-modifications. For support or inquiries, email support@lcrc.anl.gov .","title":"Managing Your LCRC Project"},{"location":"account-project-management/project-management/#mid-quarter-allocations","text":"","title":"Mid-Quarter Allocations"},{"location":"account-project-management/project-management/#overview","text":"Mid-Quarter Allocations can be granted for LCRC projects after not previously requesting time or exhausting a current allocation. Principal Investigators (PIs) need to understand the guidelines and requirements for requesting additional computational resources.","title":"Overview"},{"location":"account-project-management/project-management/#allocation-planning-and-timing","text":"Mid-quarter allocations are made only once per project per quarter. PIs should ensure their allocated hours last the entire quarter. LCRC reviews mid-quarter allocation requests once a week.","title":"Allocation Planning and Timing"},{"location":"account-project-management/project-management/#how-to-request-additional-allocations","text":"Check Current Usage : Verify usage of allocated resources using lcrc-usage <project_name> on Bebop/Swing and sbank-list-allocations -p <project_name> on Improv. Prepare Documentation : Justify core-hour requirements, guided by the Sample Project Request PDF . Special Criteria for Large Requests : Provide scaling results for requests exceeding 100K core-hours (781 node hours). Request Time : Request the additional hours following our Managing Your LCRC Project documentation above. Note : Requests with incomplete or unclear information may result in a delay of up to two weeks in the decision-making process.","title":"How to Request Additional Allocations"},{"location":"account-project-management/project-management/#allocation-limits-and-calculation-examples","text":"Maximum request: 250K core-hours (2,000 node-hours on Improv) or half of the initial allocation , whichever is less. This amount is pro-rated based on the remaining time in the quarter. Examples Initial Allocation of 600K Core-Hours : If you request an additional 500K core-hours 6 weeks into a 12-week quarter, you may be granted up to 125K additional core-hours . This is calculated as half of the maximum requestable amount, since half the quarter has passed ( 250K * 6/12 ). Initial Allocation of 200K Core-Hours : If you request an additional 300K core-hours after 7 weeks into the quarter, the maximum you may receive is approximately 58K additional core-hours . This is calculated by taking half of the initial allocation, prorated for the remaining 5 weeks of the quarter ( 200K * 0.5 * 5/12 ).","title":"Allocation Limits and Calculation Examples"},{"location":"account-project-management/project-management/#other-important-notes","text":"No allocations are made within two weeks before the quarter's end Project renewals are at the start of the fiscal year . Failing to renew before the allocation deadline results in postponement to the following quarter (Q2 onwards) . For large core-hour needs or high usage rates, consider applying through the Director's Discretionary Allocation Program in ALCF.","title":"Other Important Notes"},{"location":"account-project-management/ssh/","text":"Logging In and SSH Keys Creating SSH Keys Generate a pair of SSH keys with the ssh-keygen command; in this example we create an ed25519 key. $ ssh-keygen -a 100 -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/Users/USER/.ssh/id_ed25519): You can change the name of the ssh key here, which is useful if you have several keys. Make note of the name if you use a different one other than the default. We will presume you went with the defaults here. Note: If you choose a different name you will need to specify the key to use in the ssh config file or in the ssh command itself. Enter passphrase (empty for no passphrase): A strong passphrase is required Enter same passphrase again: Repeat the strong passphrase. Your identification has been saved in /Users/USER/.ssh/id_ed25519. ( id_ed25519 is your private key. Keep it secure, do not share it, and be cognizant of where you store it.) Your public key has been saved in /Users/USER/.ssh/id_ed25519.pub. The key fingerprint is: SHA256:lSglfiIzcdJumCtz1RI03sulFJ3pA3hZcFMbPPEY14Y USER@foo This is your public key. Adding Your Public Key to Your Account After you generate your SSH key pair, add ONLY the public key to your account at https://accounts.lcrc.anl.gov . Copy the contents of the .pub file generated above (in the example above, ~/.ssh/id_ed25519.pub ) Choose \u201cAdd Key\u201d at the bottom of the Account Information page after login. Paste what you copied into the \u201cKey\u201d section that appears. The \u201cDescription\u201d is purely informational and allows you to give it a meaningful or memorable name/description for future reference. You will then be able to SSH directly to our login nodes. Logging In LCRC supports multiple clusters, and you can adapt the following instructions to your specific cluster needs. Each cluster utilizes the same SSH keypair and file systems. Basic Configuration When using OpenSSH for logging in, it must know at least the hostname you wish to connect to. If your local username differs from your LCRC cluster username, you'll need to specify the correct one. Also, if your SSH key isn't the default ~/.ssh/ed_25519 , specify its path. ssh -i /path/to/your/ssh_private_key <username>@improv.lcrc.anl.gov Simplifying Future Logins OpenSSH allows you to store configurations in ~/.ssh/config , making logins simpler. In ~/.ssh/config , you can set parameters for various hosts. For example, to simplify logging in to Improv, you can add the following to your ~/.ssh/config file: Host improv HostName improv.lcrc.anl.gov User <username> IdentityFile /path/to/your/ssh_private_key After configuring OpenSSH, log in using: ssh improv You can apply this method to other clusters as well. Using GUI Applications Over SSH For GUI applications over SSH, enable X11 forwarding: Command line: ssh -X -i /path/to/your/ssh_private_key <username>@improv.lcrc.anl.gov Or in your config file: Host improv HostName improv.lcrc.anl.gov User username IdentityFile /path/to/your/ssh_private_key ForwardX11 yes Then connect as usual: ssh improv Debugging a Failed Connection If you encounter login issues, check your internet connection, ensure the correct public key is uploaded, verify your username, hostname, and SSH private key are correct, and check for any misconfigurations in ~/.ssh/config . Common Issues: Client Configuration: Verify the correct permissions for your SSH files. Server Configuration: Ensure your home and .ssh directories have the correct permissions. Maintenance and Login Nodes: Be aware of maintenance periods and check if you're connecting to responsive nodes Advanced Troubleshooting If you're still facing issues, provide the output from the following commands to support : ssh -vvv -i /path/to/your/ssh_private_key <username>@improv.lcrc.anl.gov ls -la ~/.ssh cat ~/.ssh/config Downed Login Nodes Occasionally LCRC login nodes may become unresponsive due to a number of various failures unexpectedly. Because of the round-robin nature of the connection, you may land on one of these bad nodes when you try to SSH to LCRC. We'll try to make sure each node is up at all times, but you may attempt to connect to the clusters during this unexpected down time. To test whether or not the problem is on your end or on the LCRC side and if you've already exhausted the other troubleshooting steps, try to connect to a specific login node. To do this, you can substitute your basic SSH command of ssh -i /path/to/your/ssh_private_key <username>@improv.lcrc.anl.gov , for example, with these instead. For Improv: ssh -i /path/to/your/ssh_private_key <username>@ilogin1.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@ilogin2.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@ilogin3.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@ilogin4.lcrc.anl.gov For Bebop: ssh -i /path/to/your/ssh_private_key <username>@beboplogin1.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@beboplogin2.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@beboplogin3.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@beboplogin4.lcrc.anl.gov If you try a couple of these nodes and still can't connect, you can continue troubleshooting. Of course in extremely rare cases most of our login nodes can be down so you can always contact us if you've exhausted all of your connection options.","title":"SSH"},{"location":"account-project-management/ssh/#logging-in-and-ssh-keys","text":"","title":"Logging In and SSH Keys"},{"location":"account-project-management/ssh/#creating-ssh-keys","text":"Generate a pair of SSH keys with the ssh-keygen command; in this example we create an ed25519 key. $ ssh-keygen -a 100 -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/Users/USER/.ssh/id_ed25519): You can change the name of the ssh key here, which is useful if you have several keys. Make note of the name if you use a different one other than the default. We will presume you went with the defaults here. Note: If you choose a different name you will need to specify the key to use in the ssh config file or in the ssh command itself. Enter passphrase (empty for no passphrase): A strong passphrase is required Enter same passphrase again: Repeat the strong passphrase. Your identification has been saved in /Users/USER/.ssh/id_ed25519. ( id_ed25519 is your private key. Keep it secure, do not share it, and be cognizant of where you store it.) Your public key has been saved in /Users/USER/.ssh/id_ed25519.pub. The key fingerprint is: SHA256:lSglfiIzcdJumCtz1RI03sulFJ3pA3hZcFMbPPEY14Y USER@foo This is your public key.","title":"Creating SSH Keys"},{"location":"account-project-management/ssh/#adding-your-public-key-to-your-account","text":"After you generate your SSH key pair, add ONLY the public key to your account at https://accounts.lcrc.anl.gov . Copy the contents of the .pub file generated above (in the example above, ~/.ssh/id_ed25519.pub ) Choose \u201cAdd Key\u201d at the bottom of the Account Information page after login. Paste what you copied into the \u201cKey\u201d section that appears. The \u201cDescription\u201d is purely informational and allows you to give it a meaningful or memorable name/description for future reference. You will then be able to SSH directly to our login nodes.","title":"Adding Your Public Key to Your Account"},{"location":"account-project-management/ssh/#logging-in","text":"LCRC supports multiple clusters, and you can adapt the following instructions to your specific cluster needs. Each cluster utilizes the same SSH keypair and file systems.","title":"Logging In"},{"location":"account-project-management/ssh/#basic-configuration","text":"When using OpenSSH for logging in, it must know at least the hostname you wish to connect to. If your local username differs from your LCRC cluster username, you'll need to specify the correct one. Also, if your SSH key isn't the default ~/.ssh/ed_25519 , specify its path. ssh -i /path/to/your/ssh_private_key <username>@improv.lcrc.anl.gov","title":"Basic Configuration"},{"location":"account-project-management/ssh/#simplifying-future-logins","text":"OpenSSH allows you to store configurations in ~/.ssh/config , making logins simpler. In ~/.ssh/config , you can set parameters for various hosts. For example, to simplify logging in to Improv, you can add the following to your ~/.ssh/config file: Host improv HostName improv.lcrc.anl.gov User <username> IdentityFile /path/to/your/ssh_private_key After configuring OpenSSH, log in using: ssh improv You can apply this method to other clusters as well.","title":"Simplifying Future Logins"},{"location":"account-project-management/ssh/#using-gui-applications-over-ssh","text":"For GUI applications over SSH, enable X11 forwarding: Command line: ssh -X -i /path/to/your/ssh_private_key <username>@improv.lcrc.anl.gov Or in your config file: Host improv HostName improv.lcrc.anl.gov User username IdentityFile /path/to/your/ssh_private_key ForwardX11 yes Then connect as usual: ssh improv","title":"Using GUI Applications Over SSH"},{"location":"account-project-management/ssh/#debugging-a-failed-connection","text":"If you encounter login issues, check your internet connection, ensure the correct public key is uploaded, verify your username, hostname, and SSH private key are correct, and check for any misconfigurations in ~/.ssh/config . Common Issues: Client Configuration: Verify the correct permissions for your SSH files. Server Configuration: Ensure your home and .ssh directories have the correct permissions. Maintenance and Login Nodes: Be aware of maintenance periods and check if you're connecting to responsive nodes Advanced Troubleshooting If you're still facing issues, provide the output from the following commands to support : ssh -vvv -i /path/to/your/ssh_private_key <username>@improv.lcrc.anl.gov ls -la ~/.ssh cat ~/.ssh/config","title":"Debugging a Failed Connection"},{"location":"account-project-management/ssh/#downed-login-nodes","text":"Occasionally LCRC login nodes may become unresponsive due to a number of various failures unexpectedly. Because of the round-robin nature of the connection, you may land on one of these bad nodes when you try to SSH to LCRC. We'll try to make sure each node is up at all times, but you may attempt to connect to the clusters during this unexpected down time. To test whether or not the problem is on your end or on the LCRC side and if you've already exhausted the other troubleshooting steps, try to connect to a specific login node. To do this, you can substitute your basic SSH command of ssh -i /path/to/your/ssh_private_key <username>@improv.lcrc.anl.gov , for example, with these instead. For Improv: ssh -i /path/to/your/ssh_private_key <username>@ilogin1.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@ilogin2.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@ilogin3.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@ilogin4.lcrc.anl.gov For Bebop: ssh -i /path/to/your/ssh_private_key <username>@beboplogin1.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@beboplogin2.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@beboplogin3.lcrc.anl.gov ssh -i /path/to/your/ssh_private_key <username>@beboplogin4.lcrc.anl.gov If you try a couple of these nodes and still can't connect, you can continue troubleshooting. Of course in extremely rare cases most of our login nodes can be down so you can always contact us if you've exhausted all of your connection options.","title":"Downed Login Nodes"},{"location":"allocation-management/allocations/","text":"Allocations on LCRC Computing Resources Overview of LCRC Clusters The LCRC operates three distinct clusters, each with its own scheduling and allocation systems: Improv : Operates with the PBS Pro job scheduler and measures allocations in node hours . Bebop : Utilizes the Slurm job scheduler and measures allocations in core hours . Swing : Also uses Slurm but charges allocations in GPU hours . Allocations Metrics for Each Cluster GPU Hours (Swing Cluster) Unlike other LCRC clusters, Swing charges time based on GPU hours instead of CPU core hours. You need to factor this in when applying for time on Swing. On Swing, the compute nodes charge as follows for each job: GPU Hours = Number of Nodes Used x GPUs Per Node x Time in Hours Number of Nodes Used : The number of compute nodes employed for the job. GPUs per Node : How many GPUs used in each node. Time in Hours : The duration for which these nodes and GPUs are used. Core Hours (Bebop Cluster) Core Hours = Number of Nodes Used x Cores per Node x Time in Hours Number of Nodes Used : How many separate computing nodes are being used. Cores per Node : Number of CPU cores in each node. Time in Hours : Duration for which these nodes (and therefore the cores within them) are used. Node Hours (Improv Cluster) Allocations on Improv are provided (and should be requested) in Node Hours. 1 node on Improv has 128 CPU Cores. When requesting or viewing your allocation(s), please take this into consideration. Balances, transactions and other sbank details displayed from sbank commands will update every 5 minutes. Node Hours = Number of Nodes Used \u00d7 Time in Hours Number of Nodes Used : The quantity of compute nodes utilized for the job. Time in Hours : The duration for which these nodes are used. Improv currently allows for an Overburn of a project allocation. Each allocation is allowed to use 10% over the granted allocation before getting an error message. Once the project allocation is exhausted, the following error message will be displayed when submitting a job: qsub: Job violates queue and/or server resource limits Projects will need to Request Additional Project Time when allocations are exhausted. Startup Projects and Allocations Startup Projects Overview New Argonne employee accounts receive a \"startup project\" with a small balance of hours on the Bebop and Swing systems. Purpose The startup project serves multiple functions: Training Ground : Familiarize yourself with system operations. Idea Incubator : Develop and test new project concepts. User Discretion : Use the hours for any purpose you see fit. Allocation Details Initial Allocation : 20,000 core-hours on Bebop, 100 GPU hours on Swing. Expiration : None. The startup hours do not have a use-by date. Additional Time : Startup projects are not eligible for additional allocations beyond the initial hours. Usage Guidelines Exclusive Usage : The startup project should only be used by the account originally associated with it. Transition to Full Project : If your needs exceed the startup hours, you should either apply for your own dedicated project or join an existing one. Special Note for Non-Argonne Employees Non-Argonne staff must be part of another active project led by an Argonne PI. Slurm Default Account Your startup project is set as your default project in the Slurm job scheduler. To submit jobs against your startup allocation, use the following account name format: startup-<username> . Allocation Usage and Tracking General Policies System Error : If a system error occurs that causes a program to crash while it is running, a project won\u2019t be charged for that time. (This policy may be amended in the future in order to promote the use of user-based checkpointing.) The scheduler may or may not deduct the time used from the project\u2019s allocation, depending on how the crash took place. If someone thinks their project should be credited time because of a system crash or other system problem, they should send email to support@lcrc.anl.gov to get that time back into the project. Run Continuation :If a project runs out of allocation time during a run, that run will be allowed to continue to completion. Project Suspension : If a project runs out of allocation time, all users on the project will not be able to run any jobs until time is added again. At the present time, no steps will be taken to stop any jobs associated with that project from running and sitting idle in the job queue. Usage Tracking Requesting Additional Project Time When to Request Additional Time Projects at LCRC may deplete their quarterly time allocation sooner than expected due to unforeseen computational demands or additional case studies. In these instances, Principal Investigators (PIs) have the option to request extra time. How to Request PIs can submit requests for additional project time through the LCRC Accounts page by accessing the project management screens. Requests must include proper justification for the additional time needed. Review Process The LCRC core team convenes every Tuesday to assess time requests for both new and ongoing projects. Additional time can be granted up to 150K core-hours directly by the LCRC core team. Requests exceeding 150K core-hours require approval from the LCRC allocations committee. Expiration of Additional Time PIs should note that any extra time granted but not used within the quarter will expire at the quarter's end. For example, if a PI is allocated an extra 100K core-hours and only utilizes 40K, the remaining 60K core-hours will be forfeited at the quarter's conclusion. Annual Time Requests for Projects Existing Projects : Time requests for the upcoming fiscal year can be made from the first week of September until October 1st. New Projects : New projects may request time throughout the year for the remaining quarters. Post-October 1st : After October 1st, existing projects can request time for any quarter, but there may be restrictions on the maximum hours granted. Mid-Quarter Allocations Mid-Quarter Allocations can be granted for LCRC projects after not previously requesting time or exhausting a current allocation. Principal Investigators (PIs) need to understand the guidelines and requirements for requesting additional computational resources. We have described, in detail, this process on the Managing Projects documentation.","title":"Allocations on LCRC Computing Resources"},{"location":"allocation-management/allocations/#allocations-on-lcrc-computing-resources","text":"","title":"Allocations on LCRC Computing Resources"},{"location":"allocation-management/allocations/#overview-of-lcrc-clusters","text":"The LCRC operates three distinct clusters, each with its own scheduling and allocation systems: Improv : Operates with the PBS Pro job scheduler and measures allocations in node hours . Bebop : Utilizes the Slurm job scheduler and measures allocations in core hours . Swing : Also uses Slurm but charges allocations in GPU hours .","title":"Overview of LCRC Clusters"},{"location":"allocation-management/allocations/#allocations-metrics-for-each-cluster","text":"","title":"Allocations Metrics for Each Cluster"},{"location":"allocation-management/allocations/#gpu-hours-swing-cluster","text":"Unlike other LCRC clusters, Swing charges time based on GPU hours instead of CPU core hours. You need to factor this in when applying for time on Swing. On Swing, the compute nodes charge as follows for each job: GPU Hours = Number of Nodes Used x GPUs Per Node x Time in Hours Number of Nodes Used : The number of compute nodes employed for the job. GPUs per Node : How many GPUs used in each node. Time in Hours : The duration for which these nodes and GPUs are used.","title":"GPU Hours (Swing Cluster)"},{"location":"allocation-management/allocations/#core-hours-bebop-cluster","text":"Core Hours = Number of Nodes Used x Cores per Node x Time in Hours Number of Nodes Used : How many separate computing nodes are being used. Cores per Node : Number of CPU cores in each node. Time in Hours : Duration for which these nodes (and therefore the cores within them) are used.","title":"Core Hours (Bebop Cluster)"},{"location":"allocation-management/allocations/#node-hours-improv-cluster","text":"Allocations on Improv are provided (and should be requested) in Node Hours. 1 node on Improv has 128 CPU Cores. When requesting or viewing your allocation(s), please take this into consideration. Balances, transactions and other sbank details displayed from sbank commands will update every 5 minutes. Node Hours = Number of Nodes Used \u00d7 Time in Hours Number of Nodes Used : The quantity of compute nodes utilized for the job. Time in Hours : The duration for which these nodes are used. Improv currently allows for an Overburn of a project allocation. Each allocation is allowed to use 10% over the granted allocation before getting an error message. Once the project allocation is exhausted, the following error message will be displayed when submitting a job: qsub: Job violates queue and/or server resource limits Projects will need to Request Additional Project Time when allocations are exhausted.","title":"Node Hours (Improv Cluster)"},{"location":"allocation-management/allocations/#startup-projects-and-allocations","text":"","title":"Startup Projects and Allocations"},{"location":"allocation-management/allocations/#startup-projects-overview","text":"New Argonne employee accounts receive a \"startup project\" with a small balance of hours on the Bebop and Swing systems.","title":"Startup Projects Overview"},{"location":"allocation-management/allocations/#purpose","text":"The startup project serves multiple functions: Training Ground : Familiarize yourself with system operations. Idea Incubator : Develop and test new project concepts. User Discretion : Use the hours for any purpose you see fit.","title":"Purpose"},{"location":"allocation-management/allocations/#allocation-details","text":"Initial Allocation : 20,000 core-hours on Bebop, 100 GPU hours on Swing. Expiration : None. The startup hours do not have a use-by date. Additional Time : Startup projects are not eligible for additional allocations beyond the initial hours.","title":"Allocation Details"},{"location":"allocation-management/allocations/#usage-guidelines","text":"Exclusive Usage : The startup project should only be used by the account originally associated with it. Transition to Full Project : If your needs exceed the startup hours, you should either apply for your own dedicated project or join an existing one.","title":"Usage Guidelines"},{"location":"allocation-management/allocations/#special-note-for-non-argonne-employees","text":"Non-Argonne staff must be part of another active project led by an Argonne PI.","title":"Special Note for Non-Argonne Employees"},{"location":"allocation-management/allocations/#slurm-default-account","text":"Your startup project is set as your default project in the Slurm job scheduler. To submit jobs against your startup allocation, use the following account name format: startup-<username> .","title":"Slurm Default Account"},{"location":"allocation-management/allocations/#allocation-usage-and-tracking","text":"","title":"Allocation Usage and Tracking"},{"location":"allocation-management/allocations/#general-policies","text":"System Error : If a system error occurs that causes a program to crash while it is running, a project won\u2019t be charged for that time. (This policy may be amended in the future in order to promote the use of user-based checkpointing.) The scheduler may or may not deduct the time used from the project\u2019s allocation, depending on how the crash took place. If someone thinks their project should be credited time because of a system crash or other system problem, they should send email to support@lcrc.anl.gov to get that time back into the project. Run Continuation :If a project runs out of allocation time during a run, that run will be allowed to continue to completion. Project Suspension : If a project runs out of allocation time, all users on the project will not be able to run any jobs until time is added again. At the present time, no steps will be taken to stop any jobs associated with that project from running and sitting idle in the job queue. Usage Tracking","title":"General Policies"},{"location":"allocation-management/allocations/#requesting-additional-project-time","text":"","title":"Requesting Additional Project Time"},{"location":"allocation-management/allocations/#when-to-request-additional-time","text":"Projects at LCRC may deplete their quarterly time allocation sooner than expected due to unforeseen computational demands or additional case studies. In these instances, Principal Investigators (PIs) have the option to request extra time.","title":"When to Request Additional Time"},{"location":"allocation-management/allocations/#how-to-request","text":"PIs can submit requests for additional project time through the LCRC Accounts page by accessing the project management screens. Requests must include proper justification for the additional time needed.","title":"How to Request"},{"location":"allocation-management/allocations/#review-process","text":"The LCRC core team convenes every Tuesday to assess time requests for both new and ongoing projects. Additional time can be granted up to 150K core-hours directly by the LCRC core team. Requests exceeding 150K core-hours require approval from the LCRC allocations committee.","title":"Review Process"},{"location":"allocation-management/allocations/#expiration-of-additional-time","text":"PIs should note that any extra time granted but not used within the quarter will expire at the quarter's end. For example, if a PI is allocated an extra 100K core-hours and only utilizes 40K, the remaining 60K core-hours will be forfeited at the quarter's conclusion.","title":"Expiration of Additional Time"},{"location":"allocation-management/allocations/#annual-time-requests-for-projects","text":"Existing Projects : Time requests for the upcoming fiscal year can be made from the first week of September until October 1st. New Projects : New projects may request time throughout the year for the remaining quarters. Post-October 1st : After October 1st, existing projects can request time for any quarter, but there may be restrictions on the maximum hours granted.","title":"Annual Time Requests for Projects"},{"location":"allocation-management/allocations/#mid-quarter-allocations","text":"Mid-Quarter Allocations can be granted for LCRC projects after not previously requesting time or exhausting a current allocation. Principal Investigators (PIs) need to understand the guidelines and requirements for requesting additional computational resources. We have described, in detail, this process on the Managing Projects documentation.","title":"Mid-Quarter Allocations"},{"location":"allocation-management/lcrc-sbank-allocation-accounting-system/","text":"lcrc-sbank Allocation Accounting System (Slurm Clusters) The lcrc-sbank commands are specifically designed for clusters that use Slurm. Make sure you're using these commands on the following LCRC clusters: Bebop Swing Project Allocation Queries and Management The commands listed below are essential for LCRC users to manage their project allocations on Slurm clusters. These tools allow you to query project balances, transactions, and set or change your default project. Remember, commands are cluster-specific and will only provide information for the cluster currently being accessed. Command Description lcrc-sbank -q balance Query all of your LCRC project balances. lcrc-sbank -q balance <proj_name> Query a specific LCRC project balance. lcrc-sbank -q default Query your default LCRC project. lcrc-sbank -s default <proj_name> Change your default LCRC project. lcrc-sbank -q trans <proj_name> Query all transactions on an LCRC project. lcrc-sbank -h Print the help menu for the lcrc-sbank command.","title":"lcrc-sbank Allocation Accounting System (Slurm Clusters)"},{"location":"allocation-management/lcrc-sbank-allocation-accounting-system/#lcrc-sbank-allocation-accounting-system-slurm-clusters","text":"The lcrc-sbank commands are specifically designed for clusters that use Slurm. Make sure you're using these commands on the following LCRC clusters: Bebop Swing","title":"lcrc-sbank Allocation Accounting System (Slurm Clusters)"},{"location":"allocation-management/lcrc-sbank-allocation-accounting-system/#project-allocation-queries-and-management","text":"The commands listed below are essential for LCRC users to manage their project allocations on Slurm clusters. These tools allow you to query project balances, transactions, and set or change your default project. Remember, commands are cluster-specific and will only provide information for the cluster currently being accessed. Command Description lcrc-sbank -q balance Query all of your LCRC project balances. lcrc-sbank -q balance <proj_name> Query a specific LCRC project balance. lcrc-sbank -q default Query your default LCRC project. lcrc-sbank -s default <proj_name> Change your default LCRC project. lcrc-sbank -q trans <proj_name> Query all transactions on an LCRC project. lcrc-sbank -h Print the help menu for the lcrc-sbank command.","title":"Project Allocation Queries and Management"},{"location":"allocation-management/sbank-allocation-accounting-system/","text":"sbank Allocation Accounting System (PBS Clusters) sbank is the new accounting system within LCRC, starting with Improv. It tracks project allocations, usage charges, and refunds. sbank allows queries about the balance and expiration of project allocations, and has replaced the outdated lcrc-sbank accounting system. The sbank accounting system helps users manage their allocations and usage per job. It gives the PIs the ability to monitor their allocation usage by user, job, and machine. It also allows the user to monitor their usage per allocation and provides insight on how many hours are left on the project. Getting Started with sbank sbank Example Commands provides a set of example commands on how to use the most common commands. sbank Man Pages Use these sbank man pages to get information on how to use the commands. sbank sbank-detail sbank-detail-allocations sbank-detail-jobs sbank-detail-projects sbank-detail-transactions sbank-detail-users sbank-list sbank-list-allocations sbank-list-jobs sbank-list-projects sbank-list-transactions sbank-list-users PBS/Slurm Conversion Chart If you are coming to PBS from Slurm, we have added a basic conversion chart for your general commands and submit scripts that you can reference that provide similar functions. Description PBS Pro Slurm Submit Job qsub [job_script] sbatch [job_script] Query Jobs qstat squeue Delete Job qdel [job_id] scancel [job_id] Hold User Job qhold [job_id] scontrol hold [job_id] Release User Job qrls [job_id] scontrol release [job_id] List Nodes pbsnodes -a scontrol show nodes Description PBS Pro Slurm Submission Directive #PBS #SBATCH Queue/Partition Selection -q [queue_name] -p [queue_name] Number of Nodes -l select=[count] -N [count] Number of CPUS per Node -l ncpus=[count] -ntasks-per-node=[count] Charge Account -A [project_name] -account=[project_name] Walltime -l walltime=[hh:mm:ss] -time=[hh:mm:ss] Job Name -N [name] -job-name=[name] Standard Out -o [file_name] -o [file_name] Standard Error -e [file_name] -e [file_name] Email Options -m abe -mail-type=[flags] Email Address -M [email_address] -mail-user=[email_address] Useful variables to use in your scripts: Description PBS Pro Slurm Submission Directory $PBS_O_WORKDIR $SLURM_SUBMIT_DIR Submit Host $PBS_O_HOST $SLURM_SUBMIT_HOST Job ID Number $PBS_JOBID $SLURM_JOBID Job Node List $PBS_NODEFILE $SLURM_JOB_NODELIST Job Array Index $PBS_ARRAYID $SLURM_ARRAY_TASK_ID","title":"sbank Allocation Accounting System (PBS Clusters)"},{"location":"allocation-management/sbank-allocation-accounting-system/#sbank-allocation-accounting-system-pbs-clusters","text":"sbank is the new accounting system within LCRC, starting with Improv. It tracks project allocations, usage charges, and refunds. sbank allows queries about the balance and expiration of project allocations, and has replaced the outdated lcrc-sbank accounting system. The sbank accounting system helps users manage their allocations and usage per job. It gives the PIs the ability to monitor their allocation usage by user, job, and machine. It also allows the user to monitor their usage per allocation and provides insight on how many hours are left on the project.","title":"sbank Allocation Accounting System (PBS Clusters)"},{"location":"allocation-management/sbank-allocation-accounting-system/#getting-started-with-sbank","text":"sbank Example Commands provides a set of example commands on how to use the most common commands.","title":"Getting Started with sbank"},{"location":"allocation-management/sbank-allocation-accounting-system/#sbank-man-pages","text":"Use these sbank man pages to get information on how to use the commands. sbank sbank-detail sbank-detail-allocations sbank-detail-jobs sbank-detail-projects sbank-detail-transactions sbank-detail-users sbank-list sbank-list-allocations sbank-list-jobs sbank-list-projects sbank-list-transactions sbank-list-users","title":"sbank Man Pages"},{"location":"allocation-management/sbank-allocation-accounting-system/#pbsslurm-conversion-chart","text":"If you are coming to PBS from Slurm, we have added a basic conversion chart for your general commands and submit scripts that you can reference that provide similar functions. Description PBS Pro Slurm Submit Job qsub [job_script] sbatch [job_script] Query Jobs qstat squeue Delete Job qdel [job_id] scancel [job_id] Hold User Job qhold [job_id] scontrol hold [job_id] Release User Job qrls [job_id] scontrol release [job_id] List Nodes pbsnodes -a scontrol show nodes Description PBS Pro Slurm Submission Directive #PBS #SBATCH Queue/Partition Selection -q [queue_name] -p [queue_name] Number of Nodes -l select=[count] -N [count] Number of CPUS per Node -l ncpus=[count] -ntasks-per-node=[count] Charge Account -A [project_name] -account=[project_name] Walltime -l walltime=[hh:mm:ss] -time=[hh:mm:ss] Job Name -N [name] -job-name=[name] Standard Out -o [file_name] -o [file_name] Standard Error -e [file_name] -e [file_name] Email Options -m abe -mail-type=[flags] Email Address -M [email_address] -mail-user=[email_address] Useful variables to use in your scripts: Description PBS Pro Slurm Submission Directory $PBS_O_WORKDIR $SLURM_SUBMIT_DIR Submit Host $PBS_O_HOST $SLURM_SUBMIT_HOST Job ID Number $PBS_JOBID $SLURM_JOBID Job Node List $PBS_NODEFILE $SLURM_JOB_NODELIST Job Array Index $PBS_ARRAYID $SLURM_ARRAY_TASK_ID","title":"PBS/Slurm Conversion Chart"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/","text":"Manpage for sbank-detail-allocations sbank-detail-allocations [options] [ ... ] Detail allocation information. NOTE: 1. The list of arguments are optional. 2. you can also enter list by using the -a option multiple times. 3. regardless, both are optional, and you can get detail allocation info using the option filters below. OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -e EVENT_ID, --event-id=EVENT_ID filter on event id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -j JOBID, --jobid=JOBID filter on jobid -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -t TRANSACTION_ID, --transaction-id=TRANSACTION_ID filter on transaction id -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -w \"FIELD_INFO\", --field-width= \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E END, --end=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. **Date Parsing Precedence: ** YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions), ... -I, --get-inactive also get inactive allocations -O, --get-only-inactive only inactive allocations -S START, --start=START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012 -T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID --award-type-name=AWARD_TYPE_NAME filter on award type name --award-category=AWARD_CATEGORY filter on award category --cbank-ref=CBANK_REF filter on Clusterbank reference id --created=CREATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --get-deleted also get deleted objects --get-only-deleted only deleted objects --all-charges only show list info that have charges regardless of project/user relationship --history-date-range=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012 --last-updated=LAST_UPDATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012 --no-commas remove commas from comma separated thousands --no-header do not display the header --no-history do not show history information --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals","title":"Manpage for sbank-detail-allocations"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#manpage-for-sbank-detail-allocations","text":"","title":"Manpage for sbank-detail-allocations"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#sbank-detail-allocations-options","text":"Detail allocation information. NOTE: 1. The list of arguments are optional. 2. you can also enter list by using the -a option multiple times. 3. regardless, both are optional, and you can get detail allocation info using the option filters below.","title":"sbank-detail-allocations [options] [ ... ]"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-version","text":"show program's version number and exit","title":"--version"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-h-help","text":"show this help message and exit","title":"-h, --help"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-a-allocation_id-allocation-idallocation_id","text":"filter on allocation id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-e-event_id-event-idevent_id","text":"filter on event id","title":"-e EVENT_ID, --event-id=EVENT_ID"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-f-field_info-field-to-displayfield_info","text":"FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\"","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-j-jobid-jobidjobid","text":"filter on jobid","title":"-j JOBID, --jobid=JOBID"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","text":"set number of fields to display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-p-project-projectproject","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-p PROJECT, --project=PROJECT"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-r-resource-resourceresource","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-r RESOURCE, --resource=RESOURCE"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-t-transaction_id-transaction-idtransaction_id","text":"filter on transaction id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-u-user-useruser","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-u USER, --user=USER"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-w-field_info-field-width","text":"\"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\"","title":"-w \"FIELD_INFO\", --field-width="},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-e-end-endend","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. **Date Parsing Precedence: ** YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"-E END, --end=END"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions), ...","title":"-H, --human-readable"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-i-get-inactive","text":"also get inactive allocations","title":"-I, --get-inactive"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-o-get-only-inactive","text":"only inactive allocations","title":"-O, --get-only-inactive"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-s-start-startstart","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"-S START, --start=START"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-t-transaction_type-transaction-typetransaction_type","text":"transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-award-type-nameaward_type_name","text":"filter on award type name","title":"--award-type-name=AWARD_TYPE_NAME"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-award-categoryaward_category","text":"filter on award category","title":"--award-category=AWARD_CATEGORY"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-cbank-refcbank_ref","text":"filter on Clusterbank reference id","title":"--cbank-ref=CBANK_REF"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-createdcreated_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"--created=CREATED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-debugdebug_level","text":"SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2","title":"--debug=DEBUG_LEVEL"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-get-deleted","text":"also get deleted objects","title":"--get-deleted"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-get-only-deleted","text":"only deleted objects","title":"--get-only-deleted"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-all-charges","text":"only show list info that have charges regardless of project/user relationship","title":"--all-charges"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-history-date-rangeend","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"--history-date-range=END"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-last-updatedlast_updated_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"--last-updated=LAST_UPDATED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-no-commas","text":"remove commas from comma separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-no-header","text":"do not display the header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-no-history","text":"do not show history information","title":"--no-history"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-no-rows","text":"do not display the row data","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-detail-allocations/#-no-totals","text":"do not display the totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-detail-jobs/","text":"sbank-detail-jobs sbank-detail-jobs [options] [ | ... | ] Detail job information. NOTE: The arguments or are NOT REQUIRED; event_id is the JOB DATABASE ID; is the SCHEDULER CREATED ID, such as Cobalt; can also be entered using option -j ; can also be entered using option -e ; can also be entered using option -r ; regardless, you can use options or arguments to get detail job information OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -e EVENT_ID, --event-id=EVENT_ID filter on event id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ] , for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -j JOBID, --jobid=JOBID filter on jobid -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -t TRANSACTION_ID, --transaction-id=TRANSACTION_ID filter on transaction id -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -w \"FIELD_INFO\", --field-width \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E END, --end=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -S START, --start=START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >,<=, <, == . Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID --created=CREATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --eligible=ELIGIBLE_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --get-not-charged only un-charged jobs --history-date-range=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --last-updated=LAST_UPDATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'gt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --no-commas remove commas from comma separated thousands --no-header do not display the header --no-history do not show history information --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals --queued=QUEUED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"sbank-detail-jobs"},{"location":"allocation-management/not_in_nav/sbank-detail-jobs/#sbank-detail-jobs","text":"","title":"sbank-detail-jobs"},{"location":"allocation-management/not_in_nav/sbank-detail-jobs/#sbank-detail-jobs-options","text":"Detail job information. NOTE: The arguments or are NOT REQUIRED; event_id is the JOB DATABASE ID; is the SCHEDULER CREATED ID, such as Cobalt; can also be entered using option -j ; can also be entered using option -e ; can also be entered using option -r ; regardless, you can use options or arguments to get detail job information","title":"sbank-detail-jobs [options] [  |  ...  | ]"},{"location":"allocation-management/not_in_nav/sbank-detail-jobs/#options","text":"--version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -e EVENT_ID, --event-id=EVENT_ID filter on event id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ] , for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -j JOBID, --jobid=JOBID filter on jobid -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -t TRANSACTION_ID, --transaction-id=TRANSACTION_ID filter on transaction id -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -w \"FIELD_INFO\", --field-width \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E END, --end=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -S START, --start=START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >,<=, <, == . Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID --created=CREATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --eligible=ELIGIBLE_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --get-not-charged only un-charged jobs --history-date-range=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --last-updated=LAST_UPDATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'gt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --no-commas remove commas from comma separated thousands --no-header do not display the header --no-history do not show history information --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals --queued=QUEUED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/","text":"Manpage for sbank-detail-projects sbank-detail-projects [options] [ ... ] Detail project information. NOTE: 1. The list of arguments are optional 2. you can also enter list by using the -p option multiple times 3. regardless, both are optional, and you can get detail project info using the option filters below OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -w \"FIELD_INFO\", --field-width \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E END, --end=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: - YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -I, --get-inactive get inactive allocations -S START, --start=START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --all-charges only show list info that have charges regardless of project/user relationship --no-commas remove commas from comma separated thousands --no-header do not display the header --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals","title":"Manpage for sbank-detail-projects"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#manpage-for-sbank-detail-projects","text":"","title":"Manpage for sbank-detail-projects"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#sbank-detail-projects-options","text":"Detail project information. NOTE: 1. The list of arguments are optional 2. you can also enter list by using the -p option multiple times 3. regardless, both are optional, and you can get detail project info using the option filters below","title":"sbank-detail-projects [options] [ ... ]"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-version","text":"show program's version number and exit","title":"--version"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-h-help","text":"show this help message and exit","title":"-h, --help"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-a-allocation_id-allocation-idallocation_id","text":"filter on allocation id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-f-field_info-field-to-displayfield_info","text":"FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\"","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","text":"set number of fields to display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-p-project-projectproject","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-p PROJECT, --project=PROJECT"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-r-resource-resourceresource","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-r RESOURCE, --resource=RESOURCE"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-u-user-useruser","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-u USER, --user=USER"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-w-field_info-field-width","text":"\"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\"","title":"-w \"FIELD_INFO\", --field-width"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-e-end-endend","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: - YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-E END, --end=END"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H, --human-readable"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-i-get-inactive","text":"get inactive allocations","title":"-I, --get-inactive"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-s-start-startstart","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-S START, --start=START"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-debugdebug_level","text":"SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2","title":"--debug=DEBUG_LEVEL"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-all-charges","text":"only show list info that have charges regardless of project/user relationship","title":"--all-charges"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-no-commas","text":"remove commas from comma separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-no-header","text":"do not display the header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-no-rows","text":"do not display the row data","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-detail-projects/#-no-totals","text":"do not display the totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/","text":"Manpage for sbank-detail-transactions sbank-detail-transactions [options] [ ... ] Detail transaction information. NOTE: 1. The list of arguments are optional 2. you can also enter list by using the -t option multiple times 3. regardless, both are optional, and you can get detail transaction info using the option filters below OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -c, --comment display comment -e EVENT_ID, --event-id=EVENT_ID filter on event id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ] for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -j JOBID, --jobid=JOBID filter on jobid -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -t TRANSACTION_ID, --transaction-id=TRANSACTION_ID filter on transaction id -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -w \"FIELD_INFO\", --field-width= \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E JOB_END, --end=JOB_END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -S JOB_START, --start=JOB_START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012 -T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID --at=TRANSACTION_AT_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012 --cbank-ref=CBANK_REF filter on Clusterbank reference id --created=JOB_CREATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --no-commas remove commas from comma separated thousands --no-header do not display the header --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals --queued=JOB_QUEUED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"Manpage for sbank-detail-transactions"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#manpage-for-sbank-detail-transactions","text":"","title":"Manpage for sbank-detail-transactions"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#sbank-detail-transactions-options","text":"Detail transaction information. NOTE: 1. The list of arguments are optional 2. you can also enter list by using the -t option multiple times 3. regardless, both are optional, and you can get detail transaction info using the option filters below","title":"sbank-detail-transactions [options] [ ... ]"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-version","text":"show program's version number and exit","title":"--version"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-h-help","text":"show this help message and exit","title":"-h, --help"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-a-allocation_id-allocation-idallocation_id","text":"filter on allocation id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-c-comment","text":"display comment","title":"-c, --comment"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-e-event_id-event-idevent_id","text":"filter on event id","title":"-e EVENT_ID, --event-id=EVENT_ID"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-f-field_info-field-to-displayfield_info","text":"FIELD_INFO is [: ] for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\"","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-j-jobid-jobidjobid","text":"filter on jobid","title":"-j JOBID, --jobid=JOBID"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","text":"set number of fields to display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-p-project-projectproject","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-p PROJECT, --project=PROJECT"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-r-resource-resourceresource","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-r RESOURCE, --resource=RESOURCE"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-t-transaction_id-transaction-idtransaction_id","text":"filter on transaction id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-u-user-useruser","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-u USER, --user=USER"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-w-field_info-field-width","text":"\"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\"","title":"-w \"FIELD_INFO\", --field-width="},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-e-job_end-endjob_end","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"-E JOB_END, --end=JOB_END"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H, --human-readable"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-s-job_start-startjob_start","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"-S JOB_START, --start=JOB_START"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-t-transaction_type-transaction-typetransaction_type","text":"transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-attransaction_at_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"--at=TRANSACTION_AT_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-cbank-refcbank_ref","text":"filter on Clusterbank reference id","title":"--cbank-ref=CBANK_REF"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-createdjob_created_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"--created=JOB_CREATED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-debugdebug_level","text":"SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2","title":"--debug=DEBUG_LEVEL"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-no-commas","text":"remove commas from comma separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-no-header","text":"do not display the header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-no-rows","text":"do not display the row data","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-no-totals","text":"do not display the totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-detail-transactions/#-queuedjob_queued_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'ge' for single date entry OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012","title":"--queued=JOB_QUEUED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-detail-users/","text":"Manpage for sbank-detail-users sbank-detail-users [options] [ ... ] Detail user information. **NOTE: ** 1. Use -I to include inactive allocations 2. the list of arguments are optional 3. you can also enter list by using the -u option multiple times 4. regardless, both are optional, and you can get detail user info using the option filters below OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -w \"FIELD_INFO\", --field-width \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E END, --end=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -I, --get-inactive get inactive allocations -S START, --start=START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --all-charges only show list info that have charges regardless of project/user relationship --no-commas remove commas from comma separated thousands --no-header do not display the header --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals","title":"Manpage for sbank-detail-users"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#manpage-for-sbank-detail-users","text":"","title":"Manpage for sbank-detail-users"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#sbank-detail-users-options","text":"Detail user information. **NOTE: ** 1. Use -I to include inactive allocations 2. the list of arguments are optional 3. you can also enter list by using the -u option multiple times 4. regardless, both are optional, and you can get detail user info using the option filters below","title":"sbank-detail-users [options] [ ... ]"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-version","text":"show program's version number and exit","title":"--version"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-h-help","text":"show this help message and exit","title":"-h, --help"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-a-allocation_id-allocation-idallocation_id","text":"filter on allocation id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-f-field_info-field-to-displayfield_info","text":"FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\"","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","text":"set number of fields to display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-p-project-projectproject","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-p PROJECT, --project=PROJECT"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-r-resource-resourceresource","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-r RESOURCE, --resource=RESOURCE"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-u-user-useruser","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-u USER, --user=USER"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-w-field_info-field-width","text":"\"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\"","title":"-w \"FIELD_INFO\", --field-width"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-e-end-endend","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-E END, --end=END"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H, --human-readable"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-i-get-inactive","text":"get inactive allocations","title":"-I, --get-inactive"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-s-start-startstart","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-S START, --start=START"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-debugdebug_level","text":"SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2","title":"--debug=DEBUG_LEVEL"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-all-charges","text":"only show list info that have charges regardless of project/user relationship","title":"--all-charges"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-no-commas","text":"remove commas from comma separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-no-header","text":"do not display the header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-no-rows","text":"do not display the row data","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-detail-users/#-no-totals","text":"do not display the totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-detail/","text":"Manpage for sbank-detail sbank-detail [options] Detail Meta Command COMMANDS allocations [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) categories [-f|-n|-w|...] messages [-f|-n|-w|...] names [-f|-n|-w|...] jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...] OPTIONS -a --allocation enter allocation id -c --comment enter comment for new or edit commands, display comment for list commands -e --event-id enter event db id; event db id is an internal id created by the charging system -f --field enter [: ], width is optional; enter -f? or -f \"?\" for available fields, + to add fields -h --help command line help -j --jobid enter jobid; jobid is created by the scheduler and is not unique -n --num-field enter number of fields to display -p --project enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -r --resource enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -s --suballocation enter suballocation id -t --transaction enter transaction id -u --user enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -w --field-width enter the field width as follows: : , enter -w? or -w \"?\" for available fields -E --end enter end datetime filter -H --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -I --get-inactive include inactive allocations -O --get-only-inactive get only inactive allocations -S --start enter start datetime filter -T --Type enter type of transaction --all-charges for list allocations | projects | users, only show info with charges --at enter transaction created datetime filter --award-category enter allocation award category --award-type-name enter allocation award-type name --created enter created datetime filter --debug enter debug level --get-deleted get deleted objects --get-not-charged get jobs that have not been charged --get-only-deleted get only deleted objects --history-date-range enter history datetime filter --last-updated enter last updated datetime filter --no-commas remove commas from comma-separated thousands --no-header do not display header --no-history do not display history information --no-rows do not display rows --no-sys-msg do not display system message --no-totals do not display totals --queued enter queued datetime filter","title":"Manpage for sbank-detail"},{"location":"allocation-management/not_in_nav/sbank-detail/#manpage-for-sbank-detail","text":"","title":"Manpage for sbank-detail"},{"location":"allocation-management/not_in_nav/sbank-detail/#sbank-detail-options","text":"Detail Meta Command","title":"sbank-detail  [options]"},{"location":"allocation-management/not_in_nav/sbank-detail/#commands","text":"allocations [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) categories [-f|-n|-w|...] messages [-f|-n|-w|...] names [-f|-n|-w|...] jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]","title":"COMMANDS"},{"location":"allocation-management/not_in_nav/sbank-detail/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-detail/#-a-allocation","text":"enter allocation id","title":"-a --allocation"},{"location":"allocation-management/not_in_nav/sbank-detail/#-c-comment","text":"enter comment for new or edit commands, display comment for list commands","title":"-c --comment"},{"location":"allocation-management/not_in_nav/sbank-detail/#-e-event-id","text":"enter event db id; event db id is an internal id created by the charging system","title":"-e --event-id"},{"location":"allocation-management/not_in_nav/sbank-detail/#-f-field","text":"enter [: ], width is optional; enter -f? or -f \"?\" for available fields, + to add fields","title":"-f --field"},{"location":"allocation-management/not_in_nav/sbank-detail/#-h-help","text":"command line help","title":"-h --help"},{"location":"allocation-management/not_in_nav/sbank-detail/#-j-jobid","text":"enter jobid; jobid is created by the scheduler and is not unique","title":"-j --jobid"},{"location":"allocation-management/not_in_nav/sbank-detail/#-n-num-field","text":"enter number of fields to display","title":"-n --num-field"},{"location":"allocation-management/not_in_nav/sbank-detail/#-p-project","text":"enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-p --project"},{"location":"allocation-management/not_in_nav/sbank-detail/#-r-resource","text":"enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-r --resource"},{"location":"allocation-management/not_in_nav/sbank-detail/#-s-suballocation","text":"enter suballocation id","title":"-s --suballocation"},{"location":"allocation-management/not_in_nav/sbank-detail/#-t-transaction","text":"enter transaction id","title":"-t --transaction"},{"location":"allocation-management/not_in_nav/sbank-detail/#-u-user","text":"enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-u --user"},{"location":"allocation-management/not_in_nav/sbank-detail/#-w-field-width","text":"enter the field width as follows: : , enter -w? or -w \"?\" for available fields","title":"-w --field-width"},{"location":"allocation-management/not_in_nav/sbank-detail/#-e-end","text":"enter end datetime filter","title":"-E --end"},{"location":"allocation-management/not_in_nav/sbank-detail/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H --human-readable"},{"location":"allocation-management/not_in_nav/sbank-detail/#-i-get-inactive","text":"include inactive allocations","title":"-I --get-inactive"},{"location":"allocation-management/not_in_nav/sbank-detail/#-o-get-only-inactive","text":"get only inactive allocations","title":"-O --get-only-inactive"},{"location":"allocation-management/not_in_nav/sbank-detail/#-s-start","text":"enter start datetime filter","title":"-S --start"},{"location":"allocation-management/not_in_nav/sbank-detail/#-t-type","text":"enter type of transaction","title":"-T --Type"},{"location":"allocation-management/not_in_nav/sbank-detail/#-all-charges","text":"for list allocations | projects | users, only show info with charges","title":"--all-charges"},{"location":"allocation-management/not_in_nav/sbank-detail/#-at","text":"enter transaction created datetime filter","title":"--at"},{"location":"allocation-management/not_in_nav/sbank-detail/#-award-category","text":"enter allocation award category","title":"--award-category"},{"location":"allocation-management/not_in_nav/sbank-detail/#-award-type-name","text":"enter allocation award-type name","title":"--award-type-name"},{"location":"allocation-management/not_in_nav/sbank-detail/#-created","text":"enter created datetime filter","title":"--created"},{"location":"allocation-management/not_in_nav/sbank-detail/#-debug","text":"enter debug level","title":"--debug"},{"location":"allocation-management/not_in_nav/sbank-detail/#-get-deleted","text":"get deleted objects","title":"--get-deleted"},{"location":"allocation-management/not_in_nav/sbank-detail/#-get-not-charged","text":"get jobs that have not been charged","title":"--get-not-charged"},{"location":"allocation-management/not_in_nav/sbank-detail/#-get-only-deleted","text":"get only deleted objects","title":"--get-only-deleted"},{"location":"allocation-management/not_in_nav/sbank-detail/#-history-date-range","text":"enter history datetime filter","title":"--history-date-range"},{"location":"allocation-management/not_in_nav/sbank-detail/#-last-updated","text":"enter last updated datetime filter","title":"--last-updated"},{"location":"allocation-management/not_in_nav/sbank-detail/#-no-commas","text":"remove commas from comma-separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-detail/#-no-header","text":"do not display header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-detail/#-no-history","text":"do not display history information","title":"--no-history"},{"location":"allocation-management/not_in_nav/sbank-detail/#-no-rows","text":"do not display rows","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-detail/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-detail/#-no-totals","text":"do not display totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-detail/#-queued","text":"enter queued datetime filter","title":"--queued"},{"location":"allocation-management/not_in_nav/sbank-examples/","text":"sbank Example Commands Below is a set of helpful commands to help you better manage the projects you have running on LCRC Improv. All transactions displayed are in Node Hours. Each Node Hour is 128 Core Hours. Balances and transactions displayed will update every 5 minutes. View your Project\u2019s Allocations Command: sbank-list-allocations [options] $ sbank-list-allocations -p <project_name> Allocation Suballocation Start End Resource Project Jobs Charged Available Balance ---------- ------------- ---------- ---------- -------- ------- ---- ------- ----------------- 2 2 2023-10-01 2024-01-01 improv projectX 17 34.2 9965.8 116 116 2023-10-01 2024-01-01 improv projectX 0 0.0 500.0 Totals: Rows: 2 Improv: Available Balance: 10465.8 node hours Charged : 34.2 node hours Jobs : 17 View your Project\u2019s Users Command: sbank-list-users [options] $ sbank-list-users -p <project_name> User Jobs Charged --------- ---- ------- user1 57 164.7 user2 28 29.5 user3 0 0.0 Totals: Rows: 3 Resources: improv Projects: projectX Jobs : 112 Charged: 194.2 node hours View your Project\u2019s Transactions Command: sbank-list-transactions [options] $ sbank-list-transactions -p <project_name> ... 1661 improv projectX 2 2 2023-10-26 user1 CHARGE 0.0 1752.imgt1 1662 improv projectX 2 2 2023-10-26 user1 CHARGE 0.0 1753.imgt1 1663 improv projectX 2 2 2023-10-26 user2 CHARGE 0.0 1754.imgt1 1664 improv projectX 2 2 2023-10-27 user2 CHARGE 0.0 1755.imgt1 1665 improv projectX 2 2 2023-10-27 user1 CHARGE 0.0 1756.imgt1 Totals: Rows: 135 Improv: Charges Amount : 229.3 node hours Deposits Amount: 11500.0 node hours List Jobs Run by a User Command: sbank-list-jobs [options] $ sbank-list-jobs -u <username> ... 1611 1727.imgt1 improv projectX 2 2 user1 0:00:26 0.1 1612 1728.imgt1 improv projectX 2 2 user1 0:00:28 0.1 1613 1729.imgt1 improv projectX 2 2 user1 0:00:26 0.1 1614 1730.imgt1 improv projectX 2 2 user1 0:00:26 0.1 Totals: Rows: 314 Improv: Charged: 61.0 node hours Get Detailed Allocation Information Command: sbank-detail-allocations [options] $ sbank-detail-allocations -p <project_name> -- Allocation Info: * Allocation: 2 * Suballocation: 2 * Start: 2023-10-01 05:00:00 * End: 2024-01-01 05:59:59 * Resource: improv * Project: projectX * Jobs: 1,517 * Charged: 3,001.3 * Available Balance: 7,076.8 * Allocation Created: 2023-08-11 06:44:25 * Award Category: NA * Award Type: NA * Subname: None * Primary: 1 * Restricted: 0 * Deposits: 10,078.1 * Pullbacks: 0.0 * Sub Deposits: 0.0 * Sub Withdraws: 0.0 * Disable Message: None * Submanagement Enabled: 0 * Users: None * Comment: None * Last Updated: 2023-10-19 14:05:24 * Allocation History: No updates * Suballocation History: No updates -- Allocation Info: * Allocation: 116 * Suballocation: 116 * Start: 2023-10-01 05:00:00 * End: 2024-01-01 05:59:59 * Resource: improv * Project: projectX * Jobs: 3 * Charged: 7.0 * Available Balance: 496.9 * Allocation Created: 2023-10-19 04:45:42 * Award Category: NA * Award Type: NA * Subname: None * Primary: 1 * Restricted: 0 * Deposits: 503.9 * Pullbacks: 0.0 * Sub Deposits: 0.0 * Sub Withdraws: 0.0 * Disable Message: None * Submanagement Enabled: 0 * Users: None * Comment: None * Last Updated: 2023-10-19 14:50:36 * Allocation History: No updates * Suballocation History: No updates Totals: Items: 2 Improv: Available Balance: 7,573.8 node hours Charged : 3,008.3 node hours Deposits : 10,582.0 node hours Jobs : 1,520 Pullbacks : 0.0 node hours Sub Deposits : 0.0 node hours Sub Withdraws : 0.0 node hours If you are looking for a specific piece of information from sbank that is not listed above, please contact LCRC Support.","title":"sbank Example Commands"},{"location":"allocation-management/not_in_nav/sbank-examples/#sbank-example-commands","text":"Below is a set of helpful commands to help you better manage the projects you have running on LCRC Improv. All transactions displayed are in Node Hours. Each Node Hour is 128 Core Hours. Balances and transactions displayed will update every 5 minutes.","title":"sbank Example Commands"},{"location":"allocation-management/not_in_nav/sbank-examples/#view-your-projects-allocations","text":"Command: sbank-list-allocations [options] $ sbank-list-allocations -p <project_name> Allocation Suballocation Start End Resource Project Jobs Charged Available Balance ---------- ------------- ---------- ---------- -------- ------- ---- ------- ----------------- 2 2 2023-10-01 2024-01-01 improv projectX 17 34.2 9965.8 116 116 2023-10-01 2024-01-01 improv projectX 0 0.0 500.0 Totals: Rows: 2 Improv: Available Balance: 10465.8 node hours Charged : 34.2 node hours Jobs : 17","title":"View your Project\u2019s Allocations"},{"location":"allocation-management/not_in_nav/sbank-examples/#view-your-projects-users","text":"Command: sbank-list-users [options] $ sbank-list-users -p <project_name> User Jobs Charged --------- ---- ------- user1 57 164.7 user2 28 29.5 user3 0 0.0 Totals: Rows: 3 Resources: improv Projects: projectX Jobs : 112 Charged: 194.2 node hours","title":"View your Project\u2019s Users"},{"location":"allocation-management/not_in_nav/sbank-examples/#view-your-projects-transactions","text":"Command: sbank-list-transactions [options] $ sbank-list-transactions -p <project_name> ... 1661 improv projectX 2 2 2023-10-26 user1 CHARGE 0.0 1752.imgt1 1662 improv projectX 2 2 2023-10-26 user1 CHARGE 0.0 1753.imgt1 1663 improv projectX 2 2 2023-10-26 user2 CHARGE 0.0 1754.imgt1 1664 improv projectX 2 2 2023-10-27 user2 CHARGE 0.0 1755.imgt1 1665 improv projectX 2 2 2023-10-27 user1 CHARGE 0.0 1756.imgt1 Totals: Rows: 135 Improv: Charges Amount : 229.3 node hours Deposits Amount: 11500.0 node hours","title":"View your Project\u2019s Transactions"},{"location":"allocation-management/not_in_nav/sbank-examples/#list-jobs-run-by-a-user","text":"Command: sbank-list-jobs [options] $ sbank-list-jobs -u <username> ... 1611 1727.imgt1 improv projectX 2 2 user1 0:00:26 0.1 1612 1728.imgt1 improv projectX 2 2 user1 0:00:28 0.1 1613 1729.imgt1 improv projectX 2 2 user1 0:00:26 0.1 1614 1730.imgt1 improv projectX 2 2 user1 0:00:26 0.1 Totals: Rows: 314 Improv: Charged: 61.0 node hours","title":"List Jobs Run by a User"},{"location":"allocation-management/not_in_nav/sbank-examples/#get-detailed-allocation-information","text":"Command: sbank-detail-allocations [options] $ sbank-detail-allocations -p <project_name> -- Allocation Info: * Allocation: 2 * Suballocation: 2 * Start: 2023-10-01 05:00:00 * End: 2024-01-01 05:59:59 * Resource: improv * Project: projectX * Jobs: 1,517 * Charged: 3,001.3 * Available Balance: 7,076.8 * Allocation Created: 2023-08-11 06:44:25 * Award Category: NA * Award Type: NA * Subname: None * Primary: 1 * Restricted: 0 * Deposits: 10,078.1 * Pullbacks: 0.0 * Sub Deposits: 0.0 * Sub Withdraws: 0.0 * Disable Message: None * Submanagement Enabled: 0 * Users: None * Comment: None * Last Updated: 2023-10-19 14:05:24 * Allocation History: No updates * Suballocation History: No updates -- Allocation Info: * Allocation: 116 * Suballocation: 116 * Start: 2023-10-01 05:00:00 * End: 2024-01-01 05:59:59 * Resource: improv * Project: projectX * Jobs: 3 * Charged: 7.0 * Available Balance: 496.9 * Allocation Created: 2023-10-19 04:45:42 * Award Category: NA * Award Type: NA * Subname: None * Primary: 1 * Restricted: 0 * Deposits: 503.9 * Pullbacks: 0.0 * Sub Deposits: 0.0 * Sub Withdraws: 0.0 * Disable Message: None * Submanagement Enabled: 0 * Users: None * Comment: None * Last Updated: 2023-10-19 14:50:36 * Allocation History: No updates * Suballocation History: No updates Totals: Items: 2 Improv: Available Balance: 7,573.8 node hours Charged : 3,008.3 node hours Deposits : 10,582.0 node hours Jobs : 1,520 Pullbacks : 0.0 node hours Sub Deposits : 0.0 node hours Sub Withdraws : 0.0 node hours If you are looking for a specific piece of information from sbank that is not listed above, please contact LCRC Support.","title":"Get Detailed Allocation Information"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/","text":"Manpage for sbank-list-allocations sbank-list-allocations [options] Generate allocation list report. Notes: 1. Use -I to include inactive allocations 2. enter \"-r all\" to get information for all resources OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -c, --comment display comment -e EVENT_ID, --event-id=EVENT_ID filter on event id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -j JOBID, --jobid=JOBID filter on jobid -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -t TRANSACTION_ID, --transaction-id=TRANSACTION_ID filter on transaction id -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -w \"FIELD_INFO\", --field-width \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E END, --end=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -I, --get-inactive get inactive allocations -O, --get-only-inactive only inactive allocations -S START, --start=START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID --award-type-name=AWARD_TYPE_NAME filter on award-type name --award-category=AWARD_CATEGORY filter on award category --cbank-ref=CBANK_REF filter on Clusterbank reference id --created=CREATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --get-deleted get deleted objects --get-only-deleted get only deleted objects --all-charges only show list info that have charges regardless of project/user relationship --last-updated=LAST_UPDATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --no-commas remove commas from comma-separated thousands --no-header do not display the header --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals","title":"Manpage for sbank-list-allocations"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#manpage-for-sbank-list-allocations","text":"","title":"Manpage for sbank-list-allocations"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#sbank-list-allocations-options","text":"Generate allocation list report. Notes: 1. Use -I to include inactive allocations 2. enter \"-r all\" to get information for all resources","title":"sbank-list-allocations [options]"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-version","text":"show program's version number and exit","title":"--version"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-h-help","text":"show this help message and exit","title":"-h, --help"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-a-allocation_id-allocation-idallocation_id","text":"filter on allocation id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-c-comment","text":"display comment","title":"-c, --comment"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-e-event_id-event-idevent_id","text":"filter on event id","title":"-e EVENT_ID, --event-id=EVENT_ID"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-f-field_info-field-to-displayfield_info","text":"FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\"","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-j-jobid-jobidjobid","text":"filter on jobid","title":"-j JOBID, --jobid=JOBID"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","text":"set number of fields to display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-p-project-projectproject","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-p PROJECT, --project=PROJECT"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-r-resource-resourceresource","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-r RESOURCE, --resource=RESOURCE"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-t-transaction_id-transaction-idtransaction_id","text":"filter on transaction id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-u-user-useruser","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-u USER, --user=USER"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-w-field_info-field-width","text":"\"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\"","title":"-w \"FIELD_INFO\", --field-width"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-e-end-endend","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-E END, --end=END"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H, --human-readable"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-i-get-inactive","text":"get inactive allocations","title":"-I, --get-inactive"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-o-get-only-inactive","text":"only inactive allocations","title":"-O, --get-only-inactive"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-s-start-startstart","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-S START, --start=START"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-t-transaction_type-transaction-typetransaction_type","text":"transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-award-type-nameaward_type_name","text":"filter on award-type name","title":"--award-type-name=AWARD_TYPE_NAME"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-award-categoryaward_category","text":"filter on award category","title":"--award-category=AWARD_CATEGORY"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-cbank-refcbank_ref","text":"filter on Clusterbank reference id","title":"--cbank-ref=CBANK_REF"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-createdcreated_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"--created=CREATED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-debugdebug_level","text":"SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2","title":"--debug=DEBUG_LEVEL"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-get-deleted","text":"get deleted objects","title":"--get-deleted"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-get-only-deleted","text":"get only deleted objects","title":"--get-only-deleted"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-all-charges","text":"only show list info that have charges regardless of project/user relationship","title":"--all-charges"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-last-updatedlast_updated_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"--last-updated=LAST_UPDATED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-no-commas","text":"remove commas from comma-separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-no-header","text":"do not display the header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-no-rows","text":"do not display the row data","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-list-allocations/#-no-totals","text":"do not display the totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/","text":"Manpage for sbank-list-jobs sbank-list-jobs [options] Generate job list report Note: To get information for all resources, enter \"-r all\". OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -e EVENT_ID, --event-id=EVENT_ID filter on event id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -j JOBID, --jobid=JOBID filter on jobid -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -t TRANSACTION_ID, --transaction-id=TRANSACTION_ID filter on transaction id -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -w \"FIELD_INFO\", --field-width \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E END, --end=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -S START, --start=START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID --created=CREATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --eligible=ELIGIBLE_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --get-not-charged get only jobs that have not been charged --last-updated=LAST_UPDATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --no-commas remove commas from comma-separated thousands --no-header do not display the header --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals --queued=QUEUED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"Manpage for sbank-list-jobs"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#manpage-for-sbank-list-jobs","text":"","title":"Manpage for sbank-list-jobs"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#sbank-list-jobs-options","text":"Generate job list report Note: To get information for all resources, enter \"-r all\".","title":"sbank-list-jobs [options]"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-version","text":"show program's version number and exit","title":"--version"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-h-help","text":"show this help message and exit","title":"-h, --help"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-a-allocation_id-allocation-idallocation_id","text":"filter on allocation id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-e-event_id-event-idevent_id","text":"filter on event id","title":"-e EVENT_ID, --event-id=EVENT_ID"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-f-field_info-field-to-displayfield_info","text":"FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\"","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-j-jobid-jobidjobid","text":"filter on jobid","title":"-j JOBID, --jobid=JOBID"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","text":"set number of fields to display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-p-project-projectproject","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-p PROJECT, --project=PROJECT"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-r-resource-resourceresource","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-r RESOURCE, --resource=RESOURCE"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-t-transaction_id-transaction-idtransaction_id","text":"filter on transaction id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-u-user-useruser","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-u USER, --user=USER"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-w-field_info-field-width","text":"\"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\"","title":"-w \"FIELD_INFO\", --field-width"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-e-end-endend","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-E END, --end=END"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H, --human-readable"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-s-start-startstart","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-S START, --start=START"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-t-transaction_type-transaction-typetransaction_type","text":"transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-createdcreated_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"--created=CREATED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-debugdebug_level","text":"SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2","title":"--debug=DEBUG_LEVEL"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-eligibleeligible_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"--eligible=ELIGIBLE_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-get-not-charged","text":"get only jobs that have not been charged","title":"--get-not-charged"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-last-updatedlast_updated_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"--last-updated=LAST_UPDATED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-no-commas","text":"remove commas from comma-separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-no-header","text":"do not display the header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-no-rows","text":"do not display the row data","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-no-totals","text":"do not display the totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-list-jobs/#-queuedqueued_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"--queued=QUEUED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-list-projects/","text":"Manpage for sbank-list-projects sbank-list-projects [options] Generate project list report. Notes: Use -I to include inactive allocations to get information for all resources, enter \"-r all\" OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -w \"FIELD_INFO\", --field-width \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -I, --get-inactive get inactive allocations -S START, --start=START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --all-charges only show list info that have charges regardless of project/user relationship --no-commas remove commas from comma-separated thousands --no-header do not display the header --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals","title":"Manpage for sbank-list-projects"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#manpage-for-sbank-list-projects","text":"","title":"Manpage for sbank-list-projects"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#sbank-list-projects-options","text":"Generate project list report. Notes: Use -I to include inactive allocations to get information for all resources, enter \"-r all\"","title":"sbank-list-projects [options]"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-version","text":"show program's version number and exit","title":"--version"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-h-help","text":"show this help message and exit","title":"-h, --help"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-a-allocation_id-allocation-idallocation_id","text":"filter on allocation id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-f-field_info-field-to-displayfield_info","text":"FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\"","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","text":"set number of fields to display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-p-project-projectproject","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-p PROJECT, --project=PROJECT"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-r-resource-resourceresource","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-r RESOURCE, --resource=RESOURCE"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-u-user-useruser","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-u USER, --user=USER"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-w-field_info-field-width","text":"\"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-w \"FIELD_INFO\", --field-width"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H, --human-readable"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-i-get-inactive","text":"get inactive allocations","title":"-I, --get-inactive"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-s-start-startstart","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-S START, --start=START"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-debugdebug_level","text":"SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2","title":"--debug=DEBUG_LEVEL"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-all-charges","text":"only show list info that have charges regardless of project/user relationship","title":"--all-charges"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-no-commas","text":"remove commas from comma-separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-no-header","text":"do not display the header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-no-rows","text":"do not display the row data","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-list-projects/#-no-totals","text":"do not display the totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/","text":"Manpage for sbank-list-transactions sbank-list-transactions [options] Generate transaction list report. Note: To get information for all resources, enter \"-r all\". OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -c, --comment display comment -e EVENT_ID, --event-id=EVENT_ID filter on event id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -j JOBID, --jobid=JOBID filter on jobid -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -t TRANSACTION_ID, --transaction-id=TRANSACTION_ID filter on transaction id -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names -w \"FIELD_INFO\", --field-width \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E JOB_END, --end=JOB_END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -S JOB_START, --start=JOB_START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID --at=TRANSACTION_AT_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --cbank-ref=CBANK_REF filter on Clusterbank reference id --created=JOB_CREATED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --no-commas remove commas from comma-separated thousands --no-header do not display the header --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals --queued=JOB_QUEUED_TIMESTAMP [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"Manpage for sbank-list-transactions"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#manpage-for-sbank-list-transactions","text":"","title":"Manpage for sbank-list-transactions"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#sbank-list-transactions-options","text":"Generate transaction list report. Note: To get information for all resources, enter \"-r all\".","title":"sbank-list-transactions [options]"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-version","text":"show program's version number and exit","title":"--version"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-h-help","text":"show this help message and exit","title":"-h, --help"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-a-allocation_id-allocation-idallocation_id","text":"filter on allocation id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-c-comment","text":"display comment","title":"-c, --comment"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-e-event_id-event-idevent_id","text":"filter on event id","title":"-e EVENT_ID, --event-id=EVENT_ID"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-f-field_info-field-to-displayfield_info","text":"FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\"","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-j-jobid-jobidjobid","text":"filter on jobid","title":"-j JOBID, --jobid=JOBID"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","text":"set number of fields to display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-p-project-projectproject","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-p PROJECT, --project=PROJECT"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-r-resource-resourceresource","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-r RESOURCE, --resource=RESOURCE"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-t-transaction_id-transaction-idtransaction_id","text":"filter on transaction id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-u-user-useruser","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names","title":"-u USER, --user=USER"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-w-field_info-field-width","text":"\"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\"","title":"-w \"FIELD_INFO\", --field-width"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-e-job_end-endjob_end","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-E JOB_END, --end=JOB_END"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H, --human-readable"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-s-job_start-startjob_start","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-S JOB_START, --start=JOB_START"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-t-transaction_type-transaction-typetransaction_type","text":"transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-attransaction_at_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"--at=TRANSACTION_AT_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-cbank-refcbank_ref","text":"filter on Clusterbank reference id","title":"--cbank-ref=CBANK_REF"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-createdjob_created_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"--created=JOB_CREATED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-debugdebug_level","text":"SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2","title":"--debug=DEBUG_LEVEL"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-no-commas","text":"remove commas from comma-separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-no-header","text":"do not display the header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-no-rows","text":"do not display the row data","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-no-totals","text":"do not display the totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-list-transactions/#-queuedjob_queued_timestamp","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"--queued=JOB_QUEUED_TIMESTAMP"},{"location":"allocation-management/not_in_nav/sbank-list-users/","text":"Manpage for sbank-list-users sbank-list-users [options] Generate user list report. Notes: Use -I to include inactive allocations for information for all resources, use \"-r all\" OPTIONS --version show program's version number and exit -h, --help show this help message and exit -a ALLOCATION_ID, --allocation-id=ALLOCATION_ID filter on allocation id -f FIELD_INFO, --field-to-display=FIELD_INFO FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\" -n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY set number of fields to display -p PROJECT, --project=PROJECT filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -r RESOURCE, --resource=RESOURCE filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -u USER, --user=USER filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -w \"FIELD_INFO\", --field-width \"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\" -E END, --end=END [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 -H, --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -I, --get-inactive also get inactive allocations -S START, --start=START [OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 --debug=DEBUG_LEVEL SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2 --all-charges only show list info that have charges regardless of project/user relationship --no-commas remove commas from comma-separated thousands --no-header do not display the header --no-rows do not display the row data --no-sys-msg do not display system message --no-totals do not display the totals","title":"Manpage for sbank-list-users"},{"location":"allocation-management/not_in_nav/sbank-list-users/#manpage-for-sbank-list-users","text":"","title":"Manpage for sbank-list-users"},{"location":"allocation-management/not_in_nav/sbank-list-users/#sbank-list-users-options","text":"Generate user list report. Notes: Use -I to include inactive allocations for information for all resources, use \"-r all\"","title":"sbank-list-users [options]"},{"location":"allocation-management/not_in_nav/sbank-list-users/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-version","text":"show program's version number and exit","title":"--version"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-h-help","text":"show this help message and exit","title":"-h, --help"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-a-allocation_id-allocation-idallocation_id","text":"filter on allocation id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-f-field_info-field-to-displayfield_info","text":"FIELD_INFO is [: ], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [: ] ...\"","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","text":"set number of fields to display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-p-project-projectproject","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-p PROJECT, --project=PROJECT"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-r-resource-resourceresource","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-r RESOURCE, --resource=RESOURCE"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-u-user-useruser","text":"filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-u USER, --user=USER"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-w-field_info-field-width","text":"\"FIELD_INFO\" FIELD_INFO is : , for available fields enter -w? or -w \"?\"","title":"-w \"FIELD_INFO\", --field-width"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-e-end-endend","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-E END, --end=END"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H, --human-readable"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-i-get-inactive","text":"also get inactive allocations","title":"-I, --get-inactive"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-s-start-startstart","text":"[OPER1] [...[OPER2] ], where the operators OPER1 and OPER2 can be one of the following: - ge, gt, le, lt, eq or >=, >, <=, <, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012","title":"-S START, --start=START"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-debugdebug_level","text":"SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2","title":"--debug=DEBUG_LEVEL"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-all-charges","text":"only show list info that have charges regardless of project/user relationship","title":"--all-charges"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-no-commas","text":"remove commas from comma-separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-no-header","text":"do not display the header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-no-rows","text":"do not display the row data","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-list-users/#-no-totals","text":"do not display the totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-list/","text":"Manpage for sbank-list sbank-list [options] List Meta Command COMMANDS allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) categories [-f|-n|-w|...] messages [-f|-n|-w|...] names [-f|-n|-w|...] jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...] OPTIONS -a --allocation enter allocation id -c --comment enter comment for new or edit commands, display comment for list commands -e --event-id enter event db id; event db id is an internal id created by the charging system -f --field enter [: ], width is optional; enter -f? or -f \"?\" for available fields, + to add fields -h --help command line help -j --jobid enter jobid; jobid is created by the scheduler and is not unique -n --num-field enter number of fields to display -p --project enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -r --resource enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -s --suballocation enter suballocation id -t --transaction enter transaction id -u --user enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -w --field-width enter the field width as follows: : , enter -w? or -w \"?\" for available fields -E --end enter end datetime filter -H --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -I --get-inactive include inactive allocations -O --get-only-inactive include inactive allocations -S --start enter start datetime filter -T --Type enter type of transaction --all-charges for list allocations | projects | users, only show info with charges --at enter transaction-created datetime filter --award-category enter allocation award category --award-type-name enter allocation award-type name --created enter created datetime filter --debug enter debug level --get-deleted get deleted objects --get-not-charged get jobs that have not been charged --get-only-deleted get only deleted objects --history-date-range enter history datetime filter --last-updated enter last updated datetime filter --no-commas remove commas from comma-separated thousands --no-header do not display header --no-history do not display history information --no-rows do not display rows --no-sys-msg do not display system message --no-totals do not display totals --queued enter queued datetime filter","title":"Manpage for sbank-list"},{"location":"allocation-management/not_in_nav/sbank-list/#manpage-for-sbank-list","text":"","title":"Manpage for sbank-list"},{"location":"allocation-management/not_in_nav/sbank-list/#sbank-list-options","text":"List Meta Command","title":"sbank-list  [options]"},{"location":"allocation-management/not_in_nav/sbank-list/#commands","text":"allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) categories [-f|-n|-w|...] messages [-f|-n|-w|...] names [-f|-n|-w|...] jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]","title":"COMMANDS"},{"location":"allocation-management/not_in_nav/sbank-list/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-list/#-a-allocation","text":"enter allocation id","title":"-a --allocation"},{"location":"allocation-management/not_in_nav/sbank-list/#-c-comment","text":"enter comment for new or edit commands, display comment for list commands","title":"-c --comment"},{"location":"allocation-management/not_in_nav/sbank-list/#-e-event-id","text":"enter event db id; event db id is an internal id created by the charging system","title":"-e --event-id"},{"location":"allocation-management/not_in_nav/sbank-list/#-f-field","text":"enter [: ], width is optional; enter -f? or -f \"?\" for available fields, + to add fields","title":"-f --field"},{"location":"allocation-management/not_in_nav/sbank-list/#-h-help","text":"command line help","title":"-h --help"},{"location":"allocation-management/not_in_nav/sbank-list/#-j-jobid","text":"enter jobid; jobid is created by the scheduler and is not unique","title":"-j --jobid"},{"location":"allocation-management/not_in_nav/sbank-list/#-n-num-field","text":"enter number of fields to display","title":"-n --num-field"},{"location":"allocation-management/not_in_nav/sbank-list/#-p-project","text":"enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-p --project"},{"location":"allocation-management/not_in_nav/sbank-list/#-r-resource","text":"enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-r --resource"},{"location":"allocation-management/not_in_nav/sbank-list/#-s-suballocation","text":"enter suballocation id","title":"-s --suballocation"},{"location":"allocation-management/not_in_nav/sbank-list/#-t-transaction","text":"enter transaction id","title":"-t --transaction"},{"location":"allocation-management/not_in_nav/sbank-list/#-u-user","text":"enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-u --user"},{"location":"allocation-management/not_in_nav/sbank-list/#-w-field-width","text":"enter the field width as follows: : , enter -w? or -w \"?\" for available fields","title":"-w --field-width"},{"location":"allocation-management/not_in_nav/sbank-list/#-e-end","text":"enter end datetime filter","title":"-E --end"},{"location":"allocation-management/not_in_nav/sbank-list/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H --human-readable"},{"location":"allocation-management/not_in_nav/sbank-list/#-i-get-inactive","text":"include inactive allocations","title":"-I --get-inactive"},{"location":"allocation-management/not_in_nav/sbank-list/#-o-get-only-inactive","text":"include inactive allocations","title":"-O --get-only-inactive"},{"location":"allocation-management/not_in_nav/sbank-list/#-s-start","text":"enter start datetime filter","title":"-S --start"},{"location":"allocation-management/not_in_nav/sbank-list/#-t-type","text":"enter type of transaction","title":"-T --Type"},{"location":"allocation-management/not_in_nav/sbank-list/#-all-charges","text":"for list allocations | projects | users, only show info with charges","title":"--all-charges"},{"location":"allocation-management/not_in_nav/sbank-list/#-at","text":"enter transaction-created datetime filter","title":"--at"},{"location":"allocation-management/not_in_nav/sbank-list/#-award-category","text":"enter allocation award category","title":"--award-category"},{"location":"allocation-management/not_in_nav/sbank-list/#-award-type-name","text":"enter allocation award-type name","title":"--award-type-name"},{"location":"allocation-management/not_in_nav/sbank-list/#-created","text":"enter created datetime filter","title":"--created"},{"location":"allocation-management/not_in_nav/sbank-list/#-debug","text":"enter debug level","title":"--debug"},{"location":"allocation-management/not_in_nav/sbank-list/#-get-deleted","text":"get deleted objects","title":"--get-deleted"},{"location":"allocation-management/not_in_nav/sbank-list/#-get-not-charged","text":"get jobs that have not been charged","title":"--get-not-charged"},{"location":"allocation-management/not_in_nav/sbank-list/#-get-only-deleted","text":"get only deleted objects","title":"--get-only-deleted"},{"location":"allocation-management/not_in_nav/sbank-list/#-history-date-range","text":"enter history datetime filter","title":"--history-date-range"},{"location":"allocation-management/not_in_nav/sbank-list/#-last-updated","text":"enter last updated datetime filter","title":"--last-updated"},{"location":"allocation-management/not_in_nav/sbank-list/#-no-commas","text":"remove commas from comma-separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-list/#-no-header","text":"do not display header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-list/#-no-history","text":"do not display history information","title":"--no-history"},{"location":"allocation-management/not_in_nav/sbank-list/#-no-rows","text":"do not display rows","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-list/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-list/#-no-totals","text":"do not display totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-list/#-queued","text":"enter queued datetime filter","title":"--queued"},{"location":"allocation-management/not_in_nav/sbank-manpage/","text":"Manpage for sbank Commands sbank [options] DESCRIPTION HPC Accounting System Command Line Interface detail meta command \"detail\" meta command displays information in a long format with history updates, where appropriate. list meta command \"list\" meta command displays information in a table format, but no history updates are displayed. IMPORTANT NOTES 1. All dates entered shall be interpreted as UTC 2. non-admin users will only be able to see their content (jobs, charges, etc.) 3. project admin users will be able to see all of the content for their projects 4. staff admin users will be able to see all the content 5. --help and -h are the help options. META COMMANDS - detail [options] - list [options] (DEFAULT) DETAIL COMMANDS * allocations [-a | -e |-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] [ ... ] (DEFAULT) * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ] * projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] [ ... ] * transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ] * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...] [ ... ] LIST COMMANDS * allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] * transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...] OPTIONS -a --allocation enter allocation id -c --comment enter comment for new or edit commands, display comment for list commands -e --event-id enter event db id; event db id is an internal id created by the charging system -f --field enter [: ], width is optional; enter -f? or -f \"?\" for available fields, + to add fields -h --help command line help -j --jobid enter jobid; jobid is created by the scheduler and is not unique -n --num-field enter number of fields to display -p --project enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -r --resource enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -s --suballocation enter suballocation id -t --transaction enter transaction id -u --user enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names -w --field-width enter the field width as follows: : , enter -w? or -w \"?\" for available fields -E --end enter end datetime filter -H --human-readable abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ... -I --get-inactive include inactive allocations -O --get-only-inactive get only inactive allocations -S --start enter start datetime filter -T --Type enter type of transaction --all-charges for list allocations | projects | users, only show info with charges --at enter transaction-created datetime filter --award-category enter allocation award category --award-type-name enter allocation award-type name --created enter created datetime filter --debug enter debug level --get-deleted get deleted objects --get-not-charged get jobs that have not been charged --get-only-deleted get only deleted objects --history-date-range enter history datetime filter --home-dir enter the directory to store the pbs meta file --ignore-pbs-files all new pbs files will be ignored and marked as processed --last-updated enter last updated datetime filter --no-commas remove commas from comma-separated thousands --no-header do not display header --no-history do not display history information --no-rows do not display rows --no-sys-msg do not display system message --no-totals do not display totals --queued enter queued datetime filter MORE OPTION EXPLANATIONS For -a, -e, -f, -w, -j, -p, -r, -t, -u, -T, --award-categories, --award_type_names, --cbank_refs options: These options can be entered multiple times for different values or entered once for multiple values. Examples: sbank-list-allocation -u \"pershey rojas allcock\" or > sbank-list-allocation -u pershey -u rojas -u allcock sbank-list-allocation -f \"id p avail\" or > sbank-list-allocation -f id -f p -f avail For -u, -p and -r the use of wild card \"*\" is allowed, but only on names, not ids: Examples: The following command will find allocations for users whose names start with \"pers\" and also users rojas and allcock. > sbank-list-allocation -u \"pers* rojas allcock\" The following command will find allocations for projects that contain \"ratio\" in the name. > sbank-list-allocation -p ratio The following command will find allocations for projects that end with \"tion\" in the name. > sbank-list-allocation -p *tion The following command will find allocations for projects that start with \"ab\" and end with \"ng\" in the name. > sbank-list-allocation -p ab*ng For -f option: This option is the display field option. To get the available fields enter -f? or -f \"?\". Default fields columns will be displayed if no field option is specified. To replace the current fields to display, enter: > sbank-list-allocations ... -f \"FIELD[:WIDTH]...FIELD[:WIDTH]\" or > sbank-list-allocations ... -f FIELD[:WIDTH] ... -f FIELD[:WIDTH] If you wish to add fields to the default fields, enter one + symbol anywhere in the quoted string: > sbank-list-allocations ... -f \"+ FIELD[:WIDTH]...FIELD[:WIDTH]\", only one + symbol is needed. The fields will be displayed in table format and in the order entered in the command line. You can specify the field width, where WIDTH can be positive or negative value. Left alignment use -, right alignment use + or nothing. For -w option: FIELD:WIDTH, if the field is displayed it will change the width for the specified field. NOTE: This will not add the field as in -f option, only change the width. To get available fields you can also use -w? or -w \"?\" as in -f option. For -S, -E, --created, --queued, --last-updated, --history-date-range options: These are the date filter options. All dates are treated as UTC. You can use any reasonable date string that resembles a date Ambiguous dates will be parsed with the following parsing precedence: **YEAR then MONTH then DAY ** For example, 10-11-12 or 101112 will be the following date: Oct. 11, 2012 Not: Nov. 12, 2010 or Nov. 10, 2012 Or you can specify a single date as follows: \"[OPER]UTC_DATE\" You can specify a date range as follows: \"[OPER1]UTC_DATE1...[OPER2]UTC_DATE2\" Where OPER can be one of the following operators: \"==\", \">=\", \"<=\", \">\", \"<\" or \"eq\", \"ge\", \"le\", \"gt\", \"lt\" Note: The following defaults for OPER, OPER1, OPER2 for the following options: Options OPER OPER1 OPER2 ------------------------- ---- ----- ----- -E, < >= < -S, >= >= < --at >= >= < --created >= >= < --eligible >= >= < --last-updated >= >= < --queued >= >= < You can also use the following key letters \"n\", \"t\", \"d\", \"w\", \"y\" as follows: KEY SYNTAX DEFINITIONS ---------- ----------- n[ow] now, where \"now\" is current-date current-time UTC t[oday] today, where \"today\" is current-date 00:00:00 UTC [+/-]d specified \"number\" of +/- days from \"today\" in UTC [+/-]w specified \"number\" of +/- weeks from \"today\" in UTC [+/-]y specified \"number\" of +/- years from \"today\" in UTC For -T option: Transaction type option. The following are the valid transaction types and their explanation: CHARGE filter on job charges PULLBACK filter on allocation pullbacks DEPOSIT filter on allocation deposits REFUND filter on job refunds VOID filter on void transactions INVOCATION sbank sbank sbank sbank-detail sbank detail sbank d sbank-detail-allocations sbank detail allocations sbank d a sbank-detail-jobs sbank detail jobs sbank d j sbank-detail-projects sbank detail project sbank d p sbank-detail-transactions sbank detail transactions sbank d t sbank-detail-users sbank detail users sbank d u sbank-list sbank list sbank l sbank-list-allocations sbank list allocations sbank l a sbank-list-jobs sbank list jobs sbank l j sbank-list-projects sbank list projects sbank l p sbank-list-transactions sbank list transactions sbank l t sbank-list-users sbank list users sbank l u ENVIRONMENT VARIABLES Command line default options: Define the following environment variables as you would in the command line. Once the environment variable is defined, it will be used as the default options and arguments for the specific command. Command line options will take precedence. sbank_DETAIL_ALLOCATIONS_ARGS Default arguments and options for sbank-detail-allocations. sbank_DETAIL_CATEGORIES_ARGS Default arguments and options for sbank-detail-categories. sbank_DETAIL_NAMES_ARGS Default arguments and options for sbank-detail-names. sbank_DETAIL_MESSAGES_ARGS Default arguments and options for sbank-detail-messages. sbank_DETAIL_JOBS_ARGS Default arguments and options for sbank-detail-jobs. sbank_DETAIL_PROJECTS_ARGS Default arguments and options for sbank-detail-projects. sbank_DETAIL_TRANSACTIONS_ARGS Default arguments and options for sbank-detail-transactions. sbank_DETAIL_USERS_ARGS Default arguments and options for sbank-detail-users. sbank_LIST_ALLOCATIONS_ARGS Default arguments and options for sbank-list-allocations. sbank_LIST_JOBS_ARGS Default arguments and options for sbank-list-jobs. sbank_LIST_PROJECTS_ARGS Default arguments and options for sbank-list-projects. sbank_LIST_TRANSACTIONS_ARGS Default arguments and options for sbank-list-transactions. sbank_LIST_USERS_ARGS Default arguments and options for sbank-list-users. EXAMPLES Example 1: -f, --field > sbank-list-transactions ... -f field1:-20 -f field2:20 -f field3 or > sbank-list-transactions ... -f \"field1:-20 field2:20 field3\" Explanation: Fields will be displayed in order of appearance, where field1:-20 means 20 characters long, left align; where field2:20 means 20 characters long, right align; where field3 uses default sizes. Number fields default to right aligned. Text fields default to left aligned. Example 2: -S, -E, --created, --queued, --last-updated, --history-start, --history-end Single date-string examples: sbank-list-allocations -S \">=Oct 11, 2014\" start dates that are >= \"2014-10-11 00:00:00\" sbank-list-allocations -S \"<=2014-11-10\" start dates that are <= \"2014-11-10 00:00:00\" sbank-list-allocations -E \"<20141110\" end dates that are < \"2014-11-10 00:00:00\" sbank-list-allocations -E \"22:30:10\" end dates that are < \" 22:30:10\" sbank-list-allocations -S \">today\" start dates that are > \" 00:00:00\" sbank-list-allocations -E t end dates that are < \" 00:00:00\" sbank-list-allocations -S gtnow start dates that are > \" \" sbank-list-allocations -E len end dates that are <= \" \" sbank-list-allocations -S \"1d\" start dates that are >= \"today +1 day\" sbank-list-allocations -E \"-2w\" end dates that are < \"today -2 weeks\" sbank-list-allocations -S \">=1y\" start dates that are >= \"today +1 year\" sbank-list-allocations -S \">2012\" start dates that are > \"2012- - 00:00:00\" Range date-string examples: sbank-list-allocations -S \"2013-01-01...2014-01-01\" \"2013-01-01\" <= DATES < \"2014-01-01\" sbank-list-allocations -S \"-1y...t\" \"today -1 year\" <= DATES < \"today\" sbank-list-allocations -E \"2013...t\"\" \"2013- - \" <= DATES < \"today\" sbank-list-allocations -E \">2013...<=t\"\" \"2013- - \" < DATES <= \"today\" Example 3: Command invocation examples sbank-list-projects list projects full command invocation sbank list projects list projects meta command invocation sbank s p list projects partial meta command invocation sbank p list projects where \"list\" is the default sbank list allocations is the default sbank a list allocations \"list\" is the default sbank s a list allocations partial meta command invocation Example 4: -h, --help sbank -h will give you help summary on all of sbank sbank list --help will give you help on all the \"list\" commands sbank list allocations -h will give you help on the \"list allocations\" command sbank-list-allocations -h will give you help on the \"list allocations\" command sbank l a --help will give you help on the \"list allocations\" command","title":"Manpage for sbank Commands"},{"location":"allocation-management/not_in_nav/sbank-manpage/#manpage-for-sbank-commands","text":"","title":"Manpage for sbank Commands"},{"location":"allocation-management/not_in_nav/sbank-manpage/#sbank-options","text":"","title":"sbank   [options] "},{"location":"allocation-management/not_in_nav/sbank-manpage/#description","text":"HPC Accounting System Command Line Interface","title":"DESCRIPTION"},{"location":"allocation-management/not_in_nav/sbank-manpage/#detail-meta-command","text":"\"detail\" meta command displays information in a long format with history updates, where appropriate.","title":"detail meta command"},{"location":"allocation-management/not_in_nav/sbank-manpage/#list-meta-command","text":"\"list\" meta command displays information in a table format, but no history updates are displayed. IMPORTANT NOTES 1. All dates entered shall be interpreted as UTC 2. non-admin users will only be able to see their content (jobs, charges, etc.) 3. project admin users will be able to see all of the content for their projects 4. staff admin users will be able to see all the content 5. --help and -h are the help options.","title":"list meta command"},{"location":"allocation-management/not_in_nav/sbank-manpage/#meta-commands","text":"","title":"META COMMANDS"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-detail-options","text":"","title":"- detail  [options]"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-list-options-default","text":"DETAIL COMMANDS * allocations [-a | -e |-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] [ ... ] (DEFAULT) * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ] * projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] [ ... ] * transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ] * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...] [ ... ] LIST COMMANDS * allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] * transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]","title":"- list  [options] (DEFAULT)"},{"location":"allocation-management/not_in_nav/sbank-manpage/#options","text":"","title":"OPTIONS"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-a-allocation","text":"enter allocation id","title":"-a --allocation"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-c-comment","text":"enter comment for new or edit commands, display comment for list commands","title":"-c --comment"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-e-event-id","text":"enter event db id; event db id is an internal id created by the charging system","title":"-e --event-id"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-f-field","text":"enter [: ], width is optional; enter -f? or -f \"?\" for available fields, + to add fields","title":"-f --field"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-h-help","text":"command line help","title":"-h --help"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-j-jobid","text":"enter jobid; jobid is created by the scheduler and is not unique","title":"-j --jobid"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-n-num-field","text":"enter number of fields to display","title":"-n --num-field"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-p-project","text":"enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-p --project"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-r-resource","text":"enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-r --resource"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-s-suballocation","text":"enter suballocation id","title":"-s --suballocation"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-t-transaction","text":"enter transaction id","title":"-t --transaction"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-u-user","text":"enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names","title":"-u --user"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-w-field-width","text":"enter the field width as follows: : , enter -w? or -w \"?\" for available fields","title":"-w --field-width"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-e-end","text":"enter end datetime filter","title":"-E --end"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-h-human-readable","text":"abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...","title":"-H --human-readable"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-i-get-inactive","text":"include inactive allocations","title":"-I --get-inactive"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-o-get-only-inactive","text":"get only inactive allocations","title":"-O --get-only-inactive"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-s-start","text":"enter start datetime filter","title":"-S --start"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-t-type","text":"enter type of transaction","title":"-T --Type"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-all-charges","text":"for list allocations | projects | users, only show info with charges","title":"--all-charges"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-at","text":"enter transaction-created datetime filter","title":"--at"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-award-category","text":"enter allocation award category","title":"--award-category"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-award-type-name","text":"enter allocation award-type name","title":"--award-type-name"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-created","text":"enter created datetime filter","title":"--created"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-debug","text":"enter debug level","title":"--debug"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-get-deleted","text":"get deleted objects","title":"--get-deleted"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-get-not-charged","text":"get jobs that have not been charged","title":"--get-not-charged"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-get-only-deleted","text":"get only deleted objects","title":"--get-only-deleted"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-history-date-range","text":"enter history datetime filter","title":"--history-date-range"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-home-dir","text":"enter the directory to store the pbs meta file","title":"--home-dir"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-ignore-pbs-files","text":"all new pbs files will be ignored and marked as processed","title":"--ignore-pbs-files"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-last-updated","text":"enter last updated datetime filter","title":"--last-updated"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-no-commas","text":"remove commas from comma-separated thousands","title":"--no-commas"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-no-header","text":"do not display header","title":"--no-header"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-no-history","text":"do not display history information","title":"--no-history"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-no-rows","text":"do not display rows","title":"--no-rows"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-no-sys-msg","text":"do not display system message","title":"--no-sys-msg"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-no-totals","text":"do not display totals","title":"--no-totals"},{"location":"allocation-management/not_in_nav/sbank-manpage/#-queued","text":"enter queued datetime filter","title":"--queued"},{"location":"allocation-management/not_in_nav/sbank-manpage/#more-option-explanations","text":"For -a, -e, -f, -w, -j, -p, -r, -t, -u, -T, --award-categories, --award_type_names, --cbank_refs options: These options can be entered multiple times for different values or entered once for multiple values. Examples: sbank-list-allocation -u \"pershey rojas allcock\" or > sbank-list-allocation -u pershey -u rojas -u allcock sbank-list-allocation -f \"id p avail\" or > sbank-list-allocation -f id -f p -f avail For -u, -p and -r the use of wild card \"*\" is allowed, but only on names, not ids: Examples: The following command will find allocations for users whose names start with \"pers\" and also users rojas and allcock. > sbank-list-allocation -u \"pers* rojas allcock\" The following command will find allocations for projects that contain \"ratio\" in the name. > sbank-list-allocation -p ratio The following command will find allocations for projects that end with \"tion\" in the name. > sbank-list-allocation -p *tion The following command will find allocations for projects that start with \"ab\" and end with \"ng\" in the name. > sbank-list-allocation -p ab*ng For -f option: This option is the display field option. To get the available fields enter -f? or -f \"?\". Default fields columns will be displayed if no field option is specified. To replace the current fields to display, enter: > sbank-list-allocations ... -f \"FIELD[:WIDTH]...FIELD[:WIDTH]\" or > sbank-list-allocations ... -f FIELD[:WIDTH] ... -f FIELD[:WIDTH] If you wish to add fields to the default fields, enter one + symbol anywhere in the quoted string: > sbank-list-allocations ... -f \"+ FIELD[:WIDTH]...FIELD[:WIDTH]\", only one + symbol is needed. The fields will be displayed in table format and in the order entered in the command line. You can specify the field width, where WIDTH can be positive or negative value. Left alignment use -, right alignment use + or nothing. For -w option: FIELD:WIDTH, if the field is displayed it will change the width for the specified field. NOTE: This will not add the field as in -f option, only change the width. To get available fields you can also use -w? or -w \"?\" as in -f option. For -S, -E, --created, --queued, --last-updated, --history-date-range options: These are the date filter options. All dates are treated as UTC. You can use any reasonable date string that resembles a date Ambiguous dates will be parsed with the following parsing precedence: **YEAR then MONTH then DAY ** For example, 10-11-12 or 101112 will be the following date: Oct. 11, 2012 Not: Nov. 12, 2010 or Nov. 10, 2012 Or you can specify a single date as follows: \"[OPER]UTC_DATE\" You can specify a date range as follows: \"[OPER1]UTC_DATE1...[OPER2]UTC_DATE2\" Where OPER can be one of the following operators: \"==\", \">=\", \"<=\", \">\", \"<\" or \"eq\", \"ge\", \"le\", \"gt\", \"lt\" Note: The following defaults for OPER, OPER1, OPER2 for the following options: Options OPER OPER1 OPER2 ------------------------- ---- ----- ----- -E, < >= < -S, >= >= < --at >= >= < --created >= >= < --eligible >= >= < --last-updated >= >= < --queued >= >= < You can also use the following key letters \"n\", \"t\", \"d\", \"w\", \"y\" as follows: KEY SYNTAX DEFINITIONS ---------- ----------- n[ow] now, where \"now\" is current-date current-time UTC t[oday] today, where \"today\" is current-date 00:00:00 UTC [+/-]d specified \"number\" of +/- days from \"today\" in UTC [+/-]w specified \"number\" of +/- weeks from \"today\" in UTC [+/-]y specified \"number\" of +/- years from \"today\" in UTC For -T option: Transaction type option. The following are the valid transaction types and their explanation: CHARGE filter on job charges PULLBACK filter on allocation pullbacks DEPOSIT filter on allocation deposits REFUND filter on job refunds VOID filter on void transactions","title":"MORE OPTION EXPLANATIONS"},{"location":"allocation-management/not_in_nav/sbank-manpage/#invocation","text":"sbank sbank sbank sbank-detail sbank detail sbank d sbank-detail-allocations sbank detail allocations sbank d a sbank-detail-jobs sbank detail jobs sbank d j sbank-detail-projects sbank detail project sbank d p sbank-detail-transactions sbank detail transactions sbank d t sbank-detail-users sbank detail users sbank d u sbank-list sbank list sbank l sbank-list-allocations sbank list allocations sbank l a sbank-list-jobs sbank list jobs sbank l j sbank-list-projects sbank list projects sbank l p sbank-list-transactions sbank list transactions sbank l t sbank-list-users sbank list users sbank l u","title":"INVOCATION"},{"location":"allocation-management/not_in_nav/sbank-manpage/#environment-variables","text":"Command line default options: Define the following environment variables as you would in the command line. Once the environment variable is defined, it will be used as the default options and arguments for the specific command. Command line options will take precedence. sbank_DETAIL_ALLOCATIONS_ARGS Default arguments and options for sbank-detail-allocations. sbank_DETAIL_CATEGORIES_ARGS Default arguments and options for sbank-detail-categories. sbank_DETAIL_NAMES_ARGS Default arguments and options for sbank-detail-names. sbank_DETAIL_MESSAGES_ARGS Default arguments and options for sbank-detail-messages. sbank_DETAIL_JOBS_ARGS Default arguments and options for sbank-detail-jobs. sbank_DETAIL_PROJECTS_ARGS Default arguments and options for sbank-detail-projects. sbank_DETAIL_TRANSACTIONS_ARGS Default arguments and options for sbank-detail-transactions. sbank_DETAIL_USERS_ARGS Default arguments and options for sbank-detail-users. sbank_LIST_ALLOCATIONS_ARGS Default arguments and options for sbank-list-allocations. sbank_LIST_JOBS_ARGS Default arguments and options for sbank-list-jobs. sbank_LIST_PROJECTS_ARGS Default arguments and options for sbank-list-projects. sbank_LIST_TRANSACTIONS_ARGS Default arguments and options for sbank-list-transactions. sbank_LIST_USERS_ARGS Default arguments and options for sbank-list-users.","title":"ENVIRONMENT VARIABLES"},{"location":"allocation-management/not_in_nav/sbank-manpage/#examples","text":"Example 1: -f, --field > sbank-list-transactions ... -f field1:-20 -f field2:20 -f field3 or > sbank-list-transactions ... -f \"field1:-20 field2:20 field3\" Explanation: Fields will be displayed in order of appearance, where field1:-20 means 20 characters long, left align; where field2:20 means 20 characters long, right align; where field3 uses default sizes. Number fields default to right aligned. Text fields default to left aligned. Example 2: -S, -E, --created, --queued, --last-updated, --history-start, --history-end Single date-string examples: sbank-list-allocations -S \">=Oct 11, 2014\" start dates that are >= \"2014-10-11 00:00:00\" sbank-list-allocations -S \"<=2014-11-10\" start dates that are <= \"2014-11-10 00:00:00\" sbank-list-allocations -E \"<20141110\" end dates that are < \"2014-11-10 00:00:00\" sbank-list-allocations -E \"22:30:10\" end dates that are < \" 22:30:10\" sbank-list-allocations -S \">today\" start dates that are > \" 00:00:00\" sbank-list-allocations -E t end dates that are < \" 00:00:00\" sbank-list-allocations -S gtnow start dates that are > \" \" sbank-list-allocations -E len end dates that are <= \" \" sbank-list-allocations -S \"1d\" start dates that are >= \"today +1 day\" sbank-list-allocations -E \"-2w\" end dates that are < \"today -2 weeks\" sbank-list-allocations -S \">=1y\" start dates that are >= \"today +1 year\" sbank-list-allocations -S \">2012\" start dates that are > \"2012- - 00:00:00\" Range date-string examples: sbank-list-allocations -S \"2013-01-01...2014-01-01\" \"2013-01-01\" <= DATES < \"2014-01-01\" sbank-list-allocations -S \"-1y...t\" \"today -1 year\" <= DATES < \"today\" sbank-list-allocations -E \"2013...t\"\" \"2013- - \" <= DATES < \"today\" sbank-list-allocations -E \">2013...<=t\"\" \"2013- - \" < DATES <= \"today\" Example 3: Command invocation examples sbank-list-projects list projects full command invocation sbank list projects list projects meta command invocation sbank s p list projects partial meta command invocation sbank p list projects where \"list\" is the default sbank list allocations is the default sbank a list allocations \"list\" is the default sbank s a list allocations partial meta command invocation Example 4: -h, --help sbank -h will give you help summary on all of sbank sbank list --help will give you help on all the \"list\" commands sbank list allocations -h will give you help on the \"list allocations\" command sbank-list-allocations -h will give you help on the \"list allocations\" command sbank l a --help will give you help on the \"list allocations\" command","title":"EXAMPLES"},{"location":"bebop/getting-started-bebop/","text":"Getting Started on Bebop Accessing Bebop To access Bebop, use the following command: ssh <your_argonne_username>@bebop.lcrc.anl.gov System Architecture For a detailed overview of the Improv system, including the compute node architecture, refer to the Hardware Overview page. Job Execution For information on how to run jobs on Bebop, refer to the Running Jobs page.","title":"Getting Started"},{"location":"bebop/getting-started-bebop/#getting-started-on-bebop","text":"","title":"Getting Started on Bebop"},{"location":"bebop/getting-started-bebop/#accessing-bebop","text":"To access Bebop, use the following command: ssh <your_argonne_username>@bebop.lcrc.anl.gov","title":"Accessing Bebop"},{"location":"bebop/getting-started-bebop/#system-architecture","text":"For a detailed overview of the Improv system, including the compute node architecture, refer to the Hardware Overview page.","title":"System Architecture"},{"location":"bebop/getting-started-bebop/#job-execution","text":"For information on how to run jobs on Bebop, refer to the Running Jobs page.","title":"Job Execution"},{"location":"bebop/hardware-overview-bebop/","text":"Bebop Hardware Overview Bebop has 1,024 compute nodes with Intel Broadwell and Knights Landing (KNL) processors. Broadwell nodes have DDR4 memory while KNL have DDR4 and MCDRAM on each node. 128 nodes have a 4TB HDD for scratch disk. The high-performance interconnect is Intel Omni-Path (OPA) 100G. There are 12 OPA connections to LCRC\u2019s existing data storage system so you will have access to all of the same files between LCRC clusters. Bebop Compute Nodes 672 Broadwell Nodes Bebop Broadwell Description Per Node Aggregate Processor Intel Xeon E5-2695v4 2 Sockets 1,344 Cores/Threads 18 Cores/1 Thread per core 36 24,192/24,192 Memory DDR4 128 GiB 86,016 GiB Local SSD 4 TB (Nodes bdwd-[0001-0064]) 1 256 TB 352 Knights Landing (KNL) Nodes Bebop KNL Description Per Node Aggregate Processor Intel Xeon Phi 7230 1 Sockets 352 Cores/Threads 64 Cores/4 Threads per core 64 22,528/90,112 Memory DDR4/MCDRAM 96 GiB/16 GiB 33,792 GiB/5,632 GiB Local SSD 4 TB (Nodes knld-[0001-0064]) 1 256 TB Bebop Login Nodes There are four login nodes available to users for editing code, building code, submitting/monitoring jobs, checking usage ( lcrc-sbank ), etc. Their full hostnames are beboploginN.lcrc.anl.gov for N equal to 1 through 4 . The login nodes hardware is identical to the compute nodes. The various compilers and libraries are present on the logins, so most users should be able to build their code. All users share the same login nodes so please be courteous and respectful of your fellow users. For example, please do not run computationally or IO intensive pre- or post-processing on the logins and keep the parallelism of your builds to a reasonable level.","title":"Hardware Overview"},{"location":"bebop/hardware-overview-bebop/#bebop-hardware-overview","text":"Bebop has 1,024 compute nodes with Intel Broadwell and Knights Landing (KNL) processors. Broadwell nodes have DDR4 memory while KNL have DDR4 and MCDRAM on each node. 128 nodes have a 4TB HDD for scratch disk. The high-performance interconnect is Intel Omni-Path (OPA) 100G. There are 12 OPA connections to LCRC\u2019s existing data storage system so you will have access to all of the same files between LCRC clusters.","title":"Bebop Hardware Overview"},{"location":"bebop/hardware-overview-bebop/#bebop-compute-nodes","text":"672 Broadwell Nodes Bebop Broadwell Description Per Node Aggregate Processor Intel Xeon E5-2695v4 2 Sockets 1,344 Cores/Threads 18 Cores/1 Thread per core 36 24,192/24,192 Memory DDR4 128 GiB 86,016 GiB Local SSD 4 TB (Nodes bdwd-[0001-0064]) 1 256 TB 352 Knights Landing (KNL) Nodes Bebop KNL Description Per Node Aggregate Processor Intel Xeon Phi 7230 1 Sockets 352 Cores/Threads 64 Cores/4 Threads per core 64 22,528/90,112 Memory DDR4/MCDRAM 96 GiB/16 GiB 33,792 GiB/5,632 GiB Local SSD 4 TB (Nodes knld-[0001-0064]) 1 256 TB","title":"Bebop Compute Nodes"},{"location":"bebop/hardware-overview-bebop/#bebop-login-nodes","text":"There are four login nodes available to users for editing code, building code, submitting/monitoring jobs, checking usage ( lcrc-sbank ), etc. Their full hostnames are beboploginN.lcrc.anl.gov for N equal to 1 through 4 . The login nodes hardware is identical to the compute nodes. The various compilers and libraries are present on the logins, so most users should be able to build their code. All users share the same login nodes so please be courteous and respectful of your fellow users. For example, please do not run computationally or IO intensive pre- or post-processing on the logins and keep the parallelism of your builds to a reasonable level.","title":"Bebop Login Nodes"},{"location":"bebop/running-jobs-bebop/","text":"Running Jobs on Bebop Overview Bebop's job scheduling system is characterized by: Uses Slurm Uses the legacy lcrc-sbank accounting system Allocations are calculated in core hours Partition Limits Bebop currently enforces the following limits on publicly available partitions: 32 Running Jobs per user. 100 Queued Jobs per user. 3 Days (72 Hours) Maximum Walltime on Broadwell Nodes. (bdws is 1 hour) 7 Days (168 Hours) Maximum Walltime on KNL Nodes. (knls is 4 hours) 1 Hour Default Walltime if not specified. bdwall (Broadwell Compute Nodes) is the default partition. Available Partitions Bebop Partition Name Description Number of Nodes CPU Type Cores Per Node Memory Per Node Local Scratch Disk bdwall All Broadwell Nodes 664 Intel Xeon E5-2695v4 36 128GB DDR4 15 GB or 4 TB bdw Broadwell Nodes with 15 GB / scratch 600 Intel Xeon E5-2695v4 36 128GB DDR4 15 GB bdwd Broadwell Nodes with 4 TB / scratch 64 Intel Xeon E5-2695v4 36 128GB DDR4 4 TB bdws Broadwell Shared Nodes (Oversubscription / Non-Exclusive) 8 Intel Xeon E5-2695v4 36 128GB DDR4 15 GB knlall All Knights Landing Nodes 348 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 15 GB or 4 TB knl Knights Landing Nodes with 15GB /scratch 284 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 15 GB knld Knights Landing Nodes with 4TB /scratch 64 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 4 TB knls Knights Landing Shared Nodes (Oversubscription / Non-Exclusive) 4 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 15 GB knl-preemptable All Knights Landing Nodes with restrictions including preemption. Click here for more details . 348 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 15 GB or 4 TB Submission Examples Example sbatch Job Submission (Simple) Example sbatch Job Submission (MPI) Example Interactive Job Submission Example Knights Landing (KNL) sbatch Job Submission (MPI) Attention KNL Users: Please note that if you wish to use a different set of modes for KNL other than Quadrant & Cache, you'll need to request a reservation that will require approval. You can request your reservation via this form . Depending on the amount of KNL nodes and the changes to be made, this could take a decent amount of time. Because this is using the resources, this time will also be charged against your core hour usage including the time it takes to complete the job. #!/bin/bash # SBATCH --job-name=<my_job_name> # SBATCH --account=<my_lcrc_project_name> # SBATCH --partition=knlall # SBATCH --constraint knl,quad,cache # Other modes require a reservation. # SBATCH --nodes=2 # SBATCH --ntasks-per-node=64 # SBATCH --output=<my_job_name>.out # SBATCH --error=<my_job_name>.error # SBATCH --mail-user=<your email address> # Optional if you require email # SBATCH --mail-type=ALL # Optional if you require email # SBATCH --time=01:00:00 # Setup My Environment module load intel-parallel-studio/cluster.2018.4-xtm134f export I_MPI_FABRICS = shm:tmi # Run My Program srun -n 128 ./helloworld This will run across two KNL nodes, using the Quadrant mode, and Cache MCDRAM mode. The default setting is quad,cache. A table of available settings, along with more detailed information about Slurm\u2019s KNL support is available here . You can then submit this job from a Bebop login nodes using: sbatch myjob.sh Please refer to the sbatch webpage for a list of full options including environment variables. KNL Preemptable Queue LCRC has set up a preemptable queue on the KNL partition (named as knl-preemptable). This covers all of the same nodes in the knlall partition. The preemptable queue is largely targeted towards users who need to run jobs but do not have sufficient time available in their projects or wish to stretch their allocations further. As the name implies, these jobs are preemptable immediately by other normal jobs if the partition (knlall) is full. The main advantage to users is that preemptable jobs are charged at 0.2 (20%) the normal core-hours. The user has to have an active project with some remaining time to be able to submit jobs to the preemptable queue. The rules for submitting jobs to the preemptable queue are as follows: Maximum job size: 6 nodes Maximum time: 24 hours Maximum number of running jobs per project: 1 A single user can submit multiple jobs to the preemptable queue if they have more than one project. If a user submits two preemptable jobs in any given project, only one of the jobs will run while the other is queued. As an example of the core-hour savings, a job run for 24 hours on 6 KNL nodes would be charged: 6x64x24x0.585 = 5391 core-hours (0.585 is the scaling factor for jobs run on the knl partition) whereas in the preemptable queue, the core-hours charged would be: 6x64x24x0.2 = 1843 core-hours (0.2 is the additional scaling factor for preemptable jobs). It is important that the user have checkpointing available and enabled so intermediate solutions are being saved at regular intervals as the job is running. If a job is preempted, the job can be automatically requeued so it can start again as new resources become available if the requeue flag is included during job submission. It is also important that users make appropriate changes to their scripts and input files to have the requeued job start from the latest saved solution. It is recommended that users periodically clean up their project spaces to delete intermediate solution files they might not require after the preemptable job is completed. Users wishing to submit jobs to the preemptable queue should include the line #SBATCH \u2013partition=knl-preemptable in their batch script. It is recommended that users include the following lines in their batch-scripts if they desire the job to be requeued and to be notified if their jobs have been preempted and requeued. This will enable them to make necessary changes to the input file to restart the jobs from the last saved solution (if need be). #SBATCH \u2013requeue #SBATCH \u2013mail-user=<your email address> #SBATCH \u2013mail-type=REQUEUE","title":"Running Jobs on Bebop"},{"location":"bebop/running-jobs-bebop/#running-jobs-on-bebop","text":"","title":"Running Jobs on Bebop"},{"location":"bebop/running-jobs-bebop/#overview","text":"Bebop's job scheduling system is characterized by: Uses Slurm Uses the legacy lcrc-sbank accounting system Allocations are calculated in core hours","title":"Overview"},{"location":"bebop/running-jobs-bebop/#partition-limits","text":"Bebop currently enforces the following limits on publicly available partitions: 32 Running Jobs per user. 100 Queued Jobs per user. 3 Days (72 Hours) Maximum Walltime on Broadwell Nodes. (bdws is 1 hour) 7 Days (168 Hours) Maximum Walltime on KNL Nodes. (knls is 4 hours) 1 Hour Default Walltime if not specified. bdwall (Broadwell Compute Nodes) is the default partition.","title":"Partition Limits"},{"location":"bebop/running-jobs-bebop/#available-partitions","text":"Bebop Partition Name Description Number of Nodes CPU Type Cores Per Node Memory Per Node Local Scratch Disk bdwall All Broadwell Nodes 664 Intel Xeon E5-2695v4 36 128GB DDR4 15 GB or 4 TB bdw Broadwell Nodes with 15 GB / scratch 600 Intel Xeon E5-2695v4 36 128GB DDR4 15 GB bdwd Broadwell Nodes with 4 TB / scratch 64 Intel Xeon E5-2695v4 36 128GB DDR4 4 TB bdws Broadwell Shared Nodes (Oversubscription / Non-Exclusive) 8 Intel Xeon E5-2695v4 36 128GB DDR4 15 GB knlall All Knights Landing Nodes 348 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 15 GB or 4 TB knl Knights Landing Nodes with 15GB /scratch 284 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 15 GB knld Knights Landing Nodes with 4TB /scratch 64 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 4 TB knls Knights Landing Shared Nodes (Oversubscription / Non-Exclusive) 4 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 15 GB knl-preemptable All Knights Landing Nodes with restrictions including preemption. Click here for more details . 348 Intel Xeon Phi 7230 64 96GB DDR4/16GB MCDRAM 15 GB or 4 TB","title":"Available Partitions"},{"location":"bebop/running-jobs-bebop/#submission-examples","text":"","title":"Submission Examples"},{"location":"bebop/running-jobs-bebop/#example-sbatch-job-submission-simple","text":"","title":"Example sbatch Job Submission (Simple)"},{"location":"bebop/running-jobs-bebop/#example-sbatch-job-submission-mpi","text":"","title":"Example sbatch Job Submission (MPI)"},{"location":"bebop/running-jobs-bebop/#example-interactive-job-submission","text":"","title":"Example Interactive Job Submission"},{"location":"bebop/running-jobs-bebop/#example-knights-landing-knl-sbatch-job-submission-mpi","text":"Attention KNL Users: Please note that if you wish to use a different set of modes for KNL other than Quadrant & Cache, you'll need to request a reservation that will require approval. You can request your reservation via this form . Depending on the amount of KNL nodes and the changes to be made, this could take a decent amount of time. Because this is using the resources, this time will also be charged against your core hour usage including the time it takes to complete the job. #!/bin/bash # SBATCH --job-name=<my_job_name> # SBATCH --account=<my_lcrc_project_name> # SBATCH --partition=knlall # SBATCH --constraint knl,quad,cache # Other modes require a reservation. # SBATCH --nodes=2 # SBATCH --ntasks-per-node=64 # SBATCH --output=<my_job_name>.out # SBATCH --error=<my_job_name>.error # SBATCH --mail-user=<your email address> # Optional if you require email # SBATCH --mail-type=ALL # Optional if you require email # SBATCH --time=01:00:00 # Setup My Environment module load intel-parallel-studio/cluster.2018.4-xtm134f export I_MPI_FABRICS = shm:tmi # Run My Program srun -n 128 ./helloworld This will run across two KNL nodes, using the Quadrant mode, and Cache MCDRAM mode. The default setting is quad,cache. A table of available settings, along with more detailed information about Slurm\u2019s KNL support is available here . You can then submit this job from a Bebop login nodes using: sbatch myjob.sh Please refer to the sbatch webpage for a list of full options including environment variables.","title":"Example Knights Landing (KNL) sbatch Job Submission (MPI)"},{"location":"bebop/running-jobs-bebop/#knl-preemptable-queue","text":"LCRC has set up a preemptable queue on the KNL partition (named as knl-preemptable). This covers all of the same nodes in the knlall partition. The preemptable queue is largely targeted towards users who need to run jobs but do not have sufficient time available in their projects or wish to stretch their allocations further. As the name implies, these jobs are preemptable immediately by other normal jobs if the partition (knlall) is full. The main advantage to users is that preemptable jobs are charged at 0.2 (20%) the normal core-hours. The user has to have an active project with some remaining time to be able to submit jobs to the preemptable queue. The rules for submitting jobs to the preemptable queue are as follows: Maximum job size: 6 nodes Maximum time: 24 hours Maximum number of running jobs per project: 1 A single user can submit multiple jobs to the preemptable queue if they have more than one project. If a user submits two preemptable jobs in any given project, only one of the jobs will run while the other is queued. As an example of the core-hour savings, a job run for 24 hours on 6 KNL nodes would be charged: 6x64x24x0.585 = 5391 core-hours (0.585 is the scaling factor for jobs run on the knl partition) whereas in the preemptable queue, the core-hours charged would be: 6x64x24x0.2 = 1843 core-hours (0.2 is the additional scaling factor for preemptable jobs). It is important that the user have checkpointing available and enabled so intermediate solutions are being saved at regular intervals as the job is running. If a job is preempted, the job can be automatically requeued so it can start again as new resources become available if the requeue flag is included during job submission. It is also important that users make appropriate changes to their scripts and input files to have the requeued job start from the latest saved solution. It is recommended that users periodically clean up their project spaces to delete intermediate solution files they might not require after the preemptable job is completed. Users wishing to submit jobs to the preemptable queue should include the line #SBATCH \u2013partition=knl-preemptable in their batch script. It is recommended that users include the following lines in their batch-scripts if they desire the job to be requeued and to be notified if their jobs have been preempted and requeued. This will enable them to make necessary changes to the input file to restart the jobs from the last saved solution (if need be). #SBATCH \u2013requeue #SBATCH \u2013mail-user=<your email address> #SBATCH \u2013mail-type=REQUEUE","title":"KNL Preemptable Queue"},{"location":"best-practices-and-policies/allocation-expiration-policy/","text":"Allocation Expiration Policy LCRC clusters are very heavily used, so the computer time your project has been given is a valuable resource. LCRC allocations are granted at the beginning of each fiscal quarter (generally at 10AM Central on the first non-weekend or Holiday of the new quarter). At the end of each fiscal quarter, any remaining time available in your project balances will be completely removed from your bank (excluding per user startup allocations). There is no carry over between quarters. Fiscal quarters are divided into the following time frames: 1st Quarter (October 1 \u2013 December 31) 2nd Quarter (January 1 \u2013 March 31) 3rd Quarter (April 1 \u2013 June 30) 4th Quarter (July 1 \u2013 September 30)","title":"Allocations Expiration Policy"},{"location":"best-practices-and-policies/allocation-expiration-policy/#allocation-expiration-policy","text":"LCRC clusters are very heavily used, so the computer time your project has been given is a valuable resource. LCRC allocations are granted at the beginning of each fiscal quarter (generally at 10AM Central on the first non-weekend or Holiday of the new quarter). At the end of each fiscal quarter, any remaining time available in your project balances will be completely removed from your bank (excluding per user startup allocations). There is no carry over between quarters. Fiscal quarters are divided into the following time frames: 1st Quarter (October 1 \u2013 December 31) 2nd Quarter (January 1 \u2013 March 31) 3rd Quarter (April 1 \u2013 June 30) 4th Quarter (July 1 \u2013 September 30)","title":"Allocation Expiration Policy"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/","text":"Best Practices and Policies Overview Monthly Maintenance Day Allocation Expiration Policy Cybersecurity Policy Data Policy Job Scheduling Policy Reservation Policy Shared Resource Policy SSH Policy","title":"Best Practices and Policies Overview"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/#best-practices-and-policies-overview","text":"","title":"Best Practices and Policies Overview"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/#monthly-maintenance-day","text":"","title":"Monthly Maintenance Day"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/#allocation-expiration-policy","text":"","title":"Allocation Expiration Policy"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/#cybersecurity-policy","text":"","title":"Cybersecurity Policy"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/#data-policy","text":"","title":"Data Policy"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/#job-scheduling-policy","text":"","title":"Job Scheduling Policy"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/#reservation-policy","text":"","title":"Reservation Policy"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/#shared-resource-policy","text":"","title":"Shared Resource Policy"},{"location":"best-practices-and-policies/best-practices-and-policies-overview/#ssh-policy","text":"","title":"SSH Policy"},{"location":"best-practices-and-policies/cybersecurity-policy/","text":"Cybersecurity Argonne National Laboratory has an aggressive cybersecurity program, combining expert staff with hardware and software capabilities that help protect against intrusion and detect attacks. Argonne\u2019s cybersecurity program is regularly reviewed by groups of external experts. Argonne staff also receive annual training in cybersecurity. While no site connected to the Internet is impervious, we have many safeguards. All interactions with LCRC computers use Secure Shell authentication, a cryptographic network protocol for secure data communication. For authorized users, access to data stored in LCRC is managed by standard Linux file access mechanisms, which enable the file owner to limit access to themselves or specific groups of users, typically the members of their project. We can also set up special groups, as needed. Note that the LCRC system administrators with root privileges are not constrained by the file permissions, and they have the capability to open and/or copy all files on the system. They can also assume a user\u2019s identity on the system. LCRC staff will never copy, expose, discuss, or in any other way communicate your project information to anyone outside of your project, the LCRC, or Argonne National Laboratory cybersecurity officials without your explicit permission unless required to do so by law. It is your responsibility to encrypt data if you wish to prevent its exposure under those circumstances. Generally speaking, administrators use these highly elevated privileges only when requested, or if a suspected problem/security issue exists. Following are instances where LCRC staff might look at your files: We may review .error, .output, and scheduler log files to determine if a job failure was due to user error or a system failure. If you request our assistance via any mechanism (for example, help ticket, direct personal email, in person, etc.), we interpret that request to be explicit permission to view your files if we think doing so will aid us in resolving your issue. We do not permit certain kinds of information on LCRC systems, including, but not limited to, personally identifiable information (data that falls under the Privacy Act of 1974, 5 U.S.C. 552a), classified information, unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), the design or development of nuclear, biological, or chemical weapons, or any weapons of mass destruction. The use of LCRC resources for personal or non-work-related activities is also prohibited. Project PIs are responsible for knowing whether their project generates any of these prohibited data types or information that falls under Export Control. Currently, the use of export-controlled codes is prohibited. For questions, please contact LCRC support, support@lcrc.anl.gov .","title":"Cybersecurity Policy"},{"location":"best-practices-and-policies/cybersecurity-policy/#cybersecurity","text":"Argonne National Laboratory has an aggressive cybersecurity program, combining expert staff with hardware and software capabilities that help protect against intrusion and detect attacks. Argonne\u2019s cybersecurity program is regularly reviewed by groups of external experts. Argonne staff also receive annual training in cybersecurity. While no site connected to the Internet is impervious, we have many safeguards. All interactions with LCRC computers use Secure Shell authentication, a cryptographic network protocol for secure data communication. For authorized users, access to data stored in LCRC is managed by standard Linux file access mechanisms, which enable the file owner to limit access to themselves or specific groups of users, typically the members of their project. We can also set up special groups, as needed. Note that the LCRC system administrators with root privileges are not constrained by the file permissions, and they have the capability to open and/or copy all files on the system. They can also assume a user\u2019s identity on the system. LCRC staff will never copy, expose, discuss, or in any other way communicate your project information to anyone outside of your project, the LCRC, or Argonne National Laboratory cybersecurity officials without your explicit permission unless required to do so by law. It is your responsibility to encrypt data if you wish to prevent its exposure under those circumstances. Generally speaking, administrators use these highly elevated privileges only when requested, or if a suspected problem/security issue exists. Following are instances where LCRC staff might look at your files: We may review .error, .output, and scheduler log files to determine if a job failure was due to user error or a system failure. If you request our assistance via any mechanism (for example, help ticket, direct personal email, in person, etc.), we interpret that request to be explicit permission to view your files if we think doing so will aid us in resolving your issue. We do not permit certain kinds of information on LCRC systems, including, but not limited to, personally identifiable information (data that falls under the Privacy Act of 1974, 5 U.S.C. 552a), classified information, unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), the design or development of nuclear, biological, or chemical weapons, or any weapons of mass destruction. The use of LCRC resources for personal or non-work-related activities is also prohibited. Project PIs are responsible for knowing whether their project generates any of these prohibited data types or information that falls under Export Control. Currently, the use of export-controlled codes is prohibited. For questions, please contact LCRC support, support@lcrc.anl.gov .","title":"Cybersecurity"},{"location":"best-practices-and-policies/data-policy/","text":"Data Policy Use of Proprietary/Licensed Software All software used on LCRC systems must be appropriately acquired and used according to the licensing agreements. Possession or use of illegally copied software is prohibited. Users shall not copy copyrighted software, unless explicitly permitted by the copyright holder(s). Export-controlled codes and/or data is prohibited unless an exception has been applied for and granted by the LCRC team. Users granted access to controlled projects must only work with the controlled codes and data in the project space made available for this purpose, and not in any shared spaces such as global scratch or temporary spaces. Prohibited Data The LCRC computer systems are operated as research systems and contain only data related to scientific research. Use of LCRC resources to store, manipulate, or remotely access any sensitive or national security information is prohibited unless documented and approved, by the PI and LCRC leadership. This includes, but is not limited to, personally identifiable information (data that falls under the Privacy Act of 1974, 5 U.S.C. 552a), controlled unclassified information (CUI) to include unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), International Traffic in Arms Relations (ITAR), the design or development of nuclear, biological, or chemical weapons, or any weapons of mass destruction. The use of LCRC resources for personal or non-work-related activities is also prohibited. Export Control All principal investigators using LCRC resources and LCRC staff members working with project teams are responsible for knowing whether their project generates any of these prohibited data types or information that falls under Export Control. For questions, contact the LCRC Support Team at support@lcrc.anl.gov . Data Storage Systems Data stored on LCRC resources should only be data directly related to work done on the LCRC systems. Currently, users have access to two storage spaces (1) home file system space (2) project space. The ability for projects to archive data to tape will also be available soon. The home file system space for each user is primarily intended to hold source codes, executable files, configuration files and other such files and is currently 100 GB. Beyond this 100 GB limit, write-access is denied and data will need to be deleted or moved to continue writes. All home directories are initially created with owner read/write/execute privileges. Users are free to give read/execute privileges to group/world if they desire on their own home directory. Group/world write access on home directories is prohibited. LCRC is not responsible for any access granted by the user beyond the defaults set at creation time. We recommend reviewing your home directory permissions at regular intervals. Data files from simulations should be stored in the project space allocated to each project. Each project is allocated 1 TB of space. PIs can request additional space with proper justification to LCRC. Each application is carefully reviewed by the LCRC team to determine if additional storage space is justified. It must be emphasized that the project space is shared between all members of a project. The project PIs should ensure that individual members of the project manage their simulation data files in order to not exceed the allocated storage space. LCRC has a soft limit of 1 TB and a hard limit of 2 TB. These limits imply that the system will allow a project to exceed the nominal soft limit of 1 TB but not the hard limit of 2 TB for 14 days. These flexible storage limits allows projects to complete simulations that might require higher storage for a period of 14 days without stopping an on-going simulation. However, no further writes are allowed after the project reaches the hard limit of 2 TB or is above the 1 TB limit beyond the 14-day period. When LCRC moves to a new shared storage system (roughly every 5-7 years) and if you have condo storage paid for on the old system, we have a couple of options. If your existing storage amounts to less than 50 terabytes, we will move your data over to the new storage for free, and there is no future charge. Otherwise, the lifetime of your purchased storage is 5 years after we first made the space available to the group. This may overlap the new storage availability. At the end of your 5 years, and if we have changed storage systems in the interim, your space reverts to the general pool (with advanced notice). Of course, you can buy new space to replace it if this is an option at the time. Backups/Tape Archiving Please see our storage writeup for complete details on our backup and archive solution. Storage policy for ex-project members Students, post-docs and Argonne staff members are encouraged to manage their home file systems and data files before their final departure day from Argonne. Please contact the support team at LCRC if you are unable to to do.","title":"Data Policy"},{"location":"best-practices-and-policies/data-policy/#data-policy","text":"","title":"Data Policy"},{"location":"best-practices-and-policies/data-policy/#use-of-proprietarylicensed-software","text":"All software used on LCRC systems must be appropriately acquired and used according to the licensing agreements. Possession or use of illegally copied software is prohibited. Users shall not copy copyrighted software, unless explicitly permitted by the copyright holder(s). Export-controlled codes and/or data is prohibited unless an exception has been applied for and granted by the LCRC team. Users granted access to controlled projects must only work with the controlled codes and data in the project space made available for this purpose, and not in any shared spaces such as global scratch or temporary spaces.","title":"Use of Proprietary/Licensed Software"},{"location":"best-practices-and-policies/data-policy/#prohibited-data","text":"The LCRC computer systems are operated as research systems and contain only data related to scientific research. Use of LCRC resources to store, manipulate, or remotely access any sensitive or national security information is prohibited unless documented and approved, by the PI and LCRC leadership. This includes, but is not limited to, personally identifiable information (data that falls under the Privacy Act of 1974, 5 U.S.C. 552a), controlled unclassified information (CUI) to include unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), International Traffic in Arms Relations (ITAR), the design or development of nuclear, biological, or chemical weapons, or any weapons of mass destruction. The use of LCRC resources for personal or non-work-related activities is also prohibited.","title":"Prohibited Data"},{"location":"best-practices-and-policies/data-policy/#export-control","text":"All principal investigators using LCRC resources and LCRC staff members working with project teams are responsible for knowing whether their project generates any of these prohibited data types or information that falls under Export Control. For questions, contact the LCRC Support Team at support@lcrc.anl.gov .","title":"Export Control"},{"location":"best-practices-and-policies/data-policy/#data-storage-systems","text":"Data stored on LCRC resources should only be data directly related to work done on the LCRC systems. Currently, users have access to two storage spaces (1) home file system space (2) project space. The ability for projects to archive data to tape will also be available soon. The home file system space for each user is primarily intended to hold source codes, executable files, configuration files and other such files and is currently 100 GB. Beyond this 100 GB limit, write-access is denied and data will need to be deleted or moved to continue writes. All home directories are initially created with owner read/write/execute privileges. Users are free to give read/execute privileges to group/world if they desire on their own home directory. Group/world write access on home directories is prohibited. LCRC is not responsible for any access granted by the user beyond the defaults set at creation time. We recommend reviewing your home directory permissions at regular intervals. Data files from simulations should be stored in the project space allocated to each project. Each project is allocated 1 TB of space. PIs can request additional space with proper justification to LCRC. Each application is carefully reviewed by the LCRC team to determine if additional storage space is justified. It must be emphasized that the project space is shared between all members of a project. The project PIs should ensure that individual members of the project manage their simulation data files in order to not exceed the allocated storage space. LCRC has a soft limit of 1 TB and a hard limit of 2 TB. These limits imply that the system will allow a project to exceed the nominal soft limit of 1 TB but not the hard limit of 2 TB for 14 days. These flexible storage limits allows projects to complete simulations that might require higher storage for a period of 14 days without stopping an on-going simulation. However, no further writes are allowed after the project reaches the hard limit of 2 TB or is above the 1 TB limit beyond the 14-day period. When LCRC moves to a new shared storage system (roughly every 5-7 years) and if you have condo storage paid for on the old system, we have a couple of options. If your existing storage amounts to less than 50 terabytes, we will move your data over to the new storage for free, and there is no future charge. Otherwise, the lifetime of your purchased storage is 5 years after we first made the space available to the group. This may overlap the new storage availability. At the end of your 5 years, and if we have changed storage systems in the interim, your space reverts to the general pool (with advanced notice). Of course, you can buy new space to replace it if this is an option at the time.","title":"Data Storage Systems"},{"location":"best-practices-and-policies/data-policy/#backupstape-archiving","text":"Please see our storage writeup for complete details on our backup and archive solution.","title":"Backups/Tape Archiving"},{"location":"best-practices-and-policies/data-policy/#storage-policy-for-ex-project-members","text":"Students, post-docs and Argonne staff members are encouraged to manage their home file systems and data files before their final departure day from Argonne. Please contact the support team at LCRC if you are unable to to do.","title":"Storage policy for ex-project members"},{"location":"best-practices-and-policies/job-scheduling-policy/","text":"Job Scheduling Policy The current LCRC job scheduling policy is pretty flexible, allowing people to submit nearly any kind of job mix into the job queue. We have implemented a policy that prevents more than 32 jobs for any one user from running at one time and we also enforce a 100 job limit per user. In addition, we have a process for handling overdrawn projects and a priority scheduling policy, both of which are described below. It should also be noted that we have maintenance days on the second Monday of every month. Therefore, if your job requires more time than there is remaining before the maintenance period, your job will be held until after the maintenance period. Priority Scheduling The priority scheduling policy is implemented using Maui queues. Jobs are submitted to a default queue, and Maui then routes the jobs to the appropriate priority queue based on the priority assigned to the job owner. The larger the priority number, the later a job will be run (i.e. a job with priority 1 will be run before a job with priority 2, if possible). Each user is assigned a priority based on their memberships in LCRC projects. If they are a member of multiple projects, they are assigned the largest priority number of the projects. The CSAC Allocations Committee has assigned a priority to each active LCRC project. If a project goes negative, its priority is set to priority level 5. For example, if project A has been assigned priority 3 by the committee and has a positive balance, jobs of members of project A are assigned priority 3. If project A goes negative, jobs of members of project A will then be assigned priority 5. Maui schedules jobs FIFO within each priority level. Priority 0 jobs are scheduled FIFO until no more priority 0 jobs will fit, then priority 1 jobs are scheduled FIFO until no more priority 1 jobs will fit, etc. Priority only affects jobs if not all jobs in the queue can be scheduled due to not enough resources. If two jobs of differing priority cannot both fit within existing resources, but either of them will fit, the job with the lower priority number will be run first. LCRC does not use job preemption so once a job has started, it will run until it is finished, aborts with an error, or is deleted by the owner or an administrator. General Scheduling Guidelines LCRC is expected to support all kinds of different usage. We do not want to unnecessarily limit the kinds of work that could be done on the system. We would prefer to maintain a flexible scheduling policy based on the needs of the community rather than impose a strict policy. As a consequence of this liberal scheduling policy, there have been instances in the past where people inappropriately loaded up the machine, resulting in delays for everyone else. Unfortunately, as LCRC becomes more heavily used, this requires increased monitoring. We ask that all users follow these guidelines: Follow good queue etiquette. This means, among other things, don\u2019t submit jobs that use a large number of nodes that run for a long time. If your job uses several hundred nodes, limit the run time to a few hours. Likewise, if your job runs for several hundred hours, limit the number of nodes to less than a dozen or so. We do encourage large jobs and if they need to run for awhile, this is okay. With others using the system, we only ask to you to be mindful of this and to break up runs when possible. Clearly, some work will require use of the machine beyond those bounds. In those cases, please send a quick note to support@lcrc.anl.gov so that we can be aware of these. If necessary, we can arrange for a reservation and notify the community. If we get well-founded complaints from other users of the system about your jobs, we will attempt to contact you to determine the best course of action. However, under some circumstances, we may have to kill running jobs. We don\u2019t want to do this, but it has been necessary a few times. We strongly encourage checkpointing. Checkpointing not only allows you to recover from a job that has died unexpectedly, but can also allow you to break a long-running job into smaller chunks that are therefore easier to schedule. Process for Overdrawn LCRC Projects The system automatically checks for LCRC projects that have exceeded their balance. The queue priority of overdrawn projects will drop automatically to priority level 5. For details on the scheduling policy, please see the priority scheduling section of this document. Overdrawn projects will be limited to running only one job per user at a time. A member of the LCRC staff will notify overdrawn projects via email (to all Project members) within one business day. We will also invite the PIs to submit a revised allocation request if needed. The PIs may request a change in the distribution of the current project allocation (e.g., some/all of their 2nd half time in the first half). LCRC staff may move up to 100K hours; larger requests need approval by the Allocations Committee. Alternately, the PIs may submit a revised allocation request, asking for more time. Increments of less than 200K hours will be managed by LCRC staff and reported to the Allocations Committee; larger requests need approval by the Allocations Committee. If no request is received, the overdrawn project will be suspended (unable to start new jobs) when its time use exceeds 100% overdrawn or 25K hours overdrawn, whichever comes first. This gives the project a cushion to finish up, to put in their revised request, or to make other arrangements. Projects are encouraged to contact LCRC staff when their project needs change, particularly when there are special needs. We will make every effort to find ways to meet your research needs. The Allocations Committee meets quarterly.","title":"Job Scheduling Policy"},{"location":"best-practices-and-policies/job-scheduling-policy/#job-scheduling-policy","text":"The current LCRC job scheduling policy is pretty flexible, allowing people to submit nearly any kind of job mix into the job queue. We have implemented a policy that prevents more than 32 jobs for any one user from running at one time and we also enforce a 100 job limit per user. In addition, we have a process for handling overdrawn projects and a priority scheduling policy, both of which are described below. It should also be noted that we have maintenance days on the second Monday of every month. Therefore, if your job requires more time than there is remaining before the maintenance period, your job will be held until after the maintenance period.","title":"Job Scheduling Policy"},{"location":"best-practices-and-policies/job-scheduling-policy/#priority-scheduling","text":"The priority scheduling policy is implemented using Maui queues. Jobs are submitted to a default queue, and Maui then routes the jobs to the appropriate priority queue based on the priority assigned to the job owner. The larger the priority number, the later a job will be run (i.e. a job with priority 1 will be run before a job with priority 2, if possible). Each user is assigned a priority based on their memberships in LCRC projects. If they are a member of multiple projects, they are assigned the largest priority number of the projects. The CSAC Allocations Committee has assigned a priority to each active LCRC project. If a project goes negative, its priority is set to priority level 5. For example, if project A has been assigned priority 3 by the committee and has a positive balance, jobs of members of project A are assigned priority 3. If project A goes negative, jobs of members of project A will then be assigned priority 5. Maui schedules jobs FIFO within each priority level. Priority 0 jobs are scheduled FIFO until no more priority 0 jobs will fit, then priority 1 jobs are scheduled FIFO until no more priority 1 jobs will fit, etc. Priority only affects jobs if not all jobs in the queue can be scheduled due to not enough resources. If two jobs of differing priority cannot both fit within existing resources, but either of them will fit, the job with the lower priority number will be run first. LCRC does not use job preemption so once a job has started, it will run until it is finished, aborts with an error, or is deleted by the owner or an administrator.","title":"Priority Scheduling"},{"location":"best-practices-and-policies/job-scheduling-policy/#general-scheduling-guidelines","text":"LCRC is expected to support all kinds of different usage. We do not want to unnecessarily limit the kinds of work that could be done on the system. We would prefer to maintain a flexible scheduling policy based on the needs of the community rather than impose a strict policy. As a consequence of this liberal scheduling policy, there have been instances in the past where people inappropriately loaded up the machine, resulting in delays for everyone else. Unfortunately, as LCRC becomes more heavily used, this requires increased monitoring. We ask that all users follow these guidelines: Follow good queue etiquette. This means, among other things, don\u2019t submit jobs that use a large number of nodes that run for a long time. If your job uses several hundred nodes, limit the run time to a few hours. Likewise, if your job runs for several hundred hours, limit the number of nodes to less than a dozen or so. We do encourage large jobs and if they need to run for awhile, this is okay. With others using the system, we only ask to you to be mindful of this and to break up runs when possible. Clearly, some work will require use of the machine beyond those bounds. In those cases, please send a quick note to support@lcrc.anl.gov so that we can be aware of these. If necessary, we can arrange for a reservation and notify the community. If we get well-founded complaints from other users of the system about your jobs, we will attempt to contact you to determine the best course of action. However, under some circumstances, we may have to kill running jobs. We don\u2019t want to do this, but it has been necessary a few times. We strongly encourage checkpointing. Checkpointing not only allows you to recover from a job that has died unexpectedly, but can also allow you to break a long-running job into smaller chunks that are therefore easier to schedule.","title":"General Scheduling Guidelines"},{"location":"best-practices-and-policies/job-scheduling-policy/#process-for-overdrawn-lcrc-projects","text":"The system automatically checks for LCRC projects that have exceeded their balance. The queue priority of overdrawn projects will drop automatically to priority level 5. For details on the scheduling policy, please see the priority scheduling section of this document. Overdrawn projects will be limited to running only one job per user at a time. A member of the LCRC staff will notify overdrawn projects via email (to all Project members) within one business day. We will also invite the PIs to submit a revised allocation request if needed. The PIs may request a change in the distribution of the current project allocation (e.g., some/all of their 2nd half time in the first half). LCRC staff may move up to 100K hours; larger requests need approval by the Allocations Committee. Alternately, the PIs may submit a revised allocation request, asking for more time. Increments of less than 200K hours will be managed by LCRC staff and reported to the Allocations Committee; larger requests need approval by the Allocations Committee. If no request is received, the overdrawn project will be suspended (unable to start new jobs) when its time use exceeds 100% overdrawn or 25K hours overdrawn, whichever comes first. This gives the project a cushion to finish up, to put in their revised request, or to make other arrangements. Projects are encouraged to contact LCRC staff when their project needs change, particularly when there are special needs. We will make every effort to find ways to meet your research needs. The Allocations Committee meets quarterly.","title":"Process for Overdrawn LCRC Projects"},{"location":"best-practices-and-policies/monthly-maintenance-day/","text":"Monthly Maintenance Day LCRC has the second Monday of every month set aside for system maintenance (software upgrades, hardware replacements, etc.) on all LCRC resources. Maintenance generally begins at 8AM CST. During this time, the cluster will be unavailable for all users. All compute nodes will be set offline and the login nodes may or may not be accessible (but may be rebooted at any time). We generally have the cluster available again by the end of the same day, however, depending on the scope of work to be done this could be longer. The compute nodes will be reserved for maintenance a week in advance. If any jobs are submitted that are scheduled to end after the maintenance period has begun, they will not run. Leading up to maintenance, if you need to run a job, please make sure the end time will be before the start of the maintenance period. We always send out a reminder email (usually the Wednesday before) of the maintenance start date and then another email immediately after the work is complete and the clusters are available again. These emails go to the LCRC Users email list. All LCRC users are subscribed to this list by default.","title":"Monthly Maintenance Day"},{"location":"best-practices-and-policies/monthly-maintenance-day/#monthly-maintenance-day","text":"LCRC has the second Monday of every month set aside for system maintenance (software upgrades, hardware replacements, etc.) on all LCRC resources. Maintenance generally begins at 8AM CST. During this time, the cluster will be unavailable for all users. All compute nodes will be set offline and the login nodes may or may not be accessible (but may be rebooted at any time). We generally have the cluster available again by the end of the same day, however, depending on the scope of work to be done this could be longer. The compute nodes will be reserved for maintenance a week in advance. If any jobs are submitted that are scheduled to end after the maintenance period has begun, they will not run. Leading up to maintenance, if you need to run a job, please make sure the end time will be before the start of the maintenance period. We always send out a reminder email (usually the Wednesday before) of the maintenance start date and then another email immediately after the work is complete and the clusters are available again. These emails go to the LCRC Users email list. All LCRC users are subscribed to this list by default.","title":"Monthly Maintenance Day"},{"location":"best-practices-and-policies/reservation-policy/","text":"Reservation Policy While the LCRC batch queues are designed to handle the vast majority of work, we recognize that there are special circumstances that require exclusive use of a portion of our resources for a period of time, e.g., to meet an urgent deadline, to carry out certain kinds of benchmarks or conduct workshops. Often we can meet such needs by reserving a specific number of nodes for your project for a specific period of time. To set up a reservation send an email to support@lcrc.anl.gov with the name of the partition/cluster, number of nodes, desired start time, and duration (hours). Based on the load on the partition/cluster at the time of the reservation request, an allocation decision will be made. In case a reservation for the total duration and/or nodes requested by the PI cannot be made, a partial reservation might be granted. Reservations must be requested at least 8 business days in advance and are subject to availability of resources. Reservations cannot be requested more than once per quarter per project. Typically, reservations can be for a maximum of 100,000 core hours. Reservations create substantial perturbations to the scheduling of other jobs long before the reservation starts. Once a reservation is scheduled it cannot be rescheduled, and if you end up not needing or using it, the project will still be charged for the reservation time. It is like a non-refundable movie ticket. If a reservation is required to host a workshop with hands-on user participation, please let us know well in advance. This will enable us to plan the reservation window(s) during the work-day(s) to ensure minimal disruption to the user community.","title":"Reservation Policy"},{"location":"best-practices-and-policies/reservation-policy/#reservation-policy","text":"While the LCRC batch queues are designed to handle the vast majority of work, we recognize that there are special circumstances that require exclusive use of a portion of our resources for a period of time, e.g., to meet an urgent deadline, to carry out certain kinds of benchmarks or conduct workshops. Often we can meet such needs by reserving a specific number of nodes for your project for a specific period of time. To set up a reservation send an email to support@lcrc.anl.gov with the name of the partition/cluster, number of nodes, desired start time, and duration (hours). Based on the load on the partition/cluster at the time of the reservation request, an allocation decision will be made. In case a reservation for the total duration and/or nodes requested by the PI cannot be made, a partial reservation might be granted. Reservations must be requested at least 8 business days in advance and are subject to availability of resources. Reservations cannot be requested more than once per quarter per project. Typically, reservations can be for a maximum of 100,000 core hours. Reservations create substantial perturbations to the scheduling of other jobs long before the reservation starts. Once a reservation is scheduled it cannot be rescheduled, and if you end up not needing or using it, the project will still be charged for the reservation time. It is like a non-refundable movie ticket. If a reservation is required to host a workshop with hands-on user participation, please let us know well in advance. This will enable us to plan the reservation window(s) during the work-day(s) to ensure minimal disruption to the user community.","title":"Reservation Policy"},{"location":"best-practices-and-policies/shared-resource-policy/","text":"Shared Resource Policy Use of Login Nodes LCRC login nodes are primarily for managing files in the project and home directories, editing files, submitting/monitoring jobs in the queue and compiling short codes/applications. Please do not run jobs on any LCRC login nodes. Not only do they slow down the system for other users but can also bring down the login node. Please use the cluster debug queues for testing and running short jobs or the batch queue for running long production jobs. The debug queue can be used in both the interactive mode and the batch mode. Use of Debugging Queues Some LCRC clusters have debug/shared queues. These queues are to be used only for debugging applications, testing the scaling/timing of a code/problem or checking to see if a new problem has been correctly set-up (proper input parameters, I/O file names etc). This queue is also extensively used by the LCRC staff to test new capabilities/codes, system performance, and address user issues. Debug queues on LCRC cluster are nodes with a maximum time-limit of 1 hour and often can be shared my multiple users. Please do not run hour-long production jobs on this queue. It is also recommended that users limit the maximum job size to 4 nodes whenever possible.","title":"Shared Resource Policy"},{"location":"best-practices-and-policies/shared-resource-policy/#shared-resource-policy","text":"","title":"Shared Resource Policy"},{"location":"best-practices-and-policies/shared-resource-policy/#use-of-login-nodes","text":"LCRC login nodes are primarily for managing files in the project and home directories, editing files, submitting/monitoring jobs in the queue and compiling short codes/applications. Please do not run jobs on any LCRC login nodes. Not only do they slow down the system for other users but can also bring down the login node. Please use the cluster debug queues for testing and running short jobs or the batch queue for running long production jobs. The debug queue can be used in both the interactive mode and the batch mode.","title":"Use of Login Nodes"},{"location":"best-practices-and-policies/shared-resource-policy/#use-of-debugging-queues","text":"Some LCRC clusters have debug/shared queues. These queues are to be used only for debugging applications, testing the scaling/timing of a code/problem or checking to see if a new problem has been correctly set-up (proper input parameters, I/O file names etc). This queue is also extensively used by the LCRC staff to test new capabilities/codes, system performance, and address user issues. Debug queues on LCRC cluster are nodes with a maximum time-limit of 1 hour and often can be shared my multiple users. Please do not run hour-long production jobs on this queue. It is also recommended that users limit the maximum job size to 4 nodes whenever possible.","title":"Use of Debugging Queues"},{"location":"best-practices-and-policies/ssh-policy/","text":"SSH Policy This document explains the policies users must follow when creating, storing, and using SSH keys for accessing the LCRC systems. Summary SSHv1 keys are not supported, therefore v1 keys are unusable. SSHv2 RSA keys are allowed. SSHv2 keys must contain at least 4096 bits. SSHv2 keys must have a strong passphrase (details below). Keys should be generated on a known secure machine. Private keys should not be stored on LCRC systems. Explanation The ability to crack SSHv2 keys depends directly on the type of key, the number of bits in the key, and the strength/quality and secrecy of the passphrase. The above guidelines are intended to minimize the risk of compromise if someone obtained a copy of your keys or was able to intercept your SSH session. SSH Passphrases SSHv2 keys must have a strong passphrase that: Is NOT a word in any language Is NOT a proper noun/name, brand name, foreign name, or other name Is NOT a word or set of words obtainable by: \"finger\" command (e.g. QWERTY), looking in your .plan, .project, or other public files, looking at any of your public web pages. Is NOT a word followed by a number. Is NOT a reversed version of any of the above. Is NOT an address (e.g. 9700S.Cass). Is NOT derived from a single word (e.g. He77o). Is a long passphrase, must be longer than 8 characters. SSH passphrases can be made up of several words or character strings. Unlike traditional Unix passwords that are limited to 8 characters, these can and should be much longer. However, it should not be so long that you are unable to type it correctly multiple times. Should contain mixed case letters, digits, and special characters. Should be kept secret and NOT written down anywhere. Should NOT be used as a password anywhere else, or as passphrases to other SSH keys. Failure to comply with the strong passphrase guidelines may make your passphrase guessable by people who are resourceful in finding information about you, or crackable using commonly available cracking software. You should NOT use the passphrase to your LCRC SSH key(s) as a password on any other machines. If you share passwords or passphrases between resources and one resource is compromised and passwords or passphrases are stolen, they could be used to compromise LCRC. If you learn of a security compromise at a remote site where you have an account please notify support@lcrc.anl.gov. NEVER GIVE YOUR PASSPHRASE TO ANYONE! Never tell anyone your passphrase or password over the phone. Nobody from LCRC will ask for your passphrase or password over the phone (we can access your account without it, system administrators never need to know your passphrase or password). If someone calls you and asks for your passphrase or password, please report this by sending mail to support@lcrc.anl.gov. If you receive an email from someone requesting your passphrase or password (including support@lcrc.anl.gov and root), please inform us immediately. NEVER WRITE YOUR PASSPHRASE OR PASSWORD DOWN. Make your passphrase unique but something you can remember so you don\u2019t have to write it down. Having a longer passphrase can help you remember it. If the piece of paper you write your passphrase down on is stolen, your SSH keys could be compromised. Storing SSH Key Pairs Protecting your ssh private keys is important: Set permissions so that only you have read/write access (i.e. chmod 600 <privatekeyfile> ). If you are a Cygwin user, please read the note about Cygwin Unix file permissions at the bottom of this page. Try to only have them on a single machine that is locked down very tightly (i.e. no remote access allowed, no sshd, ftpd, telnetd, etc running), firewall protected, up-to-date on all security patches, including kernel patches. Try not to have them on an NFS mounted file system. Public keys don\u2019t need to be protected: May be copied to other machines. May have permissions that allow others to read it. We recommend that the authorized_keys file have permissions of 600. Security of SSH keys depends on keeping both the private key and the passphrase secret. The best way to keep the private key secure is to store it on known secure machines like a personal laptop or workstation. When a machine is compromised the private keys on that machine are available to the hacker. It\u2019s very important to keep your private keys on as few machines as possible, to pick the most secure machines possible, and to avoid whenever possible storing them on machines and file-systems available to many users. If you wish to be even more secure, you can keep your keys on external media, such as a USB memory stick. Then when you wish to log onto a remote computer, mount the external media on your local system, add the key to your ssh-agent with a very short timeout (1 minute is reasonable), unmount the external media and open the connection to the remote system. This allows you get access to the remote system while keeping your private key virtually inaccessible to any remote hacker. For information on using ssh-agent, read the ssh-agent man page. Using SSH Key Pairs When you use a passphrased SSHv2 key the ssh client will prompt you for your passphrase. This passphrase is used on your local machine to decrypt your private key so it can be used to connect to the remote machine. The private key never leaves the client machine (in encrypted or decrypted form). For the remote machine to accept your ssh connection it must have your public key. Your SSHv2 public keys should be stored in the ~/.ssh/authorized_keys file. Instructions for setting up your LCRC keys are available here. If you ssh many times and you wish to avoid typing in the passphrase every time, you can use an ssh-agent. An ssh-agent allows your client machine to keep a decrypted form of your ssh private key in memory for use when ssh\u2019ing to multiple machines. For information on using ssh-agent, read the ssh-agent man page. You may use ssh-agent forwarding when connecting through one machine to another machine. But, because of security issues, you should only enable agent forwarding for connections where you will need it and shutdown the connection as soon as you are finished using it.","title":"SSH Policy"},{"location":"best-practices-and-policies/ssh-policy/#ssh-policy","text":"This document explains the policies users must follow when creating, storing, and using SSH keys for accessing the LCRC systems.","title":"SSH Policy"},{"location":"best-practices-and-policies/ssh-policy/#summary","text":"SSHv1 keys are not supported, therefore v1 keys are unusable. SSHv2 RSA keys are allowed. SSHv2 keys must contain at least 4096 bits. SSHv2 keys must have a strong passphrase (details below). Keys should be generated on a known secure machine. Private keys should not be stored on LCRC systems.","title":"Summary"},{"location":"best-practices-and-policies/ssh-policy/#explanation","text":"The ability to crack SSHv2 keys depends directly on the type of key, the number of bits in the key, and the strength/quality and secrecy of the passphrase. The above guidelines are intended to minimize the risk of compromise if someone obtained a copy of your keys or was able to intercept your SSH session.","title":"Explanation"},{"location":"best-practices-and-policies/ssh-policy/#ssh-passphrases","text":"SSHv2 keys must have a strong passphrase that: Is NOT a word in any language Is NOT a proper noun/name, brand name, foreign name, or other name Is NOT a word or set of words obtainable by: \"finger\" command (e.g. QWERTY), looking in your .plan, .project, or other public files, looking at any of your public web pages. Is NOT a word followed by a number. Is NOT a reversed version of any of the above. Is NOT an address (e.g. 9700S.Cass). Is NOT derived from a single word (e.g. He77o). Is a long passphrase, must be longer than 8 characters. SSH passphrases can be made up of several words or character strings. Unlike traditional Unix passwords that are limited to 8 characters, these can and should be much longer. However, it should not be so long that you are unable to type it correctly multiple times. Should contain mixed case letters, digits, and special characters. Should be kept secret and NOT written down anywhere. Should NOT be used as a password anywhere else, or as passphrases to other SSH keys. Failure to comply with the strong passphrase guidelines may make your passphrase guessable by people who are resourceful in finding information about you, or crackable using commonly available cracking software. You should NOT use the passphrase to your LCRC SSH key(s) as a password on any other machines. If you share passwords or passphrases between resources and one resource is compromised and passwords or passphrases are stolen, they could be used to compromise LCRC. If you learn of a security compromise at a remote site where you have an account please notify support@lcrc.anl.gov. NEVER GIVE YOUR PASSPHRASE TO ANYONE! Never tell anyone your passphrase or password over the phone. Nobody from LCRC will ask for your passphrase or password over the phone (we can access your account without it, system administrators never need to know your passphrase or password). If someone calls you and asks for your passphrase or password, please report this by sending mail to support@lcrc.anl.gov. If you receive an email from someone requesting your passphrase or password (including support@lcrc.anl.gov and root), please inform us immediately. NEVER WRITE YOUR PASSPHRASE OR PASSWORD DOWN. Make your passphrase unique but something you can remember so you don\u2019t have to write it down. Having a longer passphrase can help you remember it. If the piece of paper you write your passphrase down on is stolen, your SSH keys could be compromised.","title":"SSH Passphrases"},{"location":"best-practices-and-policies/ssh-policy/#storing-ssh-key-pairs","text":"Protecting your ssh private keys is important: Set permissions so that only you have read/write access (i.e. chmod 600 <privatekeyfile> ). If you are a Cygwin user, please read the note about Cygwin Unix file permissions at the bottom of this page. Try to only have them on a single machine that is locked down very tightly (i.e. no remote access allowed, no sshd, ftpd, telnetd, etc running), firewall protected, up-to-date on all security patches, including kernel patches. Try not to have them on an NFS mounted file system. Public keys don\u2019t need to be protected: May be copied to other machines. May have permissions that allow others to read it. We recommend that the authorized_keys file have permissions of 600. Security of SSH keys depends on keeping both the private key and the passphrase secret. The best way to keep the private key secure is to store it on known secure machines like a personal laptop or workstation. When a machine is compromised the private keys on that machine are available to the hacker. It\u2019s very important to keep your private keys on as few machines as possible, to pick the most secure machines possible, and to avoid whenever possible storing them on machines and file-systems available to many users. If you wish to be even more secure, you can keep your keys on external media, such as a USB memory stick. Then when you wish to log onto a remote computer, mount the external media on your local system, add the key to your ssh-agent with a very short timeout (1 minute is reasonable), unmount the external media and open the connection to the remote system. This allows you get access to the remote system while keeping your private key virtually inaccessible to any remote hacker. For information on using ssh-agent, read the ssh-agent man page.","title":"Storing SSH Key Pairs"},{"location":"best-practices-and-policies/ssh-policy/#using-ssh-key-pairs","text":"When you use a passphrased SSHv2 key the ssh client will prompt you for your passphrase. This passphrase is used on your local machine to decrypt your private key so it can be used to connect to the remote machine. The private key never leaves the client machine (in encrypted or decrypted form). For the remote machine to accept your ssh connection it must have your public key. Your SSHv2 public keys should be stored in the ~/.ssh/authorized_keys file. Instructions for setting up your LCRC keys are available here. If you ssh many times and you wish to avoid typing in the passphrase every time, you can use an ssh-agent. An ssh-agent allows your client machine to keep a decrypted form of your ssh private key in memory for use when ssh\u2019ing to multiple machines. For information on using ssh-agent, read the ssh-agent man page. You may use ssh-agent forwarding when connecting through one machine to another machine. But, because of security issues, you should only enable agent forwarding for connections where you will need it and shutdown the connection as soon as you are finished using it.","title":"Using SSH Key Pairs"},{"location":"data-management/lcrc-data-storage/","text":"LCRC Data Storage Storage Systems Access On all of our clusters, users have access to a global home, project, and group space (if they belong to a group that has purchased additional storage). This means that files created from compute nodes on one cluster can be used for calculations on any of our other clusters. All storage systems use GPFS as the file system and certain filesystems are backed up nightly. Filesystem Quotas Filesystem Location Soft Limit Hard Limit Home /home/<username> 100 GB 1 TB Project /lcrc/project/<project_name> 1+ TB 2+ TB Group /lcrc/group/<group_name> no quotas no quotas Additional Storage Options We also offer the ability for groups to purchase their own storage resources to be hosted with us. In doing so you get access to the storage space across all of our clusters (unless otherwise specified), and we take care of supporting the system, replacing parts, and when possible tuning the storage resources to fit your data model. Interested in more storage? Contact us at support@lcrc.anl.gov . Scratch Space You also have access to scratch space on the compute nodes while you have a job running on the node.","title":"LCRC Data Storage"},{"location":"data-management/lcrc-data-storage/#lcrc-data-storage","text":"","title":"LCRC Data Storage"},{"location":"data-management/lcrc-data-storage/#storage-systems-access","text":"On all of our clusters, users have access to a global home, project, and group space (if they belong to a group that has purchased additional storage). This means that files created from compute nodes on one cluster can be used for calculations on any of our other clusters. All storage systems use GPFS as the file system and certain filesystems are backed up nightly.","title":"Storage Systems Access"},{"location":"data-management/lcrc-data-storage/#filesystem-quotas","text":"Filesystem Location Soft Limit Hard Limit Home /home/<username> 100 GB 1 TB Project /lcrc/project/<project_name> 1+ TB 2+ TB Group /lcrc/group/<group_name> no quotas no quotas","title":"Filesystem Quotas"},{"location":"data-management/lcrc-data-storage/#additional-storage-options","text":"We also offer the ability for groups to purchase their own storage resources to be hosted with us. In doing so you get access to the storage space across all of our clusters (unless otherwise specified), and we take care of supporting the system, replacing parts, and when possible tuning the storage resources to fit your data model. Interested in more storage? Contact us at support@lcrc.anl.gov .","title":"Additional Storage Options"},{"location":"data-management/lcrc-data-storage/#scratch-space","text":"You also have access to scratch space on the compute nodes while you have a job running on the node.","title":"Scratch Space"},{"location":"data-management/data-transfer/sftp-scp/","text":"SFTP and SCP These standard utilities are available for local area transfers of small files; they are not recommended for use with large data transfers due to poor performance and excess resource utilization on the login nodes. See Globus for performing large data transfers.","title":"SFTP and SCP"},{"location":"data-management/data-transfer/sftp-scp/#sftp-and-scp","text":"These standard utilities are available for local area transfers of small files; they are not recommended for use with large data transfers due to poor performance and excess resource utilization on the login nodes. See Globus for performing large data transfers.","title":"SFTP and SCP"},{"location":"data-management/data-transfer/using-globus/","text":"Using Globus Log in to Globus using your Argonne Domain Account credentials by selecting Argonne National Laboratory from the existing organizational list. Alternatively, you can sign up for and use your local Globus username and password. Select File Manager from the side bar. On the File Manager page, you can view and search the list of available endpoints by clicking the text field labeled Collection . You can use the LCRC endpoint \"LCRC Improv DTN\" (as well as other sources or destinations). You can type letters into the box to filter endpoints. Once you select the LCRC endpoint: You will be prompted with an \"Authentication/Consent Required\" page. Click \"Continue\". On the \"Identity Required\" page, select your @anl.gov identity from the list. You may need to authenticate here with your Argonne credentials. Next, click \"Allow\" to give consent for the Globus Web App to manage transfers and access data. You will see a listing of the contents of your home directory in LCRC by default. Double click on a directory to view its contents. You can change the Path to another LCRC directory on the shared filesystem that you have access to as needed. Select a file or directory and click on the highlighted arrow button to initiate the transfer. Transferring Data Between LCRC and Your Machine To transfer data between LCRC and your laptop or desktop, you can install Globus Connect Personal on your machine and access it via Globus. Globus Connect Personal is available as a one-click install for Mac, Linux, and Windows. Install Globus Connect Personal following the instructions for your operating system. Run Globus Connect Personal on your local machine. The Globus Connect Personal webpage can walk you through the setup instructions. You should now see your machine in the list of endpoints on Globus. You can select it to view the contents of your machine and transfer files to and from it as above.","title":"Using Globus"},{"location":"data-management/data-transfer/using-globus/#using-globus","text":"Log in to Globus using your Argonne Domain Account credentials by selecting Argonne National Laboratory from the existing organizational list. Alternatively, you can sign up for and use your local Globus username and password. Select File Manager from the side bar. On the File Manager page, you can view and search the list of available endpoints by clicking the text field labeled Collection . You can use the LCRC endpoint \"LCRC Improv DTN\" (as well as other sources or destinations). You can type letters into the box to filter endpoints. Once you select the LCRC endpoint: You will be prompted with an \"Authentication/Consent Required\" page. Click \"Continue\". On the \"Identity Required\" page, select your @anl.gov identity from the list. You may need to authenticate here with your Argonne credentials. Next, click \"Allow\" to give consent for the Globus Web App to manage transfers and access data. You will see a listing of the contents of your home directory in LCRC by default. Double click on a directory to view its contents. You can change the Path to another LCRC directory on the shared filesystem that you have access to as needed. Select a file or directory and click on the highlighted arrow button to initiate the transfer.","title":"Using Globus"},{"location":"data-management/data-transfer/using-globus/#transferring-data-between-lcrc-and-your-machine","text":"To transfer data between LCRC and your laptop or desktop, you can install Globus Connect Personal on your machine and access it via Globus. Globus Connect Personal is available as a one-click install for Mac, Linux, and Windows. Install Globus Connect Personal following the instructions for your operating system. Run Globus Connect Personal on your local machine. The Globus Connect Personal webpage can walk you through the setup instructions. You should now see your machine in the list of endpoints on Globus. You can select it to view the contents of your machine and transfer files to and from it as above.","title":"Transferring Data Between LCRC and Your Machine"},{"location":"data-management/filesystem-and-storage/data-storage/","text":"Data Storage Overview Users have access to a global home, project, and group space. Files created on one cluster can be used across all clusters. All storage systems use GPFS as the file system. Certain filesystems are backed up nightly. Backup Storage Disk Drives Total Number of Drives Across All JBODs: 612 (6 JBODs x 102 Drives each) Drive Type: WDC HC530 SAS Enterprise Disk Drives Drive Capacity: 14TB each Uses ZFS for its the filesystem Maintenance and Data Integrity : Automated monthly scrubs are scheduled for each zpool. These scrubs are essential for maintaining data integrity and for early identification of potential issues. Tape Storage LCRC has roughly 760 tapes to which we backup and replicate data from the disk storage. We are currently running LTO9 tape technology. Purchasing Resources We offer the ability for groups to purchase their own storage resources to be hosted with us. In doing so you get access to the storage space across all of our clusters (unless otherwise specified), and we take care of supporting the system, replacing parts, and when possible tuning the storage resources to fit your data model. If you are interested in learning more about purchasing additional storage resources please contact us at support@lcrc.anl.gov You also have access to scratch space on the compute nodes while you have a job running on the node.","title":"Data Storage"},{"location":"data-management/filesystem-and-storage/data-storage/#data-storage","text":"","title":"Data Storage"},{"location":"data-management/filesystem-and-storage/data-storage/#overview","text":"Users have access to a global home, project, and group space. Files created on one cluster can be used across all clusters. All storage systems use GPFS as the file system. Certain filesystems are backed up nightly.","title":"Overview"},{"location":"data-management/filesystem-and-storage/data-storage/#backup-storage","text":"","title":"Backup Storage"},{"location":"data-management/filesystem-and-storage/data-storage/#disk-drives","text":"Total Number of Drives Across All JBODs: 612 (6 JBODs x 102 Drives each) Drive Type: WDC HC530 SAS Enterprise Disk Drives Drive Capacity: 14TB each Uses ZFS for its the filesystem Maintenance and Data Integrity : Automated monthly scrubs are scheduled for each zpool. These scrubs are essential for maintaining data integrity and for early identification of potential issues.","title":"Disk Drives"},{"location":"data-management/filesystem-and-storage/data-storage/#tape-storage","text":"LCRC has roughly 760 tapes to which we backup and replicate data from the disk storage. We are currently running LTO9 tape technology.","title":"Tape Storage"},{"location":"data-management/filesystem-and-storage/data-storage/#purchasing-resources","text":"We offer the ability for groups to purchase their own storage resources to be hosted with us. In doing so you get access to the storage space across all of our clusters (unless otherwise specified), and we take care of supporting the system, replacing parts, and when possible tuning the storage resources to fit your data model. If you are interested in learning more about purchasing additional storage resources please contact us at support@lcrc.anl.gov You also have access to scratch space on the compute nodes while you have a job running on the node.","title":"Purchasing Resources"},{"location":"data-management/filesystem-and-storage/disk-quota/","text":"Disk Quota Information Overview Quotas are enforced on home and project directories to manage storage resources effectively. Home and Project Directory Quotas Home Directories : Each user is allocated a standard quota of 100 GB. Project Directories : Projects are allocated a default of 1 TB, with the option to request increases. Quota Enforcement Soft Limits : Users can temporarily exceed their allocated quotas to avoid disrupting running jobs. Grace Period : Users have a two-week period to manage and reduce their data usage back within quota limits after exceeding the soft limit. Hard Limits : Persistent excess use beyond the grace period will result in write restrictions, preventing new data from being saved until the usage is reduced. Quota Table Filesystem Location Soft Limit Hard Limit Home /home/<username> 100 GB 1 TB Project /lcrc/project/<project_name> 1+ TB 2+ TB Group /lcrc/group/<group_name> No quotas No quotas Checking Your Quota To view your current quota usage, use the command lcrc-quota . $ lcrc-quota ---------------------------------------------------------------------------------------- Home Current Usage Space Avail Quota Limit Grace Time ---------------------------------------------------------------------------------------- UserX 113 GB -13 GB 100 GB 8 days ---------------------------------------------------------------------------------------- Project Current Usage Space Avail Quota Limit Grace Time ---------------------------------------------------------------------------------------- ImprovAcceptance 0 GB 1024 GB 1024 GB lcrc-app-engr 11231 GB 9248 GB 20480 GB mcnp_software 1 GB 1023 GB 1024 GB support 6442 GB 3781 GB 10240 GB Requesting Additional Storage Eligibility : Only available for project directories. Process Visit https://accounts.lcrc.anl.gov Under Owned projects, select the project requiring more storage. Input the desired storage amount in TB in the Storage (TB) field. Provide a justification under Storage Justification . Save changes to submit your request for review. Over-Quota Management If your home directory is over quota, consider deleting unnecessary files or moving data to project or group directories.","title":"Disk Quota"},{"location":"data-management/filesystem-and-storage/disk-quota/#disk-quota-information","text":"","title":"Disk Quota Information"},{"location":"data-management/filesystem-and-storage/disk-quota/#overview","text":"Quotas are enforced on home and project directories to manage storage resources effectively.","title":"Overview"},{"location":"data-management/filesystem-and-storage/disk-quota/#home-and-project-directory-quotas","text":"Home Directories : Each user is allocated a standard quota of 100 GB. Project Directories : Projects are allocated a default of 1 TB, with the option to request increases.","title":"Home and Project Directory Quotas"},{"location":"data-management/filesystem-and-storage/disk-quota/#quota-enforcement","text":"Soft Limits : Users can temporarily exceed their allocated quotas to avoid disrupting running jobs. Grace Period : Users have a two-week period to manage and reduce their data usage back within quota limits after exceeding the soft limit. Hard Limits : Persistent excess use beyond the grace period will result in write restrictions, preventing new data from being saved until the usage is reduced.","title":"Quota Enforcement"},{"location":"data-management/filesystem-and-storage/disk-quota/#quota-table","text":"Filesystem Location Soft Limit Hard Limit Home /home/<username> 100 GB 1 TB Project /lcrc/project/<project_name> 1+ TB 2+ TB Group /lcrc/group/<group_name> No quotas No quotas","title":"Quota Table"},{"location":"data-management/filesystem-and-storage/disk-quota/#checking-your-quota","text":"To view your current quota usage, use the command lcrc-quota . $ lcrc-quota ---------------------------------------------------------------------------------------- Home Current Usage Space Avail Quota Limit Grace Time ---------------------------------------------------------------------------------------- UserX 113 GB -13 GB 100 GB 8 days ---------------------------------------------------------------------------------------- Project Current Usage Space Avail Quota Limit Grace Time ---------------------------------------------------------------------------------------- ImprovAcceptance 0 GB 1024 GB 1024 GB lcrc-app-engr 11231 GB 9248 GB 20480 GB mcnp_software 1 GB 1023 GB 1024 GB support 6442 GB 3781 GB 10240 GB","title":"Checking Your Quota"},{"location":"data-management/filesystem-and-storage/disk-quota/#requesting-additional-storage","text":"Eligibility : Only available for project directories.","title":"Requesting Additional Storage"},{"location":"data-management/filesystem-and-storage/disk-quota/#process","text":"Visit https://accounts.lcrc.anl.gov Under Owned projects, select the project requiring more storage. Input the desired storage amount in TB in the Storage (TB) field. Provide a justification under Storage Justification . Save changes to submit your request for review.","title":"Process"},{"location":"data-management/filesystem-and-storage/disk-quota/#over-quota-management","text":"If your home directory is over quota, consider deleting unnecessary files or moving data to project or group directories.","title":"Over-Quota Management"},{"location":"data-management/filesystem-and-storage/file-systems/","text":"File Systems Home, Project, and Group Disks Your home, project, and group directories are located on separate GPFS filesystems that are shared by all nodes on the cluster. These filesystems are located on a raid array and are served by multiple file servers. This provides both a performance increase and protection against the filesystems being inaccessible. If one server goes down, the other servers can continue to serve the filesystems. Pros Global namespace Multi-TB filesystem Large file support (> 2GB) Backed up Raid protection Stable hardware Native InfiniBand support Cons Moderate performance Local and Global Scratch Disks Local If you need a place to put temporary files that don\u2019t need to be accessed by other nodes, we recommend that you put them into the local scratch disk on the nodes during job runs. All jobs create a job specific directory with local storage which can be referenced from your job submission script using the variable $TMPDIR . The normal publicly available nodes offer 15 GB of temporary scratch space while the \u2018biggpu\u2019 queue offers 1 TB. Diskfull Bebop nodes also house a 4TB disk on each node. Note that these spaces aren\u2019t backed up and the space will be cleared out on job end! Local Scratch Disk Pros Fast access times Large file support (> 2GB) Local Scratch Disk Cons Unique to each node; not shared between nodes GB filesystem Not backed up Cleared out at the end of your job No raid protection Global LCRC also has a global scratch space located at /lcrc/globalscratch. This space is a GPFS filesystem that is several TBs in size (the size may change over time). This space is shared to all LCRC nodes, unlike the local scratch space. Because this is a GPFS filesystem, IO will not be as fast. NOTE: This global scratch space may be cleaned up during our maintenance days or at other intervals and all files deleted in this space with or without notice. This space is not intended for long term storage and is not backed up in any way. Files deleted either accidentally or on purpose are permanently deleted. Global Scratch Disk Pros Shared between nodes Very large capacity Global Scratch Disk Cons Slower access time Not backed up Cleared out with or without notice No raid protection","title":"File Systems"},{"location":"data-management/filesystem-and-storage/file-systems/#file-systems","text":"","title":"File Systems"},{"location":"data-management/filesystem-and-storage/file-systems/#home-project-and-group-disks","text":"Your home, project, and group directories are located on separate GPFS filesystems that are shared by all nodes on the cluster. These filesystems are located on a raid array and are served by multiple file servers. This provides both a performance increase and protection against the filesystems being inaccessible. If one server goes down, the other servers can continue to serve the filesystems.","title":"Home, Project, and Group Disks"},{"location":"data-management/filesystem-and-storage/file-systems/#pros","text":"Global namespace Multi-TB filesystem Large file support (> 2GB) Backed up Raid protection Stable hardware Native InfiniBand support","title":"Pros"},{"location":"data-management/filesystem-and-storage/file-systems/#cons","text":"Moderate performance","title":"Cons"},{"location":"data-management/filesystem-and-storage/file-systems/#local-and-global-scratch-disks","text":"","title":"Local and Global Scratch Disks"},{"location":"data-management/filesystem-and-storage/file-systems/#local","text":"If you need a place to put temporary files that don\u2019t need to be accessed by other nodes, we recommend that you put them into the local scratch disk on the nodes during job runs. All jobs create a job specific directory with local storage which can be referenced from your job submission script using the variable $TMPDIR . The normal publicly available nodes offer 15 GB of temporary scratch space while the \u2018biggpu\u2019 queue offers 1 TB. Diskfull Bebop nodes also house a 4TB disk on each node. Note that these spaces aren\u2019t backed up and the space will be cleared out on job end!","title":"Local"},{"location":"data-management/filesystem-and-storage/file-systems/#local-scratch-disk-pros","text":"Fast access times Large file support (> 2GB)","title":"Local Scratch Disk Pros"},{"location":"data-management/filesystem-and-storage/file-systems/#local-scratch-disk-cons","text":"Unique to each node; not shared between nodes GB filesystem Not backed up Cleared out at the end of your job No raid protection","title":"Local Scratch Disk Cons"},{"location":"data-management/filesystem-and-storage/file-systems/#global","text":"LCRC also has a global scratch space located at /lcrc/globalscratch. This space is a GPFS filesystem that is several TBs in size (the size may change over time). This space is shared to all LCRC nodes, unlike the local scratch space. Because this is a GPFS filesystem, IO will not be as fast. NOTE: This global scratch space may be cleaned up during our maintenance days or at other intervals and all files deleted in this space with or without notice. This space is not intended for long term storage and is not backed up in any way. Files deleted either accidentally or on purpose are permanently deleted.","title":"Global"},{"location":"data-management/filesystem-and-storage/file-systems/#global-scratch-disk-pros","text":"Shared between nodes Very large capacity","title":"Global Scratch Disk Pros"},{"location":"data-management/filesystem-and-storage/file-systems/#global-scratch-disk-cons","text":"Slower access time Not backed up Cleared out with or without notice No raid protection","title":"Global Scratch Disk Cons"},{"location":"improv/getting-started-improv/","text":"Getting Started on Improv Accessing Improv To access Improv, use the following command: ssh <your_argonne_username>@improv.lcrc.anl.gov System Architecture For a detailed overview of the Improv system, including the compute node architecture, refer to the Hardware Overview page. Job Execution For information on how to run jobs on Improv, refer to the Running Jobs page.","title":"Getting Started"},{"location":"improv/getting-started-improv/#getting-started-on-improv","text":"","title":"Getting Started on Improv"},{"location":"improv/getting-started-improv/#accessing-improv","text":"To access Improv, use the following command: ssh <your_argonne_username>@improv.lcrc.anl.gov","title":"Accessing Improv"},{"location":"improv/getting-started-improv/#system-architecture","text":"For a detailed overview of the Improv system, including the compute node architecture, refer to the Hardware Overview page.","title":"System Architecture"},{"location":"improv/getting-started-improv/#job-execution","text":"For information on how to run jobs on Improv, refer to the Running Jobs page.","title":"Job Execution"},{"location":"improv/hardware-overview-improv/","text":"Improv Hardware Overview Improv has 825 dual-socket compute nodes with AMD 7713 64-core processors (2.0 GHz) (or 128 cores per node) and 256 GB DDR4 memory. 68 nodes have 6TB NVMe SSD, and 12 of those have 1024 GB DDR4, instead of 256 GB. The high-performance interconnect is Nvidia/Mellanox HDR200 (14 core, 35 edge switches). There are 12 HDR200 connections to LCRC\u2019s existing data storage system so you will have access to all of the same files between LCRC clusters. Improv Compute Nodes Improv Compute Description Per Node Aggregate Processor (Note 1) 2.0 GHz 7713 2 Sockets 1,650 Cores/Threads 64 Cores/1 Thread per core 128 105,600/105,600 Memory DDR4 256 GiB (Nodes i001-814) 1 TiB (Nodes i815-825) 219,648 GiB Local SSD 1 TB (Nodes i001-375, i444-813) 5.9 TB (Nodes i376-443, i814-825) 1 1,217 TB Note 1 L1d Cache (Level 1 Data Cache): 32 KB , L1i Cache (Level 1 Instruction Cache): 32 KB , L2 Cache: 512 KB , L3 Cache: 32,768 KB (or 32 MB) Improv Login Nodes There are four login nodes available to users for editing code, building code, submitting/monitoring jobs, checking usage ( sbank ), etc. Their full hostnames are iloginN.lcrc.anl.gov for N equal to 1 through 4 . The login nodes hardware is identical to the compute nodes. The various compilers and libraries are present on the logins, so most users should be able to build their code. All users share the same login nodes so please be courteous and respectful of your fellow users. For example, please do not run computationally or IO intensive pre- or post-processing on the logins and keep the parallelism of your builds to a reasonable level.","title":"Hardware Overview"},{"location":"improv/hardware-overview-improv/#improv-hardware-overview","text":"Improv has 825 dual-socket compute nodes with AMD 7713 64-core processors (2.0 GHz) (or 128 cores per node) and 256 GB DDR4 memory. 68 nodes have 6TB NVMe SSD, and 12 of those have 1024 GB DDR4, instead of 256 GB. The high-performance interconnect is Nvidia/Mellanox HDR200 (14 core, 35 edge switches). There are 12 HDR200 connections to LCRC\u2019s existing data storage system so you will have access to all of the same files between LCRC clusters.","title":"Improv Hardware Overview"},{"location":"improv/hardware-overview-improv/#improv-compute-nodes","text":"Improv Compute Description Per Node Aggregate Processor (Note 1) 2.0 GHz 7713 2 Sockets 1,650 Cores/Threads 64 Cores/1 Thread per core 128 105,600/105,600 Memory DDR4 256 GiB (Nodes i001-814) 1 TiB (Nodes i815-825) 219,648 GiB Local SSD 1 TB (Nodes i001-375, i444-813) 5.9 TB (Nodes i376-443, i814-825) 1 1,217 TB Note 1 L1d Cache (Level 1 Data Cache): 32 KB , L1i Cache (Level 1 Instruction Cache): 32 KB , L2 Cache: 512 KB , L3 Cache: 32,768 KB (or 32 MB)","title":"Improv Compute Nodes"},{"location":"improv/hardware-overview-improv/#improv-login-nodes","text":"There are four login nodes available to users for editing code, building code, submitting/monitoring jobs, checking usage ( sbank ), etc. Their full hostnames are iloginN.lcrc.anl.gov for N equal to 1 through 4 . The login nodes hardware is identical to the compute nodes. The various compilers and libraries are present on the logins, so most users should be able to build their code. All users share the same login nodes so please be courteous and respectful of your fellow users. For example, please do not run computationally or IO intensive pre- or post-processing on the logins and keep the parallelism of your builds to a reasonable level.","title":"Improv Login Nodes"},{"location":"improv/running-jobs-improv/","text":"Running Jobs on Improv Quickstart Presented below are fundamental commands essential for day-to-day use by most LCRC users on Improv. Comprehensive guides are available in other sections linked within our documentation. Check your Current Allocation Balance(s): sbank-list-allocations -p <project_name> Check your Filesystem Quota(s): lcrc-quota Submit a Batch Job: qsub -A <project> <your job script> List All Jobs: qstat Delete a Job: qdel <jobid> Job Scheduling System Improv's job scheduling system is characterized by: Uses PBS Pro Uses the sbank accounting system Allocations are calculated in node hours Queues Use the -q option with qsub to select a queue. The default queue is compute . We allow up to 15 jobs per user to run at the same time while 100 total jobs can be queued to run. Improv Queue Name Description Number of Nodes CPU Type Cores Per Node Memory Per Node Local Scratch Disk Max Walltime compute Standard Compute Nodes 805 2x AMD EPYC 7713 64-Core Processor 128 256GB DDR4 960GB (6TB bigdata Nodes) 72 Hours (3 Days) bigmem Large Memory Compute Nodes 12 2x AMD EPYC 7713 64-Core Processor 128 1TB DDR4 6TB 72 Hours (3 Days) debug Reduced Walltime Compute Nodes 8 2x AMD EPYC 7713 64-Core Processor 128 256GB DDR4 960GB 1 Hour The compute queue also has 68 nodes with a 6TB local NVMe scratch disk. You can request these directly by adding bigdata=true to your PBS select statement. For example: #PBS -l select=8:ncpus=128:mpiprocs=128:bigdata=true Running MPI Applications OpenMPI is the recommended MPI for use on Improv. Here are some parameters that might be useful: -np <processes> : This specifies the number of processes to launch. It should match the number of processors or cores you've requested in your job script. If you plan to use all the cores on each node you are allocated, this is not necessary in a submission script. The #PBS -l select=<# of nodes>:ncpus=128:mpiprocs=128 script header takes care of this. --display-map : This displays a map showing where MPI processes are running. --report-bindings : This reports how MPI processes are bound to cores or sockets. For the most accurate and detailed information, please refer to the OpenMPI documentation . #!/bin/bash -l # UG Section 2.5, page UG-24 Job Submission Options # Note: Command line switches will override values in this file. # ----------------- PBS Directives ----------------- # # These options are mandatory in LCRC; qsub will fail without them. # select: number of nodes. Adjust these values as per your job's requirement. In the below example, 4 nodes are requested. # ncpus: number of cores per node. (use 128 unless you have a reason otherwise) # mpiprocs: number of MPI processes (use the same value as ncpus unless you have a reason otherwise ). # walltime: a limit on the total time from the start to the completion of a job #PBS -A <project_name> #PBS -l select=4:ncpus=128:mpiprocs=128 #PBS -l walltime=HH:MM:SS # Queue for the job submission #PBS -q <queue> # Job name (first 15 characters are displayed in qstat) #PBS -N <job_name> # Output options: 'oe' to merge stdout/stderr into stdout, 'eo' for stderr, 'n' to not merge. #PBS -j n # Email notifications: 'b' at begin, 'e' at end, 'a' on abort. Remove 'n' for no emails. #PBS -m be #PBS -M <your_email_address> # --------------- Script Execution Part --------------- # # The following is an example setup for an MPI job. echo \"Working directory: $PBS_O_WORKDIR \" cd $PBS_O_WORKDIR echo \"Job ID: $PBS_JOBID \" echo \"Running on host: $( hostname ) \" echo \"Running on nodes: $( cat $PBS_NODEFILE ) \" # Loading GCC and MPI modules, and verifying the environment. module load gcc openmpi module list which mpirun # Run the MPI program mpirun ./hello_mpi Important things to note: PBS Pro on Improv is currently not configured to allow sharing nodes. When have a node allocated to you, you receive the ENTIRE node. Ensure that you use all of your allocated nodes' resources unless you have a reason not to. More information about running jobs on PBS can be found on our Running Jobs on PBS Clusters page.","title":"Running Jobs on Improv"},{"location":"improv/running-jobs-improv/#running-jobs-on-improv","text":"","title":"Running Jobs on Improv"},{"location":"improv/running-jobs-improv/#quickstart","text":"Presented below are fundamental commands essential for day-to-day use by most LCRC users on Improv. Comprehensive guides are available in other sections linked within our documentation. Check your Current Allocation Balance(s): sbank-list-allocations -p <project_name> Check your Filesystem Quota(s): lcrc-quota Submit a Batch Job: qsub -A <project> <your job script> List All Jobs: qstat Delete a Job: qdel <jobid>","title":"Quickstart"},{"location":"improv/running-jobs-improv/#job-scheduling-system","text":"Improv's job scheduling system is characterized by: Uses PBS Pro Uses the sbank accounting system Allocations are calculated in node hours","title":"Job Scheduling System"},{"location":"improv/running-jobs-improv/#queues","text":"Use the -q option with qsub to select a queue. The default queue is compute . We allow up to 15 jobs per user to run at the same time while 100 total jobs can be queued to run. Improv Queue Name Description Number of Nodes CPU Type Cores Per Node Memory Per Node Local Scratch Disk Max Walltime compute Standard Compute Nodes 805 2x AMD EPYC 7713 64-Core Processor 128 256GB DDR4 960GB (6TB bigdata Nodes) 72 Hours (3 Days) bigmem Large Memory Compute Nodes 12 2x AMD EPYC 7713 64-Core Processor 128 1TB DDR4 6TB 72 Hours (3 Days) debug Reduced Walltime Compute Nodes 8 2x AMD EPYC 7713 64-Core Processor 128 256GB DDR4 960GB 1 Hour The compute queue also has 68 nodes with a 6TB local NVMe scratch disk. You can request these directly by adding bigdata=true to your PBS select statement. For example: #PBS -l select=8:ncpus=128:mpiprocs=128:bigdata=true","title":"Queues"},{"location":"improv/running-jobs-improv/#running-mpi-applications","text":"OpenMPI is the recommended MPI for use on Improv. Here are some parameters that might be useful: -np <processes> : This specifies the number of processes to launch. It should match the number of processors or cores you've requested in your job script. If you plan to use all the cores on each node you are allocated, this is not necessary in a submission script. The #PBS -l select=<# of nodes>:ncpus=128:mpiprocs=128 script header takes care of this. --display-map : This displays a map showing where MPI processes are running. --report-bindings : This reports how MPI processes are bound to cores or sockets. For the most accurate and detailed information, please refer to the OpenMPI documentation . #!/bin/bash -l # UG Section 2.5, page UG-24 Job Submission Options # Note: Command line switches will override values in this file. # ----------------- PBS Directives ----------------- # # These options are mandatory in LCRC; qsub will fail without them. # select: number of nodes. Adjust these values as per your job's requirement. In the below example, 4 nodes are requested. # ncpus: number of cores per node. (use 128 unless you have a reason otherwise) # mpiprocs: number of MPI processes (use the same value as ncpus unless you have a reason otherwise ). # walltime: a limit on the total time from the start to the completion of a job #PBS -A <project_name> #PBS -l select=4:ncpus=128:mpiprocs=128 #PBS -l walltime=HH:MM:SS # Queue for the job submission #PBS -q <queue> # Job name (first 15 characters are displayed in qstat) #PBS -N <job_name> # Output options: 'oe' to merge stdout/stderr into stdout, 'eo' for stderr, 'n' to not merge. #PBS -j n # Email notifications: 'b' at begin, 'e' at end, 'a' on abort. Remove 'n' for no emails. #PBS -m be #PBS -M <your_email_address> # --------------- Script Execution Part --------------- # # The following is an example setup for an MPI job. echo \"Working directory: $PBS_O_WORKDIR \" cd $PBS_O_WORKDIR echo \"Job ID: $PBS_JOBID \" echo \"Running on host: $( hostname ) \" echo \"Running on nodes: $( cat $PBS_NODEFILE ) \" # Loading GCC and MPI modules, and verifying the environment. module load gcc openmpi module list which mpirun # Run the MPI program mpirun ./hello_mpi Important things to note: PBS Pro on Improv is currently not configured to allow sharing nodes. When have a node allocated to you, you receive the ENTIRE node. Ensure that you use all of your allocated nodes' resources unless you have a reason not to. More information about running jobs on PBS can be found on our Running Jobs on PBS Clusters page.","title":"Running MPI Applications"},{"location":"running-jobs-at-lcrc/overview/","text":"Running Jobs at LCRC LCRC has three generally accessible high-performance computing clusters: Bebop, Swing, and the latest addition, Improv. These clusters are designed to cater to a wide range of computational needs and research projects. To effectively manage and schedule computational tasks, LCRC employs two advanced job scheduling systems: Slurm : This is the scheduling system for the Bebop and Swing clusters. PBS Pro : With an eye towards future alignment with the ALCF, LCRC has adopted PBS Pro for the Improv cluster. For detailed guidelines on utilizing these systems, please refer to the following resources: Running Jobs on Slurm Clusters Running Jobs on PBS Pro Clusters","title":"Overview"},{"location":"running-jobs-at-lcrc/overview/#running-jobs-at-lcrc","text":"LCRC has three generally accessible high-performance computing clusters: Bebop, Swing, and the latest addition, Improv. These clusters are designed to cater to a wide range of computational needs and research projects. To effectively manage and schedule computational tasks, LCRC employs two advanced job scheduling systems: Slurm : This is the scheduling system for the Bebop and Swing clusters. PBS Pro : With an eye towards future alignment with the ALCF, LCRC has adopted PBS Pro for the Improv cluster. For detailed guidelines on utilizing these systems, please refer to the following resources: Running Jobs on Slurm Clusters Running Jobs on PBS Pro Clusters","title":"Running Jobs at LCRC"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/","text":"Running Jobs on PBS Clusters This document complements the information provided on the Improv Cluster page, forming a comprehensive resource for running jobs effectively. Both pages should be referenced for a complete understanding. Obtaining and Managing Compute Resources Definitions and Notes chunk : A set of resources allocated as a unit to a job. Specified inside a selection directive. All parts of a chunk come from the same host. In a typical MPI (Message-Passing Interface) job, there is one chunk per MPI process. vnode : A virtual node, or vnode, is an abstract object representing a host or a set of resources which form a usable part of an execution host. This could be an entire host, or a nodeboard or a blade. A single host can be made up of multiple vnodes. Each vnode can be managed and scheduled independently. Each vnode in a complex must have a unique name. Vnodes on a host can share resources, such as node-locked licenses. PBS operates on vnodes. A vnode on LCRC Improv will represent an entire host . ncpus : Number of resources available to execute a program. Each Improv node has 128 cores. Users are encouraged to use all of the cores on the node and pack nodes with multiple jobs when possible. job : A job equates to a qsub. A set of resources allocated to you for a period of time. Your will execute one or more tasks on those resources during your job. task : A single execution on the resources of your job, often an mpirun invocation. You may run one task or many tasks during your job. You may run tasks sequentially or divide your resources up and run several tasks concurrently. Also sometimes referred to as job steps. Quick Start If you are an LCRC user and are familiar with Slurm, you will find the PBS Pro commands very similar though the options to qsub are quite different. We have added a handy conversion \u201ccheat sheet\u201d here. Here are the \u201cBig Four\u201d commands you will use with PBS Pro: qsub : request resources (generally compute nodes) to run your job and start your script/executable on the head node. Here is the minimal qsub allowed at the LCRC: qsub -A <project> -l select=<# of nodes>,walltime=HH:MM:SS <your job script> The -A and walltime options are mandatory. You will receive errors if they are not specified. We automatically add -l place=scatter for you so that each of your chunks ( <# of nodes> ) gets its own vnode. -q <queue_name> will place your job on the correct queue depending on the node type you want. The compute queue is the default on Improv. If you want to run an executable rather than a script replace <your jobs script> in the example above with -- <your executable> (that is dash dash) pbsq : a user-friendly filter for qstat to view the status of jobs and queues on the cluster. qstat : view the status of jobs and queues on the cluster. Try these variations and see which you like best: qstat, qstat -was, qstat -was1, qstat -wan, qstat -wan1 . Add -x to see jobs that have completed. We keep one week of history. qalter : update your request for resources Just like qsub , just add a jobid at the end. Only works before the job starts; If you want to change the walltime to 30 minutes: qalter -l walltime=30:00:00 <jobid> qdel : cancel a job that you don\u2019t need. This will also kill a running job. qdel <jobid> Note: The page numbers in the PBS guides are unique. If you search for the specified page number it will take you directly to the relevant page. qsub: Submit a job to run At the LCRC, your qsub will likely use the following parameters: qsub -A <project> -l select=<#>,walltime=HH:MM:SS <your job script> Where: <project> is the project name associated with your allocation. What you check the balance of with the sbank command. This is a mandatory option at the LCRC. If you don\u2019t include it you will get qsub: Account_Name is required to be set . walltime=HH:MM:SS specifying a wall time is mandatory at the LCRC. Jobs on Improv can run up to 72 hours maximum. <your job script> : For options that won\u2019t change, you do have the option of taking things off the command line and putting them in your job script. For instance the above command line could be simplified to qsub -l select=<#> <your job script> if you added the following to the top (the PBS directives have to be before any executable line) of your job script: #PBS -A <project> #PBS -l walltime=HH:MM:SS Also note that if you want to run an executable directly rather than a script you use two dashes and the executable name in place of your script name like this: -- /usr/bin/sleep 600 Resource Selection and Job Placement Resources come in two flavors: Job Wide: Walltime is the most common example of a job wide resource. You use the -l option to specify job wide resources, i.e. -l walltime=06:00:00 . All the resources in the job have the same walltime. -l <resource name>=<value>[,<resource name>=<value> ...] Chunks: (see the definition above) This is how you describe what your needs are to run your job. You do this with the -l select= syntax. In the LCRC, we do whole node scheduling. This means you can typically get away with the very simple -l select=4 which will give you 4 nodes with all of the available cpus. <resource name>=<value>[:<resource name>=<value> ...] You can also tell PBS how you want the chunks distributed across the physical hardware. You do that via the -l place option (scatter is our default): -l place=[<arrangement>][: <sharing> ][: <grouping>] where arrangement is one of free | pack | scatter unless you have a specific reason to do otherwise, you probably want to set this to scatter, otherwise you may not get what you expect. For instance on a host with ncpus=128, if you requested -l select=8:ncpus=8:mpiprocs=8 you could end up with all of our chunks on one node. free means PBS can distribute them as it sees fit pack means all chunks from one host. Note that this is not the minimum number of hosts, it is one host. If the chunks can\u2019t fit on one host, the qsub will fail. scatter means take only one chunk from any given host. Here is a heavily commented sample PBS submission script that shows some more of the options, but remember that the PBS manuals referenced at the bottom of this page are the ultimate resource. If you create a file named hello.pbs for example, you can add: #!/bin/bash -l # Add another # at the beginning of the line to comment out a line # NOTE: adding a switch to the command line will override values in this file. # These options are MANDATORY in LCRC; Your qsub will fail if you don't provide them. #PBS -A <project_name> #PBS -l select=4:mpiprocs=128 #PBS -l walltime=HH:MM:SS # Highly recommended # The first 15 characters of the job name are displayed in the qstat output: #PBS -N <job_name> # If you want to merge stdout and stderr, use the -j option # oe=merge stdout/stderr to stdout, eo=merge stderr/stdout to stderr, n=don't merge #PBS -j n # Controlling email notifications # When to send email b=job begin, e=job end, a=job abort, j=subjobs (job arrays), n=no mail #PBS -m be #PBS -M <your_email_address> # The rest is an example of how an MPI job might be set up echo Working directory is $PBS_O_WORKDIR cd $PBS_O_WORKDIR echo Jobid: $PBS_JOBID echo Running on host ` hostname ` echo Running on nodes ` cat $PBS_NODEFILE ` module load gcc openmpi # Load a GCC and MPI module module list # Print all loaded modules which mpirun # Show which mpirun executable we are using mpirun ./hello_mpi # Run a script You would be able to submit the above script with: qsub hello.pbs qsub example If you don\u2019t want to use a script, you could submit via the command line as well. For example: qsub -A <project_name> -l select=4:mpiprocs=128 -l walltime=30:00 -- a.out run a.out on 4 chunks with a walltime of 30 minutes ; charge project_name; run on 128 CPUs (ncpus) per node. On Improv by default, we allocate 128 cpus per node. Set ncpus if you want less. All jobs will be charged a whole node no matter how many cpus are requested. Allocate 128 MPI slots (mpiprocs) per node. Note: When using MPI, you must specify mpiprocs. We recommend it to be the same as ncpus. Since we allocate full nodes on Improv, 4 chunks will be 4 nodes. If we shared nodes, that would be 4 threads. use the -- (dash dash) syntax when directly running an executable. pbsq: A user-friendly filter for qstat pbsq is a user-friendly filter for qstat to view the status of jobs and queues on the cluster. It provides a more user-friendly interface for qstat by providing a more intuitive way to view job and queue status. pbsq basic usage Query all jobs that are running and queued: pbsq Query all jobs that belong to a project: pbsq -f <project_name> Query all jobs that belong to a user: pbsq -f <user_name> Use pbsq -h to see the help menu and other options. qstat: Query the status of jobs/queues The qstat command lists jobs on the system, showing their status, user, and other details. You can specify job IDs for detailed information about specific jobs. qstat basic usage qstat : Lists all jobs with basic details. qstat [jobID] : Displays information for specific jobs. Options for expanded information: -w : Expands the width of the output to reduce truncation. -was1 : Shows nodes, tasks, requested walltime, and comments in a single line. -wan : Provides the list of nodes used. -T : Estimates the start time for the job (only for the next expected jobs). -f : Provides comprehensive details about a job. -x : Shows finished jobs (history retained for one week) along with comments. The comment field is crucial for understanding job status or issues. It's often the first place to check when troubleshooting job-related queries in PBS. qalter: Alter a queued job Basically takes the same options as qsub ; Say you typoed and set the walltime to 300 minutes instead of 30 minutes. You could fix it (if the job had not started running) by doing qalter -l walltime=30:00 <jobid> [<jobid> <jobid>...] The new value overwrites any previous value. qdel: Delete a queued or running job qdel <jobid> [<jobid> <jobid>...] qhold,qrls: Place / release a user hold on a job [qhold | qrls] <jobid> [<jobid> <jobid>...] qselect: Query jobids for use in commands qdel $(qselect -N test1) will delete all the jobs that had the job name set to test1. qmsg: Write a message into a jobs output file qmsg -E -O \"This is the message\" <jobid> [<jobid> <jobid>...] -E writes it to standard error, -O writes it to standard out qsig: Send a signal to a job qsig -s <signal> <jobid> [<jobid> <jobid>...] If you don\u2019t specify a signal, SIGTERM is sent. pbsnodes: Get information about the current state of nodes This is more for admins, but it can tell you more about the nodes themselves. pbsnodes <node name> : Everything there is to know about a node pbsnodes -avSj : A nice table to see what is free and in use pbsnodes -l : (lowercase l) see which nodes are down. The comment often indicates why it is down Job Priority In PBS it is not easy to see a priority order for which jobs will run next. The best way is to use the -T option on qsub and look at the estimated start times. LCRC runs a custom scheduler algorithm, but in general, the job priority in the queue is based on several criteria: positive balance of your project size (in nodes) of the job, larger jobs receive higher priority job duration: shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible General PBS Example Scripts/Commands Submitting an Interactive Job Here is how to submit an interactive job to, for example, edit/build/test an application: qsub -I -A <PROJECT_NAME> -l select=1:ncpus=128:mpiprocs=128,walltime=01:00:00 -q compute This command requests 1 node for a period of 1 hour in the compute queue. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. Remember to replace <PROJECT_NAME> with a valid LCRC project. Submitting a PBS Array Job #!/bin/bash ###### Tells PBS the job name #PBS -N array_example ###### Tells PBS to run on 1 node with 128 cpus #PBS -l select=1:ncpus=128:mpiprocs=128 ###### Tells PBS the walltime (Max 72 hours) #PBS -l walltime=00:05:00 ###### Tells PBS the project to charge (Replace with a valid project) #PBS -A <PROJECT_NAME> ###### Tells PBS to run 5 subjobs (Required to run in an array) #PBS -J 1-5 ###### Tells PBS to rerun the job (Rquired to run in an array) #PBS -r y cd $PBS_O_WORKDIR echo \"Running subjob $PBS_ARRAY_INDEX \" # Create a subjob-specific directory mkdir -p subjob_ ${ PBS_ARRAY_INDEX } cd subjob_ ${ PBS_ARRAY_INDEX } # Run a command unique to each subjob echo \"This is subjob ${ PBS_ARRAY_INDEX } \" > output_ ${ PBS_ARRAY_INDEX } .txt # Sleep for a different amount of time based on the subjob index sleep ${ PBS_ARRAY_INDEX } echo \"Subjob $PBS_ARRAY_INDEX completed\" The pbsq command subsequently shows the status of the job array. $ pbsq 138617[].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 B compute array_example Job Array Began at Thu Mar 21 at 12:02 138617[1].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i475 138617[2].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i730 138617[3].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i799 138617[4].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i285 138617[5].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i286 If you want to increase the number of nodes per subjob, you can increase the select value in #PBS -l select=1:ncpus=128 to be equal to the number of nodes you want per subjob. Running Multiple MPI Applications on a Node Multiple applications can be run simultaneously on a node by launching several mpirun commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources. #!/bin/bash ###### Tells PBS the job name #PBS -N packing_example ###### Tells PBS to run on 1 node with 128 cpus #PBS -l select=1:ncpus=128:mpiprocs=128 ###### Tells PBS the walltime (Max 72 hours) #PBS -l walltime=00:05:00 ###### Tells PBS the project to charge (Replace with a valid project) #PBS -A <PROJECT_NAME> cd $PBS_O_WORKDIR export range = $( eval echo { 0 ..31 } | xargs | sed -e 's/ /,/g\u2019) mpirun \u2013map-by pe-list:${range}:ordered \u2013np 32 a.out & export range=$(eval echo {32..95} | xargs | sed -e ' s/ /,/g\u2019 ) mpirun \u2013map-by pe-list: ${ range } :ordered \u2013np 64 b.out & export range = $( eval echo { 96 ..111 } | xargs | sed -e ' s/ /,/g\u2019 ) mpirun \u2013map-by pe-list: ${ range } :ordered \u2013np 16 c.out & wait Put each MPI tasks in the background with \"&\". Use mpirun placement options to specifically place them on certain cores, to avoid oversubscription on some cores. A good practice would be to run each command in its own directory. Add \u2013report-bindings to see what is happening. You must use wait command. Troubleshooting / Common Errors If you receive a qsub: Job rejected by all possible destinations error , then check your submission parameters. The issue is most likely that your walltime or node count do not fall within the ranges listed above for the production execution queues. Please see the table above for limits on production queue job sizes. NOTE: For batch submissions, if the parameters within your submission script do not meet the parameters of any of the above queues you might not receive the \u201cJob submission\u201d error on the command line at all. This can happen because your job is in waiting in a routing queue and has not yet reached the execution queues. In this case you will receive a jobid back and qsub will exit, however when the proposed job is routed, it will be rejected from the execution queues. In that case, the job will be deleted from the system and will not show up in the job history for that system. If you run a qstat on the jobid, it will return qstat: Unknown Job Id <jobid> . Documentation and Tools The PBS \u201cBigBook\u201d : This is really excellent. We highly suggest you download it and search through it when you have questions. However, it is big at about 2000 pages / 40MB and contains a bunch of stuff you don\u2019t really need, so you can also download the guides separately here: The PBS User Guide : This is the user guide. The PBS Reference Guide : This is the Reference Guide. It shows every option and gives you details on how to format various elements on the command line.","title":"Running Jobs on PBS Pro Clusters"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#running-jobs-on-pbs-clusters","text":"This document complements the information provided on the Improv Cluster page, forming a comprehensive resource for running jobs effectively. Both pages should be referenced for a complete understanding.","title":"Running Jobs on PBS Clusters"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#obtaining-and-managing-compute-resources","text":"","title":"Obtaining and Managing Compute Resources"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#definitions-and-notes","text":"chunk : A set of resources allocated as a unit to a job. Specified inside a selection directive. All parts of a chunk come from the same host. In a typical MPI (Message-Passing Interface) job, there is one chunk per MPI process. vnode : A virtual node, or vnode, is an abstract object representing a host or a set of resources which form a usable part of an execution host. This could be an entire host, or a nodeboard or a blade. A single host can be made up of multiple vnodes. Each vnode can be managed and scheduled independently. Each vnode in a complex must have a unique name. Vnodes on a host can share resources, such as node-locked licenses. PBS operates on vnodes. A vnode on LCRC Improv will represent an entire host . ncpus : Number of resources available to execute a program. Each Improv node has 128 cores. Users are encouraged to use all of the cores on the node and pack nodes with multiple jobs when possible. job : A job equates to a qsub. A set of resources allocated to you for a period of time. Your will execute one or more tasks on those resources during your job. task : A single execution on the resources of your job, often an mpirun invocation. You may run one task or many tasks during your job. You may run tasks sequentially or divide your resources up and run several tasks concurrently. Also sometimes referred to as job steps.","title":"Definitions and Notes"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#quick-start","text":"If you are an LCRC user and are familiar with Slurm, you will find the PBS Pro commands very similar though the options to qsub are quite different. We have added a handy conversion \u201ccheat sheet\u201d here. Here are the \u201cBig Four\u201d commands you will use with PBS Pro: qsub : request resources (generally compute nodes) to run your job and start your script/executable on the head node. Here is the minimal qsub allowed at the LCRC: qsub -A <project> -l select=<# of nodes>,walltime=HH:MM:SS <your job script> The -A and walltime options are mandatory. You will receive errors if they are not specified. We automatically add -l place=scatter for you so that each of your chunks ( <# of nodes> ) gets its own vnode. -q <queue_name> will place your job on the correct queue depending on the node type you want. The compute queue is the default on Improv. If you want to run an executable rather than a script replace <your jobs script> in the example above with -- <your executable> (that is dash dash) pbsq : a user-friendly filter for qstat to view the status of jobs and queues on the cluster. qstat : view the status of jobs and queues on the cluster. Try these variations and see which you like best: qstat, qstat -was, qstat -was1, qstat -wan, qstat -wan1 . Add -x to see jobs that have completed. We keep one week of history. qalter : update your request for resources Just like qsub , just add a jobid at the end. Only works before the job starts; If you want to change the walltime to 30 minutes: qalter -l walltime=30:00:00 <jobid> qdel : cancel a job that you don\u2019t need. This will also kill a running job. qdel <jobid> Note: The page numbers in the PBS guides are unique. If you search for the specified page number it will take you directly to the relevant page.","title":"Quick Start"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qsub-submit-a-job-to-run","text":"At the LCRC, your qsub will likely use the following parameters: qsub -A <project> -l select=<#>,walltime=HH:MM:SS <your job script> Where: <project> is the project name associated with your allocation. What you check the balance of with the sbank command. This is a mandatory option at the LCRC. If you don\u2019t include it you will get qsub: Account_Name is required to be set . walltime=HH:MM:SS specifying a wall time is mandatory at the LCRC. Jobs on Improv can run up to 72 hours maximum. <your job script> : For options that won\u2019t change, you do have the option of taking things off the command line and putting them in your job script. For instance the above command line could be simplified to qsub -l select=<#> <your job script> if you added the following to the top (the PBS directives have to be before any executable line) of your job script: #PBS -A <project> #PBS -l walltime=HH:MM:SS Also note that if you want to run an executable directly rather than a script you use two dashes and the executable name in place of your script name like this: -- /usr/bin/sleep 600","title":"qsub: Submit a job to run"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#resource-selection-and-job-placement","text":"Resources come in two flavors: Job Wide: Walltime is the most common example of a job wide resource. You use the -l option to specify job wide resources, i.e. -l walltime=06:00:00 . All the resources in the job have the same walltime. -l <resource name>=<value>[,<resource name>=<value> ...] Chunks: (see the definition above) This is how you describe what your needs are to run your job. You do this with the -l select= syntax. In the LCRC, we do whole node scheduling. This means you can typically get away with the very simple -l select=4 which will give you 4 nodes with all of the available cpus. <resource name>=<value>[:<resource name>=<value> ...] You can also tell PBS how you want the chunks distributed across the physical hardware. You do that via the -l place option (scatter is our default): -l place=[<arrangement>][: <sharing> ][: <grouping>] where arrangement is one of free | pack | scatter unless you have a specific reason to do otherwise, you probably want to set this to scatter, otherwise you may not get what you expect. For instance on a host with ncpus=128, if you requested -l select=8:ncpus=8:mpiprocs=8 you could end up with all of our chunks on one node. free means PBS can distribute them as it sees fit pack means all chunks from one host. Note that this is not the minimum number of hosts, it is one host. If the chunks can\u2019t fit on one host, the qsub will fail. scatter means take only one chunk from any given host. Here is a heavily commented sample PBS submission script that shows some more of the options, but remember that the PBS manuals referenced at the bottom of this page are the ultimate resource. If you create a file named hello.pbs for example, you can add: #!/bin/bash -l # Add another # at the beginning of the line to comment out a line # NOTE: adding a switch to the command line will override values in this file. # These options are MANDATORY in LCRC; Your qsub will fail if you don't provide them. #PBS -A <project_name> #PBS -l select=4:mpiprocs=128 #PBS -l walltime=HH:MM:SS # Highly recommended # The first 15 characters of the job name are displayed in the qstat output: #PBS -N <job_name> # If you want to merge stdout and stderr, use the -j option # oe=merge stdout/stderr to stdout, eo=merge stderr/stdout to stderr, n=don't merge #PBS -j n # Controlling email notifications # When to send email b=job begin, e=job end, a=job abort, j=subjobs (job arrays), n=no mail #PBS -m be #PBS -M <your_email_address> # The rest is an example of how an MPI job might be set up echo Working directory is $PBS_O_WORKDIR cd $PBS_O_WORKDIR echo Jobid: $PBS_JOBID echo Running on host ` hostname ` echo Running on nodes ` cat $PBS_NODEFILE ` module load gcc openmpi # Load a GCC and MPI module module list # Print all loaded modules which mpirun # Show which mpirun executable we are using mpirun ./hello_mpi # Run a script You would be able to submit the above script with: qsub hello.pbs","title":"Resource Selection and Job Placement"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qsub-example","text":"If you don\u2019t want to use a script, you could submit via the command line as well. For example: qsub -A <project_name> -l select=4:mpiprocs=128 -l walltime=30:00 -- a.out run a.out on 4 chunks with a walltime of 30 minutes ; charge project_name; run on 128 CPUs (ncpus) per node. On Improv by default, we allocate 128 cpus per node. Set ncpus if you want less. All jobs will be charged a whole node no matter how many cpus are requested. Allocate 128 MPI slots (mpiprocs) per node. Note: When using MPI, you must specify mpiprocs. We recommend it to be the same as ncpus. Since we allocate full nodes on Improv, 4 chunks will be 4 nodes. If we shared nodes, that would be 4 threads. use the -- (dash dash) syntax when directly running an executable.","title":"qsub example"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#pbsq-a-user-friendly-filter-for-qstat","text":"pbsq is a user-friendly filter for qstat to view the status of jobs and queues on the cluster. It provides a more user-friendly interface for qstat by providing a more intuitive way to view job and queue status.","title":"pbsq: A user-friendly filter for qstat"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#pbsq-basic-usage","text":"Query all jobs that are running and queued: pbsq Query all jobs that belong to a project: pbsq -f <project_name> Query all jobs that belong to a user: pbsq -f <user_name> Use pbsq -h to see the help menu and other options.","title":"pbsq basic usage"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qstat-query-the-status-of-jobsqueues","text":"The qstat command lists jobs on the system, showing their status, user, and other details. You can specify job IDs for detailed information about specific jobs.","title":"qstat: Query the status of jobs/queues"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qstat-basic-usage","text":"qstat : Lists all jobs with basic details. qstat [jobID] : Displays information for specific jobs. Options for expanded information: -w : Expands the width of the output to reduce truncation. -was1 : Shows nodes, tasks, requested walltime, and comments in a single line. -wan : Provides the list of nodes used. -T : Estimates the start time for the job (only for the next expected jobs). -f : Provides comprehensive details about a job. -x : Shows finished jobs (history retained for one week) along with comments. The comment field is crucial for understanding job status or issues. It's often the first place to check when troubleshooting job-related queries in PBS.","title":"qstat basic usage"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qalter-alter-a-queued-job","text":"Basically takes the same options as qsub ; Say you typoed and set the walltime to 300 minutes instead of 30 minutes. You could fix it (if the job had not started running) by doing qalter -l walltime=30:00 <jobid> [<jobid> <jobid>...] The new value overwrites any previous value.","title":"qalter: Alter a queued job"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qdel-delete-a-queued-or-running-job","text":"qdel <jobid> [<jobid> <jobid>...]","title":"qdel: Delete a queued or running job"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qholdqrls-place-release-a-user-hold-on-a-job","text":"[qhold | qrls] <jobid> [<jobid> <jobid>...]","title":"qhold,qrls: Place / release a user hold on a job"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qselect-query-jobids-for-use-in-commands","text":"qdel $(qselect -N test1) will delete all the jobs that had the job name set to test1.","title":"qselect: Query jobids for use in commands"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qmsg-write-a-message-into-a-jobs-output-file","text":"qmsg -E -O \"This is the message\" <jobid> [<jobid> <jobid>...] -E writes it to standard error, -O writes it to standard out","title":"qmsg: Write a message into a jobs output file"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#qsig-send-a-signal-to-a-job","text":"qsig -s <signal> <jobid> [<jobid> <jobid>...] If you don\u2019t specify a signal, SIGTERM is sent.","title":"qsig: Send a signal to a job"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#pbsnodes-get-information-about-the-current-state-of-nodes","text":"This is more for admins, but it can tell you more about the nodes themselves. pbsnodes <node name> : Everything there is to know about a node pbsnodes -avSj : A nice table to see what is free and in use pbsnodes -l : (lowercase l) see which nodes are down. The comment often indicates why it is down","title":"pbsnodes: Get information about the current state of nodes"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#job-priority","text":"In PBS it is not easy to see a priority order for which jobs will run next. The best way is to use the -T option on qsub and look at the estimated start times. LCRC runs a custom scheduler algorithm, but in general, the job priority in the queue is based on several criteria: positive balance of your project size (in nodes) of the job, larger jobs receive higher priority job duration: shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible","title":"Job Priority"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#general-pbs-example-scriptscommands","text":"","title":"General PBS Example Scripts/Commands"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#submitting-an-interactive-job","text":"Here is how to submit an interactive job to, for example, edit/build/test an application: qsub -I -A <PROJECT_NAME> -l select=1:ncpus=128:mpiprocs=128,walltime=01:00:00 -q compute This command requests 1 node for a period of 1 hour in the compute queue. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. Remember to replace <PROJECT_NAME> with a valid LCRC project.","title":"Submitting an Interactive Job"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#submitting-a-pbs-array-job","text":"#!/bin/bash ###### Tells PBS the job name #PBS -N array_example ###### Tells PBS to run on 1 node with 128 cpus #PBS -l select=1:ncpus=128:mpiprocs=128 ###### Tells PBS the walltime (Max 72 hours) #PBS -l walltime=00:05:00 ###### Tells PBS the project to charge (Replace with a valid project) #PBS -A <PROJECT_NAME> ###### Tells PBS to run 5 subjobs (Required to run in an array) #PBS -J 1-5 ###### Tells PBS to rerun the job (Rquired to run in an array) #PBS -r y cd $PBS_O_WORKDIR echo \"Running subjob $PBS_ARRAY_INDEX \" # Create a subjob-specific directory mkdir -p subjob_ ${ PBS_ARRAY_INDEX } cd subjob_ ${ PBS_ARRAY_INDEX } # Run a command unique to each subjob echo \"This is subjob ${ PBS_ARRAY_INDEX } \" > output_ ${ PBS_ARRAY_INDEX } .txt # Sleep for a different amount of time based on the subjob index sleep ${ PBS_ARRAY_INDEX } echo \"Subjob $PBS_ARRAY_INDEX completed\" The pbsq command subsequently shows the status of the job array. $ pbsq 138617[].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 B compute array_example Job Array Began at Thu Mar 21 at 12:02 138617[1].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i475 138617[2].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i730 138617[3].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i799 138617[4].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i285 138617[5].imgt1 user1 support 00:05:00 00:00:04 n/a 00:00:06 00:04:54 1 E compute array_example i286 If you want to increase the number of nodes per subjob, you can increase the select value in #PBS -l select=1:ncpus=128 to be equal to the number of nodes you want per subjob.","title":"Submitting a PBS Array Job"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#running-multiple-mpi-applications-on-a-node","text":"Multiple applications can be run simultaneously on a node by launching several mpirun commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources. #!/bin/bash ###### Tells PBS the job name #PBS -N packing_example ###### Tells PBS to run on 1 node with 128 cpus #PBS -l select=1:ncpus=128:mpiprocs=128 ###### Tells PBS the walltime (Max 72 hours) #PBS -l walltime=00:05:00 ###### Tells PBS the project to charge (Replace with a valid project) #PBS -A <PROJECT_NAME> cd $PBS_O_WORKDIR export range = $( eval echo { 0 ..31 } | xargs | sed -e 's/ /,/g\u2019) mpirun \u2013map-by pe-list:${range}:ordered \u2013np 32 a.out & export range=$(eval echo {32..95} | xargs | sed -e ' s/ /,/g\u2019 ) mpirun \u2013map-by pe-list: ${ range } :ordered \u2013np 64 b.out & export range = $( eval echo { 96 ..111 } | xargs | sed -e ' s/ /,/g\u2019 ) mpirun \u2013map-by pe-list: ${ range } :ordered \u2013np 16 c.out & wait Put each MPI tasks in the background with \"&\". Use mpirun placement options to specifically place them on certain cores, to avoid oversubscription on some cores. A good practice would be to run each command in its own directory. Add \u2013report-bindings to see what is happening. You must use wait command.","title":"Running Multiple MPI Applications on a Node"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#troubleshooting-common-errors","text":"If you receive a qsub: Job rejected by all possible destinations error , then check your submission parameters. The issue is most likely that your walltime or node count do not fall within the ranges listed above for the production execution queues. Please see the table above for limits on production queue job sizes. NOTE: For batch submissions, if the parameters within your submission script do not meet the parameters of any of the above queues you might not receive the \u201cJob submission\u201d error on the command line at all. This can happen because your job is in waiting in a routing queue and has not yet reached the execution queues. In this case you will receive a jobid back and qsub will exit, however when the proposed job is routed, it will be rejected from the execution queues. In that case, the job will be deleted from the system and will not show up in the job history for that system. If you run a qstat on the jobid, it will return qstat: Unknown Job Id <jobid> .","title":"Troubleshooting / Common Errors"},{"location":"running-jobs-at-lcrc/pbs-pro-clusters/#documentation-and-tools","text":"The PBS \u201cBigBook\u201d : This is really excellent. We highly suggest you download it and search through it when you have questions. However, it is big at about 2000 pages / 40MB and contains a bunch of stuff you don\u2019t really need, so you can also download the guides separately here: The PBS User Guide : This is the user guide. The PBS Reference Guide : This is the Reference Guide. It shows every option and gives you details on how to format various elements on the command line.","title":"Documentation and Tools"},{"location":"running-jobs-at-lcrc/slurm-clusters/","text":"Running Jobs on Slurm Clusters This document complements the information provided on the Bebop and Swing Cluster pages, forming a comprehensive resource for running jobs effectively. Both pages should be referenced for a complete understanding. Job Submission Commands The 3 most common tools you will use to submit jobs are sbatch , srun and salloc . You can reference the table below for a simple, quick cheat sheet on a few examples about jobs in Slurm: Slurm Command Description sbatch <job_script> Submit <job_script> to the Scheduler srun <options> Run Parallel Jobs salloc <options> Request an Interactive Job squeue View Job Information scancel <job_id> Delete a Job Example sbatch Job Submission (Simple) Here you'll find a couple of very simple submission scripts to get you started that you can use with sbatch to submit your job. For this example, the script can be named myjob.sh : #!/bin/bash #SBATCH --job-name=<my_job_name> #SBATCH --account=<my_lcrc_project_name> #SBATCH --partition=bdwall #SBATCH --nodes=1 #SBATCH --ntasks-per-node=36 #SBATCH --output=<my_job_name>.out #SBATCH --error=<my_job_name>.error #SBATCH --mail-user=<your email address> # Optional if you require email #SBATCH --mail-type=ALL # Optional if you require email #SBATCH --time=01:00:00 # Run My Program srun /bin/hostname Example sbatch Job Submission (MPI) #!/bin/bash # SBATCH --job-name=<my_job_name> # SBATCH --account=<my_lcrc_project_name> # SBATCH --partition=bdwall # SBATCH --nodes=2 # SBATCH --ntasks-per-node=36 # SBATCH --output=<my_job_name>.out # SBATCH --error=<my_job_name>.error # SBATCH --mail-user=<your_email_address> # Optional if you require email # SBATCH --mail-type=ALL # Optional if you require email # SBATCH --time=01:00:00 # Setup My Environment module load intel-parallel-studio/cluster.2018.4-xtm134f export I_MPI_FABRICS = shm:tmi # Run My Program srun -n 72 ./helloworld NOTE: I_MPI_FABRICS=shm:tmi \u2013 Use shared memory (shm) for communication within a single host, and the tag matching interface (tmi) (Omni-Path optimized) for host to host communication. Example Interactive Job Submission There are a couple of ways to run an interactive job on Bebop. First, you can just get a session on a node by using the srun command in the following way: srun --pty -p <partition> -t <walltime> /bin/bash This will drop you onto one node. Once you exit the node, the allocation will be relinquished. If you want more flexibility, you can instead have the system first allocate resources for the job using the the salloc command: salloc -N 2 -p bdwall -t 00:30:00 This job will allocate 2 nodes from bdwall partition for 30 minutes. You should get the job number from the output. This command will not log you into any of your allocated nodes by default. You can get a list of your allocated nodes and many other slurm settings set by the salloc command by doing: printenv | grep SLURM After the resources were allocated and the session was granted use srun command to run your job: srun -n 8 ./myprog This will start 8 threads on the allocated nodes. If you try and use more resources than you allocated (say 3 nodes worth of resources while you only asked for 2), this will create a separate reservation and the other will continue to run and use hours as well. When you allocate resources via salloc , you can also now freely SSH to the nodes in your allocation as well if you prefer to run jobs from the nodes themselves. Checking Queues and Jobs To view job and job step information use squeue . Here's a quick example of what the output may look like: squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 999 bdwall test-joba user2 R 2 :40:31 2 bdw- [ 0010 -0011 ] 998 bdwall test-job2 user1 R 45 :20 1 bdwd-0120 997 knld test-job1 user1 R 3 :04 1 knld-0030 Here are also some common options for squeue: Option Description -a Display information about all jobs in all partitions. This is the default when running squeue with no options. -u Request jobs or job steps from a comma separated list of users. The list can consist of user names or user id numbers. j Requests a comma separated list of job IDs to display. Defaults to all jobs. -l Report more of the available information for the selected jobs or job steps. Deleting a Job To delete a job use scancel This command will take the job id as its argument. Your job id will be given to when you submit the job. You can also retrieve this from the squeue command detailed above. scancel <job_id> Other Useful Slurm Commands scontrol scontrol can be used to report more detailed information about nodes, partitions, jobs, job steps, and configuration. Common examples: Option Description scontrol show node node-name Shows detailed information about the nodes. scontrol show partition partition-name Shows detailed information about a specific partition. scontrol show job job-id Shows detailed information about a specific job or all jobs if no job id is given. scontrol update job job-id Change attributes of submitted job. For an extensive list of formatting options please consult scontrol man page. sinfo sinfo allows you to view information about jobs, nodes and partitions located in the Slurm scheduling queue Option Description -a, --all Display information about all partitions. -t, --states Display nodes in a specific state. Example: idle -i , --iterate= Print the state on a periodic basis. Sleep for the indicated number of seconds between reports. -l, --long Print more detailed information. -n , --nodes= Print information only about the specified node(s). Multiple nodes may be comma separated or expressed using a node range expression. For example \u201cbdw-[0001-0007]\u201d -o <output_format> , --format=<output_format> Specify the information to be displayed using an sinfo format string. For an extensive list of formatting options please consult sinfo man page. sacct sacct displays accounting data for all jobs and job steps and can be used to display the information about the complete jobs. Option Description -S , --starttime Select jobs in any state after the specified time. -E end_time , --endtime=end_time Select jobs in any state before the specified time. Valid time formats are: HH:MM[:SS] [AM|PM] MMDD[YY] or MM/DD[/YY] or MM.DD[.YY] MM/DD[/YY]-HH:MM[:SS] YYYY-MM-DD[THH:MM[:SS]] Example: # sacct -S2014-07-03-11:40 -E2014-07-03-12:00 -X -ojobid,start,end,state JobID Start End State --------- --------------------- -------------------- ------------ 2 2014 -07-03T11:33:16 2014 -07-03T11:59:01 COMPLETED 3 2014 -07-03T11:35:21 Unknown RUNNING 4 2014 -07-03T11:35:21 2014 -07-03T11:45:21 COMPLETED 5 2014 -07-03T11:41:01 Unknown RUNNING For an extensive list of formatting options please consult sacct man page. sprio sprio is used to view the components of a job\u2019s scheduling priority when the multi-factor priority plugin is installed. sprio is a read-only utility that extracts information from the multi-factor priority plugin. By default, sprio returns information for all pending jobs. Options exist to display specific jobs by job ID and user name. For an extensive list of formatting options please consult the sprio man page. Why Isn\u2019t My Job Running Yet? If today is NOT LCRC Maintenance Day and you find that your job is in the pending (PD) state after running squeue, Slurm will provide a reason for this shown in the squeue command. Here are a few of the most common reasons your job may not be running. First, check to the see reason code by querying your job number in Slurm: squeue -j <job_id> Then, you can determine why the job has not started by deciphering this sample reason list: Reason Code Description AccountNotAllowed The job isn\u2019t using an account that is allowed on the partition. Certain projects may be restricted to certain partitions. For example, a project may only be allowed to run on the knl partitions. Bebop condo node users must use the \u2018condo\u2018 account when running on their dedicated partitions. AssocGrpBillingMinutes The job doesn\u2019t have enough time in the banking account to begin. BadConstraints The job\u2019s constraints can not be satisfied. BeginTime The job\u2019s earliest start time has not yet been reached. Cleaning The job is being requeued and still cleaning up from its previous execution. Dependency This job is waiting for a dependent job to complete. JobHeldAdmin The job is held by a system administrator. JobHeldUser The job is held by the user. NodeDown A node required by the job is down. PartitionNodeLimit The number of nodes required by this job is outside of it\u2019s partitions current limits. Can also indicate that required nodes are DOWN or DRAINED. PartitionTimeLimit The job\u2019s time limit exceeds it\u2019s partition\u2019s current time limit. Priority One or more higher priority jobs exist for this partition or advanced reservation. QOSMaxJobsPerUserLimit The job\u2019s QOS has reached its maximum job count for the user at one time. ReqNodeNotAvail During LCRC Maintenance Day, you may see this reason. In addition, jobs that have a walltime that runs into a scheduled maintenance period will also show this message. The job TimeLimit should be adjusted accordingly. Otherwise, some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job\u2019s \u201creason\u201d field as \u201cUnavailableNodes\u201d. Such nodes will typically require the intervention of a system administrator to make available. Reservation The job is waiting its advanced reservation to become available. Resources The job is waiting for resources to become available. TimeLimit The job exhausted its time limit. While this is not every reason code, these are the most common on Bebop. You can view the full list of Slurm reason codes here . Assuming your job is in the Priority/Resources state, you can use the sprio command to get a closer idea on when your job may start based on the priorities of other pending jobs. The priority is the sum of age, fairshare, jobsize and QOS (quality of service). Command Line Quick Reference Guide Command Description sbatch <script_name> Submit a job. scancel <job_id> Delete a job. squeue squeue -u <username> Show queued jobs via the scheduler. Show queued jobs from a specific user. scontrol show job <job_id> Provide a detailed status report for a specified job via the scheduler. sinfo -t idle Get a list of all free/idle nodes.","title":"Running Jobs on Slurm Clusters"},{"location":"running-jobs-at-lcrc/slurm-clusters/#running-jobs-on-slurm-clusters","text":"This document complements the information provided on the Bebop and Swing Cluster pages, forming a comprehensive resource for running jobs effectively. Both pages should be referenced for a complete understanding.","title":"Running Jobs on Slurm Clusters"},{"location":"running-jobs-at-lcrc/slurm-clusters/#job-submission-commands","text":"The 3 most common tools you will use to submit jobs are sbatch , srun and salloc . You can reference the table below for a simple, quick cheat sheet on a few examples about jobs in Slurm: Slurm Command Description sbatch <job_script> Submit <job_script> to the Scheduler srun <options> Run Parallel Jobs salloc <options> Request an Interactive Job squeue View Job Information scancel <job_id> Delete a Job","title":"Job Submission Commands"},{"location":"running-jobs-at-lcrc/slurm-clusters/#example-sbatch-job-submission-simple","text":"Here you'll find a couple of very simple submission scripts to get you started that you can use with sbatch to submit your job. For this example, the script can be named myjob.sh : #!/bin/bash #SBATCH --job-name=<my_job_name> #SBATCH --account=<my_lcrc_project_name> #SBATCH --partition=bdwall #SBATCH --nodes=1 #SBATCH --ntasks-per-node=36 #SBATCH --output=<my_job_name>.out #SBATCH --error=<my_job_name>.error #SBATCH --mail-user=<your email address> # Optional if you require email #SBATCH --mail-type=ALL # Optional if you require email #SBATCH --time=01:00:00 # Run My Program srun /bin/hostname","title":"Example sbatch Job Submission (Simple)"},{"location":"running-jobs-at-lcrc/slurm-clusters/#example-sbatch-job-submission-mpi","text":"#!/bin/bash # SBATCH --job-name=<my_job_name> # SBATCH --account=<my_lcrc_project_name> # SBATCH --partition=bdwall # SBATCH --nodes=2 # SBATCH --ntasks-per-node=36 # SBATCH --output=<my_job_name>.out # SBATCH --error=<my_job_name>.error # SBATCH --mail-user=<your_email_address> # Optional if you require email # SBATCH --mail-type=ALL # Optional if you require email # SBATCH --time=01:00:00 # Setup My Environment module load intel-parallel-studio/cluster.2018.4-xtm134f export I_MPI_FABRICS = shm:tmi # Run My Program srun -n 72 ./helloworld NOTE: I_MPI_FABRICS=shm:tmi \u2013 Use shared memory (shm) for communication within a single host, and the tag matching interface (tmi) (Omni-Path optimized) for host to host communication.","title":"Example sbatch Job Submission (MPI)"},{"location":"running-jobs-at-lcrc/slurm-clusters/#example-interactive-job-submission","text":"There are a couple of ways to run an interactive job on Bebop. First, you can just get a session on a node by using the srun command in the following way: srun --pty -p <partition> -t <walltime> /bin/bash This will drop you onto one node. Once you exit the node, the allocation will be relinquished. If you want more flexibility, you can instead have the system first allocate resources for the job using the the salloc command: salloc -N 2 -p bdwall -t 00:30:00 This job will allocate 2 nodes from bdwall partition for 30 minutes. You should get the job number from the output. This command will not log you into any of your allocated nodes by default. You can get a list of your allocated nodes and many other slurm settings set by the salloc command by doing: printenv | grep SLURM After the resources were allocated and the session was granted use srun command to run your job: srun -n 8 ./myprog This will start 8 threads on the allocated nodes. If you try and use more resources than you allocated (say 3 nodes worth of resources while you only asked for 2), this will create a separate reservation and the other will continue to run and use hours as well. When you allocate resources via salloc , you can also now freely SSH to the nodes in your allocation as well if you prefer to run jobs from the nodes themselves.","title":"Example Interactive Job Submission"},{"location":"running-jobs-at-lcrc/slurm-clusters/#checking-queues-and-jobs","text":"To view job and job step information use squeue . Here's a quick example of what the output may look like: squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 999 bdwall test-joba user2 R 2 :40:31 2 bdw- [ 0010 -0011 ] 998 bdwall test-job2 user1 R 45 :20 1 bdwd-0120 997 knld test-job1 user1 R 3 :04 1 knld-0030 Here are also some common options for squeue: Option Description -a Display information about all jobs in all partitions. This is the default when running squeue with no options. -u Request jobs or job steps from a comma separated list of users. The list can consist of user names or user id numbers. j Requests a comma separated list of job IDs to display. Defaults to all jobs. -l Report more of the available information for the selected jobs or job steps.","title":"Checking Queues and Jobs"},{"location":"running-jobs-at-lcrc/slurm-clusters/#deleting-a-job","text":"To delete a job use scancel This command will take the job id as its argument. Your job id will be given to when you submit the job. You can also retrieve this from the squeue command detailed above. scancel <job_id>","title":"Deleting a Job"},{"location":"running-jobs-at-lcrc/slurm-clusters/#other-useful-slurm-commands","text":"","title":"Other Useful Slurm Commands"},{"location":"running-jobs-at-lcrc/slurm-clusters/#scontrol","text":"scontrol can be used to report more detailed information about nodes, partitions, jobs, job steps, and configuration. Common examples: Option Description scontrol show node node-name Shows detailed information about the nodes. scontrol show partition partition-name Shows detailed information about a specific partition. scontrol show job job-id Shows detailed information about a specific job or all jobs if no job id is given. scontrol update job job-id Change attributes of submitted job. For an extensive list of formatting options please consult scontrol man page.","title":"scontrol"},{"location":"running-jobs-at-lcrc/slurm-clusters/#sinfo","text":"sinfo allows you to view information about jobs, nodes and partitions located in the Slurm scheduling queue Option Description -a, --all Display information about all partitions. -t, --states Display nodes in a specific state. Example: idle -i , --iterate= Print the state on a periodic basis. Sleep for the indicated number of seconds between reports. -l, --long Print more detailed information. -n , --nodes= Print information only about the specified node(s). Multiple nodes may be comma separated or expressed using a node range expression. For example \u201cbdw-[0001-0007]\u201d -o <output_format> , --format=<output_format> Specify the information to be displayed using an sinfo format string. For an extensive list of formatting options please consult sinfo man page.","title":"sinfo"},{"location":"running-jobs-at-lcrc/slurm-clusters/#sacct","text":"sacct displays accounting data for all jobs and job steps and can be used to display the information about the complete jobs. Option Description -S , --starttime Select jobs in any state after the specified time. -E end_time , --endtime=end_time Select jobs in any state before the specified time. Valid time formats are: HH:MM[:SS] [AM|PM] MMDD[YY] or MM/DD[/YY] or MM.DD[.YY] MM/DD[/YY]-HH:MM[:SS] YYYY-MM-DD[THH:MM[:SS]] Example: # sacct -S2014-07-03-11:40 -E2014-07-03-12:00 -X -ojobid,start,end,state JobID Start End State --------- --------------------- -------------------- ------------ 2 2014 -07-03T11:33:16 2014 -07-03T11:59:01 COMPLETED 3 2014 -07-03T11:35:21 Unknown RUNNING 4 2014 -07-03T11:35:21 2014 -07-03T11:45:21 COMPLETED 5 2014 -07-03T11:41:01 Unknown RUNNING For an extensive list of formatting options please consult sacct man page.","title":"sacct"},{"location":"running-jobs-at-lcrc/slurm-clusters/#sprio","text":"sprio is used to view the components of a job\u2019s scheduling priority when the multi-factor priority plugin is installed. sprio is a read-only utility that extracts information from the multi-factor priority plugin. By default, sprio returns information for all pending jobs. Options exist to display specific jobs by job ID and user name. For an extensive list of formatting options please consult the sprio man page.","title":"sprio"},{"location":"running-jobs-at-lcrc/slurm-clusters/#why-isnt-my-job-running-yet","text":"If today is NOT LCRC Maintenance Day and you find that your job is in the pending (PD) state after running squeue, Slurm will provide a reason for this shown in the squeue command. Here are a few of the most common reasons your job may not be running. First, check to the see reason code by querying your job number in Slurm: squeue -j <job_id> Then, you can determine why the job has not started by deciphering this sample reason list: Reason Code Description AccountNotAllowed The job isn\u2019t using an account that is allowed on the partition. Certain projects may be restricted to certain partitions. For example, a project may only be allowed to run on the knl partitions. Bebop condo node users must use the \u2018condo\u2018 account when running on their dedicated partitions. AssocGrpBillingMinutes The job doesn\u2019t have enough time in the banking account to begin. BadConstraints The job\u2019s constraints can not be satisfied. BeginTime The job\u2019s earliest start time has not yet been reached. Cleaning The job is being requeued and still cleaning up from its previous execution. Dependency This job is waiting for a dependent job to complete. JobHeldAdmin The job is held by a system administrator. JobHeldUser The job is held by the user. NodeDown A node required by the job is down. PartitionNodeLimit The number of nodes required by this job is outside of it\u2019s partitions current limits. Can also indicate that required nodes are DOWN or DRAINED. PartitionTimeLimit The job\u2019s time limit exceeds it\u2019s partition\u2019s current time limit. Priority One or more higher priority jobs exist for this partition or advanced reservation. QOSMaxJobsPerUserLimit The job\u2019s QOS has reached its maximum job count for the user at one time. ReqNodeNotAvail During LCRC Maintenance Day, you may see this reason. In addition, jobs that have a walltime that runs into a scheduled maintenance period will also show this message. The job TimeLimit should be adjusted accordingly. Otherwise, some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job\u2019s \u201creason\u201d field as \u201cUnavailableNodes\u201d. Such nodes will typically require the intervention of a system administrator to make available. Reservation The job is waiting its advanced reservation to become available. Resources The job is waiting for resources to become available. TimeLimit The job exhausted its time limit. While this is not every reason code, these are the most common on Bebop. You can view the full list of Slurm reason codes here . Assuming your job is in the Priority/Resources state, you can use the sprio command to get a closer idea on when your job may start based on the priorities of other pending jobs. The priority is the sum of age, fairshare, jobsize and QOS (quality of service).","title":"Why Isn\u2019t My Job Running Yet?"},{"location":"running-jobs-at-lcrc/slurm-clusters/#command-line-quick-reference-guide","text":"Command Description sbatch <script_name> Submit a job. scancel <job_id> Delete a job. squeue squeue -u <username> Show queued jobs via the scheduler. Show queued jobs from a specific user. scontrol show job <job_id> Provide a detailed status report for a specified job via the scheduler. sinfo -t idle Get a list of all free/idle nodes.","title":"Command Line Quick Reference Guide"},{"location":"swing/getting-started-swing/","text":"Getting Started on Swing Accessing Swing To access Swing, use the following command: ssh <your_argonne_username>@swing.lcrc.anl.gov System Architecture For a detailed overview of the Swing cluster, including the compute node architecture, refer to the Hardware Overview page. Job Execution For information on how to run jobs on Swing, refer to the Running Jobs page.","title":"Getting Started"},{"location":"swing/getting-started-swing/#getting-started-on-swing","text":"","title":"Getting Started on Swing"},{"location":"swing/getting-started-swing/#accessing-swing","text":"To access Swing, use the following command: ssh <your_argonne_username>@swing.lcrc.anl.gov","title":"Accessing Swing"},{"location":"swing/getting-started-swing/#system-architecture","text":"For a detailed overview of the Swing cluster, including the compute node architecture, refer to the Hardware Overview page.","title":"System Architecture"},{"location":"swing/getting-started-swing/#job-execution","text":"For information on how to run jobs on Swing, refer to the Running Jobs page.","title":"Job Execution"},{"location":"swing/hardware-overview-swing/","text":"Swing Hardware Overview 8x NVIDIA A100 GPUS per node 1-2TB DDR4 and 320-640GB GPU memory per node 128 cpu cores per compute node Infiniband HDR Interconnect Swing Compute Nodes Swing Compute Description Per Node Aggregate Processor (Note 1) AMD EPYC 7742 2.25GHz 2 Sockets 12 Cores/Threads 64 Cores/2 Threads per core 128/256 768/1,536 Memory DDR4 1 TB (gpu1-4,6) 2 TB(gpu5) 7,000 GiB Local SSD 14 TB (gpu1-4,6) 28 TB (gpu5) 1 98 TB GPUs NVIDIA A100 80GB (Node gpu5) NVIDIA A100 40GB (Nodes gpu1-4,6) 8 48 Swing A100 GPU Information Description A100-SXM4-80GB A100-SXM4-40GB GPU Memory 80 GiB HBM2 GPU Memory BW 2.4 TB/s Interconnect FP64 9.7 TF FP64 Tensor Core 19.5 TF FP32 19.5 TF BF16 Tensor Core 312 TF FP16 Tensor Core 312 TF INT8 Tensor Core 624 TOPS Max TDP Power 400 W Swing Login Nodes There are two login nodes available to users for editing code, building code, submitting/monitoring jobs, checking usage ( lcrc-sbank ), etc. Their full hostnames are gpulogin1.lcrc.anl.gov and gpulogin2.lcrc.anl.gov . The login nodes hardware is quite different to the compute nodes. The various compilers and libraries are present on the logins, so most users should be able to build their code. However, software requiring GPUs should be built on the compute nodes as the login nodes do not have GPUs installed in them. All users share the same login nodes so please be courteous and respectful of your fellow users. For example, please do not run computationally or IO intensive pre or post-processing on the logins and keep the parallelism of your builds to a reasonable level.","title":"Hardware Overview"},{"location":"swing/hardware-overview-swing/#swing-hardware-overview","text":"8x NVIDIA A100 GPUS per node 1-2TB DDR4 and 320-640GB GPU memory per node 128 cpu cores per compute node Infiniband HDR Interconnect","title":"Swing Hardware Overview"},{"location":"swing/hardware-overview-swing/#swing-compute-nodes","text":"Swing Compute Description Per Node Aggregate Processor (Note 1) AMD EPYC 7742 2.25GHz 2 Sockets 12 Cores/Threads 64 Cores/2 Threads per core 128/256 768/1,536 Memory DDR4 1 TB (gpu1-4,6) 2 TB(gpu5) 7,000 GiB Local SSD 14 TB (gpu1-4,6) 28 TB (gpu5) 1 98 TB GPUs NVIDIA A100 80GB (Node gpu5) NVIDIA A100 40GB (Nodes gpu1-4,6) 8 48","title":"Swing Compute Nodes"},{"location":"swing/hardware-overview-swing/#swing-a100-gpu-information","text":"Description A100-SXM4-80GB A100-SXM4-40GB GPU Memory 80 GiB HBM2 GPU Memory BW 2.4 TB/s Interconnect FP64 9.7 TF FP64 Tensor Core 19.5 TF FP32 19.5 TF BF16 Tensor Core 312 TF FP16 Tensor Core 312 TF INT8 Tensor Core 624 TOPS Max TDP Power 400 W","title":"Swing A100 GPU Information"},{"location":"swing/hardware-overview-swing/#swing-login-nodes","text":"There are two login nodes available to users for editing code, building code, submitting/monitoring jobs, checking usage ( lcrc-sbank ), etc. Their full hostnames are gpulogin1.lcrc.anl.gov and gpulogin2.lcrc.anl.gov . The login nodes hardware is quite different to the compute nodes. The various compilers and libraries are present on the logins, so most users should be able to build their code. However, software requiring GPUs should be built on the compute nodes as the login nodes do not have GPUs installed in them. All users share the same login nodes so please be courteous and respectful of your fellow users. For example, please do not run computationally or IO intensive pre or post-processing on the logins and keep the parallelism of your builds to a reasonable level.","title":"Swing Login Nodes"},{"location":"swing/running-jobs-swing/","text":"Running Jobs on Swing Overview Swing's job scheduling system is characterized by: Uses Slurm Uses the legacy lcrc-sbank accounting system Allocations are calculated in GPU hours Partition Limits Swing currently enforces the following limits on publicly available partitions: 4 Running Jobs per user. 10 Queued Jobs per user. 1 Days (24 Hours) Maximum Walltime. 1 Hour Default Walltime if not specified. 16 GPUs (2 full nodes) Max in use at one time. gpu is the default partition. Slurm Partitions Swing has two partitions, the default being named gpu. By default, you will be allocated 1/8th of the node resources per GPU. Nodes allow for multiple jobs from multiple users up until the resources are fully consumed (8 jobs with 1 GPU each per node, 1 job with 8 GPU per node, and everything in between). You MUST request at least 1 GPU to run a job otherwise you will see the following error: srun: error: Please request at least 1 GPU in the partition 'gpu' srun: error: e.g '#SBATCH --gres=gpu:1') srun: error: Unable to allocate resources: Invalid generic resource (gres) specification Partition Name Number of Nodes GPUs Per Node GPU Memory Per Node CPUs Per Node DDR4 Memory Per Node Local Scratch Disk Operating System gpu 5 8x NVIDIA A100 40GB 320GB 2x AMD EPYC 7742 64-Core Processor (128 Total Cores) 1TB 14TB Ubuntu 20.04.2 LTS gpu-large 1 8x NVIDIA A100 80GB 640GB 2x AMD EPYC 7742 64-Core Processor (128 Total Cores) 2TB 28TB Ubuntu 20.04.2 LTS Submission Examples Example sbatch Job Submission Script Here is an example Slurm submission script called gpu-app-script.sh that requests available GPUs for the job. #!/bin/bash #SBATCH --job-name=gpu-test #SBATCH --account=<my_lcrc_project_name> #SBATCH --nodes=2 #SBATCH --gres=gpu:8 #SBATCH --time=00:05:00 srun ./your-gpu-application You can then submit the script with sbatch gpu-app-script.sh . Example Interactive Job Submission To run an interactive job in a computing environment using Slurm, you have two main options: Direct Node Session with srun: Use srun --gres=gpu:1 --pty bash to get a session on a node with 1 GPU. This session ends once you exit the node. Resource Allocation with salloc: Use salloc -N 2 --gres=gpu:4 -t 00:30:00 to allocate resources for a job. This allocates 2 nodes with 4 GPUs each for 30 minutes. The job number will be provided in the output. To see the list of allocated nodes and Slurm settings, use printenv | grep SLURM . After allocation, use srun to run your job, or SSH directly to the nodes in your allocation to execute jobs.","title":"Running Jobs on Swing"},{"location":"swing/running-jobs-swing/#running-jobs-on-swing","text":"","title":"Running Jobs on Swing"},{"location":"swing/running-jobs-swing/#overview","text":"Swing's job scheduling system is characterized by: Uses Slurm Uses the legacy lcrc-sbank accounting system Allocations are calculated in GPU hours","title":"Overview"},{"location":"swing/running-jobs-swing/#partition-limits","text":"Swing currently enforces the following limits on publicly available partitions: 4 Running Jobs per user. 10 Queued Jobs per user. 1 Days (24 Hours) Maximum Walltime. 1 Hour Default Walltime if not specified. 16 GPUs (2 full nodes) Max in use at one time. gpu is the default partition.","title":"Partition Limits"},{"location":"swing/running-jobs-swing/#slurm-partitions","text":"Swing has two partitions, the default being named gpu. By default, you will be allocated 1/8th of the node resources per GPU. Nodes allow for multiple jobs from multiple users up until the resources are fully consumed (8 jobs with 1 GPU each per node, 1 job with 8 GPU per node, and everything in between). You MUST request at least 1 GPU to run a job otherwise you will see the following error: srun: error: Please request at least 1 GPU in the partition 'gpu' srun: error: e.g '#SBATCH --gres=gpu:1') srun: error: Unable to allocate resources: Invalid generic resource (gres) specification Partition Name Number of Nodes GPUs Per Node GPU Memory Per Node CPUs Per Node DDR4 Memory Per Node Local Scratch Disk Operating System gpu 5 8x NVIDIA A100 40GB 320GB 2x AMD EPYC 7742 64-Core Processor (128 Total Cores) 1TB 14TB Ubuntu 20.04.2 LTS gpu-large 1 8x NVIDIA A100 80GB 640GB 2x AMD EPYC 7742 64-Core Processor (128 Total Cores) 2TB 28TB Ubuntu 20.04.2 LTS","title":"Slurm Partitions"},{"location":"swing/running-jobs-swing/#submission-examples","text":"","title":"Submission Examples"},{"location":"swing/running-jobs-swing/#example-sbatch-job-submission-script","text":"Here is an example Slurm submission script called gpu-app-script.sh that requests available GPUs for the job. #!/bin/bash #SBATCH --job-name=gpu-test #SBATCH --account=<my_lcrc_project_name> #SBATCH --nodes=2 #SBATCH --gres=gpu:8 #SBATCH --time=00:05:00 srun ./your-gpu-application You can then submit the script with sbatch gpu-app-script.sh .","title":"Example sbatch Job Submission Script"},{"location":"swing/running-jobs-swing/#example-interactive-job-submission","text":"To run an interactive job in a computing environment using Slurm, you have two main options: Direct Node Session with srun: Use srun --gres=gpu:1 --pty bash to get a session on a node with 1 GPU. This session ends once you exit the node. Resource Allocation with salloc: Use salloc -N 2 --gres=gpu:4 -t 00:30:00 to allocate resources for a job. This allocates 2 nodes with 4 GPUs each for 30 minutes. The job number will be provided in the output. To see the list of allocated nodes and Slurm settings, use printenv | grep SLURM . After allocation, use srun to run your job, or SSH directly to the nodes in your allocation to execute jobs.","title":"Example Interactive Job Submission"},{"location":"using-software/getting-started/","text":"Software Lmod LCRC uses Lmod (Lua Environment Modules) for software and environment variable management. Lmod has several advantages over other software environments. For example, it prevents you from loading multiple versions of the same package at the same time. It also prevents you from having multiple compilers and MPI libraries loaded at the same time. See the Lmod User Guide for information on how to use Lmod. Some basic commands Finding Available Modules Task Lmod List available modules for the current compiler/MPI library module avail List all available modules module spider List modules with a certain keyword in their description module spider <keyword> List currently loaded modules module list Loading/Unloading Modules Task Lmod Load a module module load <module_name> Unload a module module unload <module_name> Reload all modules module update Unload all modules module purge Setting Default Modules Task Lmod File containing default modules ~/.lmod.d/default Save currently loaded modules module save Save currently loaded modules to a new collection module save <filename> Restore previously saved modules module restore Restore previously saved modules from another collection module restore <filename> Return to default modules module reset Finding More Information on a Module Task Lmod Print info for a module module whatis Print description of a module module help See contents of a module module show <module_name> Available Software With over 500 active users in fields as diverse as climate modeling, engine simulation, and ab-initio molecular dynamics, LCRC provides a diverse software stack. If you don\u2019t see a software package installed in our environment that you would like to use, please let us know by contacting support@lcrc.anl.gov . We will consider adding software system wide if appropriate for general LCRC use. We generally use a package manager called Spack for most of our software installs, but often need to install packages manually. Keep in mind that the software you want may have dozens of dependencies that also need to be installed, so it may take some time for us to complete the installation for you. Of course, you\u2019re always welcome to install your own software in your home or project directory (if applicable) as well. LCRC does not purchase licenses for users that need paid commercial software. If you need a paid application, please consult with LCRC staff before mkaing a purchase.","title":"Getting Started"},{"location":"using-software/getting-started/#software","text":"","title":"Software"},{"location":"using-software/getting-started/#lmod","text":"LCRC uses Lmod (Lua Environment Modules) for software and environment variable management. Lmod has several advantages over other software environments. For example, it prevents you from loading multiple versions of the same package at the same time. It also prevents you from having multiple compilers and MPI libraries loaded at the same time. See the Lmod User Guide for information on how to use Lmod.","title":"Lmod"},{"location":"using-software/getting-started/#some-basic-commands","text":"","title":"Some basic commands"},{"location":"using-software/getting-started/#finding-available-modules","text":"Task Lmod List available modules for the current compiler/MPI library module avail List all available modules module spider List modules with a certain keyword in their description module spider <keyword> List currently loaded modules module list","title":"Finding Available Modules"},{"location":"using-software/getting-started/#loadingunloading-modules","text":"Task Lmod Load a module module load <module_name> Unload a module module unload <module_name> Reload all modules module update Unload all modules module purge","title":"Loading/Unloading Modules"},{"location":"using-software/getting-started/#setting-default-modules","text":"Task Lmod File containing default modules ~/.lmod.d/default Save currently loaded modules module save Save currently loaded modules to a new collection module save <filename> Restore previously saved modules module restore Restore previously saved modules from another collection module restore <filename> Return to default modules module reset","title":"Setting Default Modules"},{"location":"using-software/getting-started/#finding-more-information-on-a-module","text":"Task Lmod Print info for a module module whatis Print description of a module module help See contents of a module module show <module_name>","title":"Finding More Information on a Module"},{"location":"using-software/getting-started/#available-software","text":"With over 500 active users in fields as diverse as climate modeling, engine simulation, and ab-initio molecular dynamics, LCRC provides a diverse software stack. If you don\u2019t see a software package installed in our environment that you would like to use, please let us know by contacting support@lcrc.anl.gov . We will consider adding software system wide if appropriate for general LCRC use. We generally use a package manager called Spack for most of our software installs, but often need to install packages manually. Keep in mind that the software you want may have dozens of dependencies that also need to be installed, so it may take some time for us to complete the installation for you. Of course, you\u2019re always welcome to install your own software in your home or project directory (if applicable) as well. LCRC does not purchase licenses for users that need paid commercial software. If you need a paid application, please consult with LCRC staff before mkaing a purchase.","title":"Available Software"},{"location":"using-software/cluster-specific/improv-software/","text":"","title":"Improv software"},{"location":"using-software/software-specific-guides/conda/","text":"Using Conda in LCRC Due to the increasing amount of python packages, we switched to Anaconda python distribution for python package management. Anaconda modules that we provide include a common set of packages: pip, numpy, scipy, matplotlib, mpi4py, etc If the Anaconda module does not have python package that you need already installed, you can create an Anaconda environment inside your home directory and install the needed packages there. First, load the Anaconda module that you need: module load anaconda/<version> Now you can list the already installed packages by running: conda list If your package is not installed, you can search to see if it exists to install by running: conda search <package_name> If your package is not installed and appears in the available package list, you can create a new conda environment to install the package in. The following command will create an Anaconda environment in your home directory: conda create -n <environment_name> To activate that environment, run: source activate <environment_name> Now you are ready to install the package: conda install <package_name> The complete instructions on how to use Anaconda are at: https://conda.io/docs/index.html If the packages are not provided with pip or Anaconda and the procedure for installation is very complex, feel free to email to support@lcrc.anl.gov .","title":"Conda"},{"location":"using-software/software-specific-guides/conda/#using-conda-in-lcrc","text":"Due to the increasing amount of python packages, we switched to Anaconda python distribution for python package management. Anaconda modules that we provide include a common set of packages: pip, numpy, scipy, matplotlib, mpi4py, etc If the Anaconda module does not have python package that you need already installed, you can create an Anaconda environment inside your home directory and install the needed packages there. First, load the Anaconda module that you need: module load anaconda/<version> Now you can list the already installed packages by running: conda list If your package is not installed, you can search to see if it exists to install by running: conda search <package_name> If your package is not installed and appears in the available package list, you can create a new conda environment to install the package in. The following command will create an Anaconda environment in your home directory: conda create -n <environment_name> To activate that environment, run: source activate <environment_name> Now you are ready to install the package: conda install <package_name> The complete instructions on how to use Anaconda are at: https://conda.io/docs/index.html If the packages are not provided with pip or Anaconda and the procedure for installation is very complex, feel free to email to support@lcrc.anl.gov .","title":"Using Conda in LCRC"},{"location":"using-software/software-specific-guides/gaussian/","text":"Gaussian Gaussian uses Linda rather than MPI to communicate between nodes. You need to pass directives to Gaussian to specify which nodes to use and how many processes to use on each node. The simplest way to do this is to start a Linda worker on each node and spawn a number of threads equal to the number of cores on the node. Using Gaussian on Improv Gaussian can be loaded and unloaded with the following commands respectively (for these examples, I'm using gaussian/16.C.02 ): module load gaussian/16.C.02 module unload gaussian/16.C.02 It is extremely important that you define the environmental variable GAUSS_SCRDIR to be the local disk on the compute node (/scratch). Otherwise, Gaussian will use the working directory on a shared file system as scratch space. This can slow down the LCRC servers. Below are sample PBS script that can be used to run Gaussian on Improv node(s) with optimal performance. The first example will run on 2 nodes for larger calculations that can scale to a full node of Improv. #!/bin/bash #PBS -N <JOB_NAME> #PBS -l select=2:ncpus=128:mpiprocs=1:ompthreads=128 #PBS -A <PROJECT_ALLOCATION> #PBS -l walltime=02:00:00 module load gaussian/16.C.02 cd $PBS_O_WORKDIR np = ` wc -l < $PBS_NODEFILE ` nn = ` sort -u $PBS_NODEFILE | wc -l ` node_list = ` sort -u $PBS_NODEFILE | paste -d, -s ` echo \" np= \" $np echo \" nn= \" $nn echo \" node_list= \" $node_list export GAUSS_CDEF = 0 -127 export GAUSS_MDEF = 200GB export GAUSS_WDEF = $node_list export GAUSS_SCRDIR = /scratch/test mkdir $GAUSS_SCRDIR export GAUSS_SDEF = ssh export GAUSS_LFLAGS = \"-vv\" g16 Grubbs2 rm -rf $GAUSS_SCRDIR The next example will pack 4 Gaussian jobs onto 1 node to utilize all 128 cores when the calculation does not scale as well. We do this to not waste cores that would otherwise sit idle. The PBS directive -l select=1:ncpus=128:mpiprocs=4:ompthreads=32 requests a single node ( select=1 ) with 128 CPUs, where 4 MPI processes ( mpiprocs=4 ) are to be launched. Each MPI process is configured to use 32 OpenMP threads ( ompthreads=32 ). #!/bin/bash #PBS -N <JOB_NAME> #PBS -l select=1:ncpus=128:mpiprocs=4:ompthreads=32 #PBS -A <PROJECT_ALLOCATION> #PBS -l walltime=02:00:00 module load gaussian/16.C.02 #This script will run four gaussian jobs #on 32 cores of improv simulataneously #Note that each calculation runs in a unique subfolder #and has a unique scratch disk. cd $PBS_O_WORKDIR for ii in 0 1 2 3 ; do let jj = ii*32 let kk = jj+31 let ll = ii+1 export GAUSS_CDEF = $jj - $kk export GAUSS_MDEF = 200GB export GAUSS_SCRDIR = /scratch/ $PBS_JOBID / $ll mkdir -p $GAUSS_SCRDIR cd test $ll ( g16 test_ $ll rm -rf $GAUSS_SCRDIR ) & cd .. done wait","title":"Gaussian"},{"location":"using-software/software-specific-guides/gaussian/#gaussian","text":"Gaussian uses Linda rather than MPI to communicate between nodes. You need to pass directives to Gaussian to specify which nodes to use and how many processes to use on each node. The simplest way to do this is to start a Linda worker on each node and spawn a number of threads equal to the number of cores on the node.","title":"Gaussian"},{"location":"using-software/software-specific-guides/gaussian/#using-gaussian-on-improv","text":"Gaussian can be loaded and unloaded with the following commands respectively (for these examples, I'm using gaussian/16.C.02 ): module load gaussian/16.C.02 module unload gaussian/16.C.02 It is extremely important that you define the environmental variable GAUSS_SCRDIR to be the local disk on the compute node (/scratch). Otherwise, Gaussian will use the working directory on a shared file system as scratch space. This can slow down the LCRC servers. Below are sample PBS script that can be used to run Gaussian on Improv node(s) with optimal performance. The first example will run on 2 nodes for larger calculations that can scale to a full node of Improv. #!/bin/bash #PBS -N <JOB_NAME> #PBS -l select=2:ncpus=128:mpiprocs=1:ompthreads=128 #PBS -A <PROJECT_ALLOCATION> #PBS -l walltime=02:00:00 module load gaussian/16.C.02 cd $PBS_O_WORKDIR np = ` wc -l < $PBS_NODEFILE ` nn = ` sort -u $PBS_NODEFILE | wc -l ` node_list = ` sort -u $PBS_NODEFILE | paste -d, -s ` echo \" np= \" $np echo \" nn= \" $nn echo \" node_list= \" $node_list export GAUSS_CDEF = 0 -127 export GAUSS_MDEF = 200GB export GAUSS_WDEF = $node_list export GAUSS_SCRDIR = /scratch/test mkdir $GAUSS_SCRDIR export GAUSS_SDEF = ssh export GAUSS_LFLAGS = \"-vv\" g16 Grubbs2 rm -rf $GAUSS_SCRDIR The next example will pack 4 Gaussian jobs onto 1 node to utilize all 128 cores when the calculation does not scale as well. We do this to not waste cores that would otherwise sit idle. The PBS directive -l select=1:ncpus=128:mpiprocs=4:ompthreads=32 requests a single node ( select=1 ) with 128 CPUs, where 4 MPI processes ( mpiprocs=4 ) are to be launched. Each MPI process is configured to use 32 OpenMP threads ( ompthreads=32 ). #!/bin/bash #PBS -N <JOB_NAME> #PBS -l select=1:ncpus=128:mpiprocs=4:ompthreads=32 #PBS -A <PROJECT_ALLOCATION> #PBS -l walltime=02:00:00 module load gaussian/16.C.02 #This script will run four gaussian jobs #on 32 cores of improv simulataneously #Note that each calculation runs in a unique subfolder #and has a unique scratch disk. cd $PBS_O_WORKDIR for ii in 0 1 2 3 ; do let jj = ii*32 let kk = jj+31 let ll = ii+1 export GAUSS_CDEF = $jj - $kk export GAUSS_MDEF = 200GB export GAUSS_SCRDIR = /scratch/ $PBS_JOBID / $ll mkdir -p $GAUSS_SCRDIR cd test $ll ( g16 test_ $ll rm -rf $GAUSS_SCRDIR ) & cd .. done wait","title":"Using Gaussian on Improv"},{"location":"using-software/software-specific-guides/jupyter-notebooks/","text":"Using Jupyter Notebooks in LCRC Here, we will outline how to use Jupyter Notebooks in LCRC from a local machine running Linux/MacOS. Below are examples that can be done on any LCRC cluster. Jupyter Notebooks should not be launched on any of the login nodes, but instead be run from an interactive compute node. In this full example, commands are shown for the Improv cluster on ilogin1 (login node) and i001 (compute node), but should be changed appropriately based on the cluster and nodes you actually land on. First, login to Improv for this example: ssh <username>@improv.lcrc.anl.gov Now we will activate a more recent version of conda for python3: /soft/software/custom-built/anaconda3/2023.09/bin/conda init bash You may see a lot of output here, but it will instruct you to reload your shell. For changes to take effect, close and re-open your current shell, preferably by logging out and back in. Once you log back in, you should verify you have the correct conda environment loaded: conda --version This should output: conda 23.7.4 If you see the version above (or the one you expect), you can move on to the next step. Start an interactive job. We have used the Improv debug queue in this example, since these nodes are always available for short testing/debugging jobs. Remember to replace PROJECT_NAME with a valid project you belong to. qsub -I -A PROJECT_NAME -l select=1:ncpus=128:mpiprocs=128,walltime=01:00:00 -q debug This command will drop you into 1 random debug node for 1 hour \u2013 in this example we will show it as i001. On the compute node i001, type: conda activate This will give you access to the newer conda environment now. Now run: which jupyter You should see: /soft/software/custom-built/anaconda3/2023.09/bin/jupyter confirming you have the right Jupyter executable loaded. Now, launch Jupyter Notebook on the compute node (again, i001 in our case): jupyter notebook --no-browser After several seconds, you should see something like: [I 14:00:16.503 NotebookApp] Jupyter Notebook 6.5.4 is running at: [I 14:00:16.503 NotebookApp] http://localhost:8888/?token=3ghbn74a98fbcf4235342d7a835a6d3b5e9d19934be3786e [I 14:00:16.503 NotebookApp] or http://127.0.0.1:8888/?token=3ghbn74a98fbcf4235342d7a835a6d3b5e9d19934be3786e [I 14:00:16.503 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 14:00:16.508 NotebookApp] To access the notebook, open this file in a browser: file:///gpfs/fs1/home/username/.local/share/jupyter/runtime/nbserver-2561216-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=3ghbn74a98fbcf4235342d7a835a6d3b5e9d19934be3786e or http://127.0.0.1:8888/?token=3ghbn74a98fbcf4235342d7a835a6d3b5e9d19934be3786e Please make a note of the provided URL at the bottom of the output (make sure to use your output and not the example text above). You will need that later. Also, please note that the port number after localhost may be different. In this example, it is 8888 . You should change the below commands to match this port where necessary. From another terminal on your local machine, run: ssh -L 8888:localhost:8888 <username>@ilogin1.lcrc.anl.gov This will open a new, port forwarded session on ilogin1. You may encounter a scenario where the default port of 8888 is already in use and see something like: bind [127.0.0.1]:8888: Address already in use channel_setup_fwd_listener_tcpip: cannot listen to port: 8888 Could not request local forwarding. If this happens, you may want to change the port altogether. You can change it by adding the following to your Jupyter Notebook command from earlier. Simply cancel the process and restart the process with: jupyter notebook --port=<port number> --no-browser We recommend you simply increase the port number by one to the next value from the default. For example, 8889 or 8890 if others are in use. Now, from this new session on ilogin1, SSH into the compute node running your Jupyter Notebook command from earlier: ssh -L 8888:localhost:8888 i001 Finally, cut-paste the URL obtained by launching Jupyter Notebook on the compute node earlier into a browser (Firefox/Chrome for example) running on your local machine. This should launch the Jupyter Notebook on the browser of your local machine.","title":"Jupyter Notebooks"},{"location":"using-software/software-specific-guides/jupyter-notebooks/#using-jupyter-notebooks-in-lcrc","text":"Here, we will outline how to use Jupyter Notebooks in LCRC from a local machine running Linux/MacOS. Below are examples that can be done on any LCRC cluster. Jupyter Notebooks should not be launched on any of the login nodes, but instead be run from an interactive compute node. In this full example, commands are shown for the Improv cluster on ilogin1 (login node) and i001 (compute node), but should be changed appropriately based on the cluster and nodes you actually land on. First, login to Improv for this example: ssh <username>@improv.lcrc.anl.gov Now we will activate a more recent version of conda for python3: /soft/software/custom-built/anaconda3/2023.09/bin/conda init bash You may see a lot of output here, but it will instruct you to reload your shell. For changes to take effect, close and re-open your current shell, preferably by logging out and back in. Once you log back in, you should verify you have the correct conda environment loaded: conda --version This should output: conda 23.7.4 If you see the version above (or the one you expect), you can move on to the next step. Start an interactive job. We have used the Improv debug queue in this example, since these nodes are always available for short testing/debugging jobs. Remember to replace PROJECT_NAME with a valid project you belong to. qsub -I -A PROJECT_NAME -l select=1:ncpus=128:mpiprocs=128,walltime=01:00:00 -q debug This command will drop you into 1 random debug node for 1 hour \u2013 in this example we will show it as i001. On the compute node i001, type: conda activate This will give you access to the newer conda environment now. Now run: which jupyter You should see: /soft/software/custom-built/anaconda3/2023.09/bin/jupyter confirming you have the right Jupyter executable loaded. Now, launch Jupyter Notebook on the compute node (again, i001 in our case): jupyter notebook --no-browser After several seconds, you should see something like: [I 14:00:16.503 NotebookApp] Jupyter Notebook 6.5.4 is running at: [I 14:00:16.503 NotebookApp] http://localhost:8888/?token=3ghbn74a98fbcf4235342d7a835a6d3b5e9d19934be3786e [I 14:00:16.503 NotebookApp] or http://127.0.0.1:8888/?token=3ghbn74a98fbcf4235342d7a835a6d3b5e9d19934be3786e [I 14:00:16.503 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 14:00:16.508 NotebookApp] To access the notebook, open this file in a browser: file:///gpfs/fs1/home/username/.local/share/jupyter/runtime/nbserver-2561216-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=3ghbn74a98fbcf4235342d7a835a6d3b5e9d19934be3786e or http://127.0.0.1:8888/?token=3ghbn74a98fbcf4235342d7a835a6d3b5e9d19934be3786e Please make a note of the provided URL at the bottom of the output (make sure to use your output and not the example text above). You will need that later. Also, please note that the port number after localhost may be different. In this example, it is 8888 . You should change the below commands to match this port where necessary. From another terminal on your local machine, run: ssh -L 8888:localhost:8888 <username>@ilogin1.lcrc.anl.gov This will open a new, port forwarded session on ilogin1. You may encounter a scenario where the default port of 8888 is already in use and see something like: bind [127.0.0.1]:8888: Address already in use channel_setup_fwd_listener_tcpip: cannot listen to port: 8888 Could not request local forwarding. If this happens, you may want to change the port altogether. You can change it by adding the following to your Jupyter Notebook command from earlier. Simply cancel the process and restart the process with: jupyter notebook --port=<port number> --no-browser We recommend you simply increase the port number by one to the next value from the default. For example, 8889 or 8890 if others are in use. Now, from this new session on ilogin1, SSH into the compute node running your Jupyter Notebook command from earlier: ssh -L 8888:localhost:8888 i001 Finally, cut-paste the URL obtained by launching Jupyter Notebook on the compute node earlier into a browser (Firefox/Chrome for example) running on your local machine. This should launch the Jupyter Notebook on the browser of your local machine.","title":"Using Jupyter Notebooks in LCRC"},{"location":"using-software/software-specific-guides/openmc/","text":"OpenMC OpenMC uses both distributed-memory (MPI) and shared-memory (OpenMP) parallelism to maximize computational efficiency. Optimal performance is achieved by aligning parallelization strategies with hardware architecture, such as using one MPI process per NUMA node on servers with large core counts or employing hardware threading. Using OpenMC on Improv On Improv, OpenMC was installed in a Conda environment. The envionment can be loaded and unloaded with the following commands respectively (for these examples, I'm using openmc/0.14.0 ): module load openmc/0.14.0 module unload openmc/0.14.0 Below is a sample PBS script that can be used to run OpenMC on an Improv node with optimal performance. The PBS directive -l select=1:ncpus=128:mpiprocs=8:ompthreads=16 requests a single node ( select=1 ) with 128 CPUs, where 8 MPI processes ( mpiprocs=8 ) are to be launched. Each MPI process is configured to use 16 OpenMP threads ( ompthreads=16 ). When the script executes the mpiexec command, it launches 8 separate MPI processes ( -np 8 ). The --bind-to numa and --map-by numa options instruct OpenMPI to bind each process to a NUMA node and map the processes by NUMA node. #!/bin/bash #PBS -N <JOB_NAME> #PBS -l select=1:ncpus=128:mpiprocs=8:ompthreads=16 #PBS -l walltime=6:00:00 #PBS -j oe #PBS -A <PROJECT_ALLOCATION> #PBS -o output_<JOB_NAME> #PBS -e error_<JOB_NAME> module load openmc/0.14.0 # Set the necessary OpenMC variables and paths. For example: export OPENMC_CROSS_SECTIONS = <PATH_TO_CROSS_SECTIONS> export OPENMC_DEPLETE_CHAIN = <PATH_TO_DEPLETE_CHAIN> # Set the number of OpenMP threads export OMP_NUM_THREADS = 16 # Change to the directory from which the job was submitted cd $PBS_O_WORKDIR # Launch mpiexec with proper NUMA binding and process mapping. Replace the python script name with your script. mpiexec -np 8 --bind-to numa --map-by numa python <YOUR_SCRIPT_NAME>.py","title":"OpenMC"},{"location":"using-software/software-specific-guides/openmc/#openmc","text":"OpenMC uses both distributed-memory (MPI) and shared-memory (OpenMP) parallelism to maximize computational efficiency. Optimal performance is achieved by aligning parallelization strategies with hardware architecture, such as using one MPI process per NUMA node on servers with large core counts or employing hardware threading.","title":"OpenMC"},{"location":"using-software/software-specific-guides/openmc/#using-openmc-on-improv","text":"On Improv, OpenMC was installed in a Conda environment. The envionment can be loaded and unloaded with the following commands respectively (for these examples, I'm using openmc/0.14.0 ): module load openmc/0.14.0 module unload openmc/0.14.0 Below is a sample PBS script that can be used to run OpenMC on an Improv node with optimal performance. The PBS directive -l select=1:ncpus=128:mpiprocs=8:ompthreads=16 requests a single node ( select=1 ) with 128 CPUs, where 8 MPI processes ( mpiprocs=8 ) are to be launched. Each MPI process is configured to use 16 OpenMP threads ( ompthreads=16 ). When the script executes the mpiexec command, it launches 8 separate MPI processes ( -np 8 ). The --bind-to numa and --map-by numa options instruct OpenMPI to bind each process to a NUMA node and map the processes by NUMA node. #!/bin/bash #PBS -N <JOB_NAME> #PBS -l select=1:ncpus=128:mpiprocs=8:ompthreads=16 #PBS -l walltime=6:00:00 #PBS -j oe #PBS -A <PROJECT_ALLOCATION> #PBS -o output_<JOB_NAME> #PBS -e error_<JOB_NAME> module load openmc/0.14.0 # Set the necessary OpenMC variables and paths. For example: export OPENMC_CROSS_SECTIONS = <PATH_TO_CROSS_SECTIONS> export OPENMC_DEPLETE_CHAIN = <PATH_TO_DEPLETE_CHAIN> # Set the number of OpenMP threads export OMP_NUM_THREADS = 16 # Change to the directory from which the job was submitted cd $PBS_O_WORKDIR # Launch mpiexec with proper NUMA binding and process mapping. Replace the python script name with your script. mpiexec -np 8 --bind-to numa --map-by numa python <YOUR_SCRIPT_NAME>.py","title":"Using OpenMC on Improv"},{"location":"using-software/software-specific-guides/overview/","text":"","title":"Overview"},{"location":"using-software/software-specific-guides/paraview/","text":"Using Paraview in LCRC Here, we will outline how to use Paraview in client-server mode in LCRC from a local machine running Linux/MacOS. Below are examples using the Bebop cluster. Paraview itself should not be launched on any of the login nodes, but instead be run in client-server mode. Paraview Client-Server to an LCRC Login Node In this first example, commands are shown for beboplogin3 and should be changed appropriately if you are using another login node. First, login to Bebop for this example: ssh <username>@bebop.lcrc.anl.gov Make note of the login node name and number that you end up on. As mentioned for this example, it is beboplogin3, but may be different for you. Now, load the Paraview module (in this example we are using paraview 5.4.0. Users can check the available versions of paraview by using the command module spider paraview . Load any of the modules of paraview available and ensure that you have the same version available on your local machine.): module load paraview/5.4.0 You can verify that Paraview is loaded if you can produce this output: $ which pvserver /soft/bebop/paraview/5.4.0/bin/pvserver Launch pvserver in your case directory: pvserver You should get output similar to the following: Waiting for client... Connection URL: cs://beboplogin3:11111 Accepting connection(s): beboplogin3:11111 Now, open new terminal on your local machine/desktop (Mac/Linux). Create an SSH tunnel to the login node you started pvserver on: ssh -L 11111:localhost:11111 <username>@beboplogin3.lcrc.anl.gov Back on your local machine, download and install the Linux/MacOS Paraview version 5.4.0 (or the matching version from the Paraview module you loaded in the first step) from the Paraview website . Once installed, launch Paraview locally and set up the connection to connect to our server instance. Click on the icon for Connect (marked in red circle on the figure below) and then click on the Add Server tab: Fill in the details as shown below. You should use the port number given to you from when you started the pvserver instance if different than below. Set the name as one of the login nodes, beboplogin2, beboplogin3, etc. and hit Configure : This will take you to the next tab where you can hit Save to save the configuration (Startup Type is Manual which is the default setting): After adding a server, you should see a panel as shown below. You could add multiple servers using the above procedure. Click on the server name where you have started pvserver (in this example it would be beboplogin3) and hit Connect : To verify your connection was successful (aside from no error messages locally), you should now see a connection message on the window running pvserver (note the last line below): Waiting for client... Connection URL: cs://beboplogin3:11111 Accepting connection(s): beboplogin3:11111 Client connected. Once connected, you should be able to open your solution files in your case directory on Bebop (since pvserver was launched from it). Open a case file clicking on the icon marked in the red circle below: Your solution files should now be running in client-server mode. Paraview Client-Server to an LCRC Compute Node (GPU Example) Users who need to visualize large problems will have to use a compute node with more resources, preferably on a GPU resource. The below is an example of how to do this on the LCRC Swing cluster. Note: If needed, please reference the first section of this page for any screenshots needed if you have trouble finding an option in the Paraview GUI. First, login to Swing for this example: ssh <username>@swing.lcrc.anl.gov Make note of the login node name and number that you end up on. Request an interactive job on 1 node and 1 GPU (for 30 minutes as an example): salloc -N 1 --gres=gpu:1 -t 00:30:00 -A <project_name> Note: The partition may not have free nodes so you might not be able to start your interactive job immediately. If a node is available and your job starts, you should see something like: salloc: Granted job allocation <job_number> Once your job starts, check the node number granted to your job: squeue -u $USER You should see something like: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) <job_number> gpu bash <username> R 0:04 1 <node_number> SSH into the node allocated to your job (say gpu1 for example): ssh <node_number> Change to bash shell if you aren\u2019t already using it: bash Now, load the Paraview module: module load paraview/5.11.1 You can verify that Paraview is loaded if you can produce this output: $ which pvserver /gpfs/fs1/soft/swing/manual/paraview/5.11.1/bin/pvserver Launch pvserver in your case directory: pvserver --server-port=11111 You should get output similar to the following: Waiting for client... Connection URL: cs://<node_number>:11111 Accepting connection(s): <node_number>:11111 From your local Linux/MacOS machine type the following command to set up the tunnel to the allotted node: ssh -L 11111:<node_number>.lcrc.anl.gov:11111 <username>@gpulogin<node_number>.lcrc.anl.gov For example, if you have logged onto gpulogin1 to launch your interactive job and your allotted node number is gpu1 then your command will be: ssh -L 11111:gpu1.lcrc.anl.gov:11111 <username>@gpulogin1.lcrc.anl.gov Launch Paraview from your local machine and connect to the gpulogin node (gpulogin1 in the above example) as explained in the first section of this page. On the node you should see: Waiting for client... Connection URL: cs://<node_number>:11111 Accepting connection(s): <node_number>:11111 Client connected. You can now navigate to the case folder and load the case as described above. After viewing your case, you can cancel the interactive job by either exiting completely out of your node session or by using Slurm: scancel <job_number>","title":"Paraview"},{"location":"using-software/software-specific-guides/paraview/#using-paraview-in-lcrc","text":"Here, we will outline how to use Paraview in client-server mode in LCRC from a local machine running Linux/MacOS. Below are examples using the Bebop cluster. Paraview itself should not be launched on any of the login nodes, but instead be run in client-server mode.","title":"Using Paraview in LCRC"},{"location":"using-software/software-specific-guides/paraview/#paraview-client-server-to-an-lcrc-login-node","text":"In this first example, commands are shown for beboplogin3 and should be changed appropriately if you are using another login node. First, login to Bebop for this example: ssh <username>@bebop.lcrc.anl.gov Make note of the login node name and number that you end up on. As mentioned for this example, it is beboplogin3, but may be different for you. Now, load the Paraview module (in this example we are using paraview 5.4.0. Users can check the available versions of paraview by using the command module spider paraview . Load any of the modules of paraview available and ensure that you have the same version available on your local machine.): module load paraview/5.4.0 You can verify that Paraview is loaded if you can produce this output: $ which pvserver /soft/bebop/paraview/5.4.0/bin/pvserver Launch pvserver in your case directory: pvserver You should get output similar to the following: Waiting for client... Connection URL: cs://beboplogin3:11111 Accepting connection(s): beboplogin3:11111 Now, open new terminal on your local machine/desktop (Mac/Linux). Create an SSH tunnel to the login node you started pvserver on: ssh -L 11111:localhost:11111 <username>@beboplogin3.lcrc.anl.gov Back on your local machine, download and install the Linux/MacOS Paraview version 5.4.0 (or the matching version from the Paraview module you loaded in the first step) from the Paraview website . Once installed, launch Paraview locally and set up the connection to connect to our server instance. Click on the icon for Connect (marked in red circle on the figure below) and then click on the Add Server tab: Fill in the details as shown below. You should use the port number given to you from when you started the pvserver instance if different than below. Set the name as one of the login nodes, beboplogin2, beboplogin3, etc. and hit Configure : This will take you to the next tab where you can hit Save to save the configuration (Startup Type is Manual which is the default setting): After adding a server, you should see a panel as shown below. You could add multiple servers using the above procedure. Click on the server name where you have started pvserver (in this example it would be beboplogin3) and hit Connect : To verify your connection was successful (aside from no error messages locally), you should now see a connection message on the window running pvserver (note the last line below): Waiting for client... Connection URL: cs://beboplogin3:11111 Accepting connection(s): beboplogin3:11111 Client connected. Once connected, you should be able to open your solution files in your case directory on Bebop (since pvserver was launched from it). Open a case file clicking on the icon marked in the red circle below: Your solution files should now be running in client-server mode.","title":"Paraview Client-Server to an LCRC Login Node"},{"location":"using-software/software-specific-guides/paraview/#paraview-client-server-to-an-lcrc-compute-node-gpu-example","text":"Users who need to visualize large problems will have to use a compute node with more resources, preferably on a GPU resource. The below is an example of how to do this on the LCRC Swing cluster. Note: If needed, please reference the first section of this page for any screenshots needed if you have trouble finding an option in the Paraview GUI. First, login to Swing for this example: ssh <username>@swing.lcrc.anl.gov Make note of the login node name and number that you end up on. Request an interactive job on 1 node and 1 GPU (for 30 minutes as an example): salloc -N 1 --gres=gpu:1 -t 00:30:00 -A <project_name> Note: The partition may not have free nodes so you might not be able to start your interactive job immediately. If a node is available and your job starts, you should see something like: salloc: Granted job allocation <job_number> Once your job starts, check the node number granted to your job: squeue -u $USER You should see something like: JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) <job_number> gpu bash <username> R 0:04 1 <node_number> SSH into the node allocated to your job (say gpu1 for example): ssh <node_number> Change to bash shell if you aren\u2019t already using it: bash Now, load the Paraview module: module load paraview/5.11.1 You can verify that Paraview is loaded if you can produce this output: $ which pvserver /gpfs/fs1/soft/swing/manual/paraview/5.11.1/bin/pvserver Launch pvserver in your case directory: pvserver --server-port=11111 You should get output similar to the following: Waiting for client... Connection URL: cs://<node_number>:11111 Accepting connection(s): <node_number>:11111 From your local Linux/MacOS machine type the following command to set up the tunnel to the allotted node: ssh -L 11111:<node_number>.lcrc.anl.gov:11111 <username>@gpulogin<node_number>.lcrc.anl.gov For example, if you have logged onto gpulogin1 to launch your interactive job and your allotted node number is gpu1 then your command will be: ssh -L 11111:gpu1.lcrc.anl.gov:11111 <username>@gpulogin1.lcrc.anl.gov Launch Paraview from your local machine and connect to the gpulogin node (gpulogin1 in the above example) as explained in the first section of this page. On the node you should see: Waiting for client... Connection URL: cs://<node_number>:11111 Accepting connection(s): <node_number>:11111 Client connected. You can now navigate to the case folder and load the case as described above. After viewing your case, you can cancel the interactive job by either exiting completely out of your node session or by using Slurm: scancel <job_number>","title":"Paraview Client-Server to an LCRC Compute Node (GPU Example)"},{"location":"using-software/software-specific-guides/tensorflow/","text":"Using Tensorflow in LCRC To use TensorFlow on the LCRC GPU resources, we recommend using a container. In LCRC, we support Singularity for containers. Below, we\u2019ll highlight how to get a container, load Singularity to build your container and run a simple test. Before proceeding, you should be familiar with how to run an interactive job on Swing. Getting the Container To get a suitable container which has the latest CUDA and TensorFlow installed, head over to the NVIDIA NGC website and search for Tensorflow . Click on the TensorFlow link. You should now see some basic information about TensorFlow. At the top of the page, you\u2019ll notice versions next to \u2018Pull Tag\u2018. Also notice there are containers tagged with tf1 and tf2. Make sure to select the correct version that applies to you. Choose the correct version and click 'Pull Tag' . This will copy a Docker URL to your clipboard. More on this later. For this example, we\u2019ll use version 21.12-tf2-py3 . Building your Container Next, from a Swing login node for example, start an interactive job. In this example, we\u2019ll request 2 GPUs. gpulogin1:~$ srun --gres = gpu:2 --pty bash Once your job starts, you should be put onto your allocated node. Node gpu6 in this example. Now we can load the Singularity module: gpu6:~$ module load singularity Using the Tag you copied in the previous section, let\u2019s build a container with Singularity. Because these are actually Docker built containers by default, what we are doing is converting them to a Singularity ready container. The Tag you copied will probably look like this: docker pull nvcr.io/nvidia/tensorflow:21.12-tf2-py3 . Let\u2019s use what we need below to convert this now: gpu6:~$ singularity build tensorflow-21.12-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:21.12-tf2-py3 tensorflow-21.12-tf2-py3.simg is the name of your new container. You can name this whatever you\u2019d like, but we\u2019ll keep the naming consistent. The above command may take several minutes. Testing TensorFlow Lastly, we\u2019ll test that the TensorFlow container works. We\u2019ll do a simple query of the GPUs we allocated earlier. Remember, we only requested 2 GPUs: gpu6:~$ singularity exec --nv tensorflow-21.12-tf2-py3.simg python -c \"import tensorflow as tf; tf.test.gpu_device_name()\" 2022-01-05 15:20:08.205124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 38188 MB memory: -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0 2022-01-05 15:20:08.230207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:1 with 38188 MB memory: -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4e:00.0, compute capability: 8.0 When you are finished testing, remember to relinquish your node allocation. For more information, including tutorials, please review the TensorFlow documentation on the NVIDIA website and well as the Singularity user guide.","title":"Tensorflow"},{"location":"using-software/software-specific-guides/tensorflow/#using-tensorflow-in-lcrc","text":"To use TensorFlow on the LCRC GPU resources, we recommend using a container. In LCRC, we support Singularity for containers. Below, we\u2019ll highlight how to get a container, load Singularity to build your container and run a simple test. Before proceeding, you should be familiar with how to run an interactive job on Swing.","title":"Using Tensorflow in LCRC"},{"location":"using-software/software-specific-guides/tensorflow/#getting-the-container","text":"To get a suitable container which has the latest CUDA and TensorFlow installed, head over to the NVIDIA NGC website and search for Tensorflow . Click on the TensorFlow link. You should now see some basic information about TensorFlow. At the top of the page, you\u2019ll notice versions next to \u2018Pull Tag\u2018. Also notice there are containers tagged with tf1 and tf2. Make sure to select the correct version that applies to you. Choose the correct version and click 'Pull Tag' . This will copy a Docker URL to your clipboard. More on this later. For this example, we\u2019ll use version 21.12-tf2-py3 .","title":"Getting the Container"},{"location":"using-software/software-specific-guides/tensorflow/#building-your-container","text":"Next, from a Swing login node for example, start an interactive job. In this example, we\u2019ll request 2 GPUs. gpulogin1:~$ srun --gres = gpu:2 --pty bash Once your job starts, you should be put onto your allocated node. Node gpu6 in this example. Now we can load the Singularity module: gpu6:~$ module load singularity Using the Tag you copied in the previous section, let\u2019s build a container with Singularity. Because these are actually Docker built containers by default, what we are doing is converting them to a Singularity ready container. The Tag you copied will probably look like this: docker pull nvcr.io/nvidia/tensorflow:21.12-tf2-py3 . Let\u2019s use what we need below to convert this now: gpu6:~$ singularity build tensorflow-21.12-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:21.12-tf2-py3 tensorflow-21.12-tf2-py3.simg is the name of your new container. You can name this whatever you\u2019d like, but we\u2019ll keep the naming consistent. The above command may take several minutes.","title":"Building your Container"},{"location":"using-software/software-specific-guides/tensorflow/#testing-tensorflow","text":"Lastly, we\u2019ll test that the TensorFlow container works. We\u2019ll do a simple query of the GPUs we allocated earlier. Remember, we only requested 2 GPUs: gpu6:~$ singularity exec --nv tensorflow-21.12-tf2-py3.simg python -c \"import tensorflow as tf; tf.test.gpu_device_name()\" 2022-01-05 15:20:08.205124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 38188 MB memory: -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0 2022-01-05 15:20:08.230207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:1 with 38188 MB memory: -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4e:00.0, compute capability: 8.0 When you are finished testing, remember to relinquish your node allocation. For more information, including tutorials, please review the TensorFlow documentation on the NVIDIA website and well as the Singularity user guide.","title":"Testing TensorFlow"},{"location":"using-software/software-specific-guides/vasp/","text":"VASP Using VASP on Improv VASP can be loaded and unloaded with the following commands respectively (for these examples, I'm using vasp/6.4.2 ): module load vasp/6.4.2 module unload vasp/6.4.2 Below is a sample PBS script that can be used to run VASP on an Improv node with optimal performance. It will pack 8 VASP jobs onto 1 node to utilize all 128 cores. The PBS directive -l select=1:mpiprocs=128 requests a single node ( select=1 ) with 128 CPUs, where 128 MPI processes ( mpiprocs=128 ) are to be launched. #!/bin/bash #PBS -N <JOB_NAME> #PBS -l select=1:mpiprocs=128 #PBS -A <PROJECT_ALLOCATION> #PBS -l walltime=01:00:00 # Note that VASP was built with openmpi version 4.1.6. # This method of assigning CPU affinities will not work with openmpi version 5.0.0 and above, # or MPICH, MVAPICH or Intel-MPI. module load vasp/6.4.2 cd $PBS_O_WORKDIR # The following loop with spawn eight VASP jobs. # Each mpi task spawned by this loop will run on a unique core. # The output and input for each VASP job will be in a unique subfolder. for ii in { 1 ..8 } ; do # Change to unique directory for this job. cd $ii # Define the first CPU for this jobi. let jj =( ii-1 ) *16 # Define the last CPU for this job. let kk = $jj +15 # The RANGE variable contains the list of cpus for this job to run on. export RANGE = $( eval echo { $jj .. $kk } | xargs | sed -e 's/ /,/g' ) # The MPIOPT variable contains the MPIRUN directives defining the CPU affinities. export MPIOPT = \"--bind-to cpu-list:ordered --cpu-list $RANGE \" # The ampersand at the end of the next command spawns a subprocess. # The --report-bindings directive is optional. mpirun $MPIOPT --report-bindings -np 16 vasp_gam & # Change back to PBS_O_WORKDIR. cd .. done # Wait for all the subprocesses to complete. wait","title":"VASP"},{"location":"using-software/software-specific-guides/vasp/#vasp","text":"","title":"VASP"},{"location":"using-software/software-specific-guides/vasp/#using-vasp-on-improv","text":"VASP can be loaded and unloaded with the following commands respectively (for these examples, I'm using vasp/6.4.2 ): module load vasp/6.4.2 module unload vasp/6.4.2 Below is a sample PBS script that can be used to run VASP on an Improv node with optimal performance. It will pack 8 VASP jobs onto 1 node to utilize all 128 cores. The PBS directive -l select=1:mpiprocs=128 requests a single node ( select=1 ) with 128 CPUs, where 128 MPI processes ( mpiprocs=128 ) are to be launched. #!/bin/bash #PBS -N <JOB_NAME> #PBS -l select=1:mpiprocs=128 #PBS -A <PROJECT_ALLOCATION> #PBS -l walltime=01:00:00 # Note that VASP was built with openmpi version 4.1.6. # This method of assigning CPU affinities will not work with openmpi version 5.0.0 and above, # or MPICH, MVAPICH or Intel-MPI. module load vasp/6.4.2 cd $PBS_O_WORKDIR # The following loop with spawn eight VASP jobs. # Each mpi task spawned by this loop will run on a unique core. # The output and input for each VASP job will be in a unique subfolder. for ii in { 1 ..8 } ; do # Change to unique directory for this job. cd $ii # Define the first CPU for this jobi. let jj =( ii-1 ) *16 # Define the last CPU for this job. let kk = $jj +15 # The RANGE variable contains the list of cpus for this job to run on. export RANGE = $( eval echo { $jj .. $kk } | xargs | sed -e 's/ /,/g' ) # The MPIOPT variable contains the MPIRUN directives defining the CPU affinities. export MPIOPT = \"--bind-to cpu-list:ordered --cpu-list $RANGE \" # The ampersand at the end of the next command spawns a subprocess. # The --report-bindings directive is optional. mpirun $MPIOPT --report-bindings -np 16 vasp_gam & # Change back to PBS_O_WORKDIR. cd .. done # Wait for all the subprocesses to complete. wait","title":"Using VASP on Improv"}]}